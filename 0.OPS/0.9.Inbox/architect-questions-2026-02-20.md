---
type: meeting-prep
date: 2026-02-20
with: Архитектор
topics: bot-scaling, IWE-architecture
status: resolved
meeting_date: 2026-02-19
---

# Вопросы для обсуждения с Архитектором — 20 фев 2026

> **Статус:** Обсуждение состоялось 19 фев 2026. Решения зафиксированы ниже (тег `→ Решение:`).

---

## Сводка решений (19.02.2026)

| # | Вопрос | Решение |
|---|--------|---------|
| A1 | Polling → Webhook | **Webhook** — переходить |
| A2 | Scheduler отдельный сервис | **Да**, но отложить (не сейчас) |
| A3 | Claude API cost | Без дополнительных решений (Haiku routing уже есть) |
| A4 | Circuit breaker | **Единый паттерн** для всех внешних зависимостей |
| A5 | Observability | **Grafana Cloud** (писать в Grafana) |
| A6 | Connection pooling | **Neon Pro** — купить чуть позже (когда подойдём к лимиту) |
| A7 | Slot scheduling | Без дополнительных решений (slot suggestion уже есть) |
| C1 | Топология деплоя | **Railway + CF Workers + Neon + GitHub** — текущая связка верна. Позже заменить Railway → Kubernetes |
| C2 | MCP границы | **knowledge-mcp** (Pack + онтология) — **публичный для всех**. **digital-twin-mcp** — для закрытых данных. Возможно отдельный MCP для личного репо «Моё стратегирование» |
| C3 | Pack→Downstream sync | **Event-driven** через **GitHub Actions** (Pack commit → webhook → reindex) |
| C5 | Изоляция данных | **Требуется решение** — не ясно, как защитить данные пользователей друг от друга |
| — | Агенты | Много агентов, каждый ждёт команду. Нужна **конфигурация агента** (что может/не может). Взаимодействие агентов через **Discourse** |
| — | Платформа Андрея | Андрей исследует **Discourse** как личную платформу. Договорённость позднее |
| — | ORY + репликация | Андрею решить вопрос хранения ORY за границей + репликация с Пашей |

---

## A. Бот: масштабирование до 10K пользователей

### Контекст

Бот @aist_me_bot (Telegram, aiogram, PostgreSQL/Neon, Claude API). Текущая ёмкость ~50 concurrent. Цель: 10K DAU (~500 concurrent). Диагноз ошибок: 60% архитектура, 25% инфраструктура, 15% код.

Уже сделано (Phase 0): DB pool 10→50, Claude semaphore(10) + retry, MCP session reuse, scheduler semaphore 5→20.

### Вопросы

**A1. Polling → Webhook: когда и как?**
Сейчас бот работает через long polling (один процесс). Для horizontal scaling нужны webhooks. Вопрос: делать переход сейчас (300 users) или отложить до 500+? Что нужно для stateless webhooks с aiogram (FSM storage, session affinity)?

> → **Решение:** Переходить на **Webhook**. Это необходимое условие для horizontal scaling.

**A2. Scheduler как отдельный сервис**
Scheduler сейчас внутри bot process. При multi-instance дублируется. Варианты:
- Redis pub/sub + один scheduler worker
- Postgres advisory locks (бесплатно, но coupling)
- Cron-сервис (Railway cron + HTTP trigger)
Какой подход правильнее для 5-10 инстансов?

> → **Решение:** Scheduler как **отдельный сервис** — правильный путь, но **отложить** реализацию. Сейчас не приоритет.

**A3. Claude API cost: $5K/мес при 10K DAU**
96% стоимости — Claude API. Mitigation:
- Haiku для простых задач (навигация, команды) — какой порог переключения?
- Кэширование сгенерированного контента (тот же контент для той же темы?) — trade-off персонализация vs. стоимость
- Prompt caching (Anthropic feature) — применимо ли?
Какая архитектура оптимальна?

> → **Решение:** Без дополнительных решений. Haiku routing для простых задач уже реализован. Кэширование контента уже работает.

**A4. Circuit breaker: единый паттерн**
Сейчас circuit breaker есть для MCP (2 failures → 60s cooldown), но нет для Claude API. Нужен ли единый circuit breaker middleware? Или достаточно per-client?

> → **Решение:** **Единый паттерн** circuit breaker для всех внешних зависимостей (Claude API, MCP, Neon). Цель: чтобы циклические запросы не списывали деньги.

**A5. Observability: Sentry vs. текущий async error logger**
Текущий мониторинг: async DB logging + Telegram alerts. Для 10K нужен managed monitoring? Sentry ($26/мес), Grafana Cloud (free tier), или достаточно улучшить текущее?

> → **Решение:** **Grafana Cloud** — писать метрики и логи в Grafana. Free tier достаточен на текущем этапе.

**A6. Connection pooling: PgBouncer vs. Neon Pro**
Neon Free: 100 connections. При 5 инстансах × 50 = 250 нужно. Варианты:
- Neon Pro ($20/мес): 100 connections + PgBouncer built-in
- Supabase PgBouncer: connection multiplexing (1000 client → 100 DB)
- Self-hosted PgBouncer на Railway
Что выбрать?

> → **Решение:** **Neon Pro** ($20/мес) — купить чуть позже, когда подойдём к лимиту connections.

**A7. Рассредоточение пиковой нагрузки (slot-based scheduling)**
Сейчас пользователь выбирает время рассылки (большинство — 10:00). При 1000 users в 10:00 — scheduler перегружен. Идея: ограничивать количество пользователей на один слот (например, max 50 на 10:00), предлагать ближайшее свободное окно (10:05, 10:10...). Вопросы:
- Гранулярность слотов: 5 мин? 15 мин?
- UX: «10:00 занято, предлагаем 10:05» или автоматический сдвиг?
- Нужна ли таблица слотов в БД или достаточно COUNT по scheduled_hour/minute?

> → **Решение:** Slot suggestion уже реализован (WP-44 Phase 0). Дополнительных решений не требуется.

---

## B. IWE: архитектура развёртывания среды T1→T4

### Контекст

IWE (Intellectual Work Environment) — среда для интеллектуальной работы, аналог IDE, но для мышления и знаний. Участник экосистемы проходит по уровням T1→T4, постепенно получая всю функциональность IWE. Текущая архитектура тиров (DP.ARCH.002): T1 Старт → T2 Изучение → T3 Персонализация → T4 Созидание (полная IWE).

Компоненты IWE (аналогия IDE): Pack = Editor, Extractor = Compiler, FPF/SPF = Linter, Assessment = Debugger, Projection pipeline = Build system, MCP = Package manager, Git + Synchronizer = Version control, Template = Project templates, Bot = Runtime.

Ключевое ограничение: участник на T1 не должен видеть сложности T4. Но инфраструктура должна быть готова к бесшовному переходу.

### Решения по секции B (общие)

> → **Решение (агенты):** Много агентов (каждый ждёт команду и после получения выполняет). Нужно описать конфигурацию каждого агента: что может и не может. Агент запускается → далее взаимодействие агентов через **Discourse** (платформа для коммуникации агентов).
>
> → **Решение (платформа):** Андрей исследует платформу **Discourse**, чтобы сделать личную платформу Андрея. Договорённость по платформе Андрея будет позднее.
>
> → **Решение (ORY):** Андрею решить вопрос хранения ORY за границей. Вопрос репликации решить с Пашей.
>
> → Вопросы B1-B8 **остаются открытыми** — детальные решения по тирам будут позднее, после прояснения платформы.

### Вопросы

**B1. Архитектура перехода T1→T2: от бота к полному профилю**
На T1 пользователь взаимодействует только через бот (Telegram). На T2 появляется полный профиль, программы, сообщество. Вопросы:
- Где хранится «учебный прогресс» пользователя на T1→T2? Достаточно ли digital twin в текущей схеме (Neon + digital-twin-mcp)?
- Как интегрироваться с Aisystant (руководства, программы)? Бот = фронтенд к Aisystant, или бот = отдельная система со своими программами?
- T2 предполагает подписку БР (Бесконечное Развитие). Техническая привязка подписки к расширению доступа — через Telegram Stars или внешний биллинг?

**B2. Архитектура перехода T2→T3: появление экзокортекса**
На T3 пользователь получает: standard CLAUDE.md + standard memory/ + digital twin read/write. Это первая точка, где появляется **персональная среда** (не просто бот). Вопросы:
- **Вариант A (Cloud-managed):** Платформа хостит экзокортекс пользователя (GitHub repo создаётся автоматически, пользователь работает через бот + веб-интерфейс). Плюс: нулевой порог входа. Минус: нужен multi-tenant Git management.
- **Вариант B (Self-hosted):** Пользователь делает fork шаблона сам, подключает Claude Code. Плюс: полный контроль. Минус: высокий технический порог.
- **Вариант C (Гибрид):** На T3 — cloud-managed (бот + ЦД). На T4 — fork + self-hosted. T3→T4 = миграция из cloud в собственный репо.
- Какой вариант правильный? Или нужен четвёртый?

**B3. Standard vs Personal на T3: управление обновлениями**
T3 даёт standard CLAUDE.md + standard memory/ (обновляются централизованно через template-sync). Personal-файлы пользователь пишет сам. Вопросы:
- Механизм доставки обновлений standard-контента на T3 (cloud-managed): push через GitHub Actions? Или MCP-сервер отдаёт standard-контент динамически (не файлы, а API)?
- Если standard-контент через API (не файлы) — нужен ли Git на T3 вообще? Может, T3 = бот + ЦД + standard system prompt (без репозитория)?
- Как предотвратить ситуацию, когда standard-обновление ломает personal-настройки?

**B4. Архитектура перехода T3→T4: полная IWE**
На T4 пользователь получает: personal CLAUDE.md + personal memory/ + git-based экзокортекс + агенты (Стратег, Экстрактор, Синхронизатор). Это полная IWE. Вопросы:
- T4 предполагает, что пользователь умеет работать с Git + CLI. Реалистично ли это для целевой аудитории (люди из МИМ, бизнесмены, менеджеры)? Нужен ли промежуточный тир T3.5 (Git, но без CLI)?
- Миграция T3→T4: если на T3 был cloud-managed экзокортекс — как перенести данные (ЦД, заметки, Pack) в собственные репозитории? Export через API? Git clone?
- Агенты на T4 (Стратег, Экстрактор, Синхронизатор) сейчас запускаются через launchd (macOS). Для кросс-платформы (Windows, Linux) — нужен ли scheduler-сервис в облаке? Или GitHub Actions достаточно?

**B5. Компоненты IWE по тирам: что даём на каждом уровне?**
Предлагаемое распределение компонентов IWE (аналогия IDE):

| Компонент IWE | IDE-аналог | T1 | T2 | T3 | T4 |
|---------------|------------|----|----|----|----|
| Знания (Pack) | Editor (read) | Поиск через бот | + программы Aisystant | + browse ЦД | + свои Pack |
| Извлечение (Extractor) | Compiler | — | — | — | Экстрактор |
| Рамки (FPF/SPF) | Linter | Встроены в бот | Встроены в бот | standard CLAUDE.md | + personal правила |
| Оценка | Debugger | Базовый feedback | + тесты | + self-assessment | + Assessment агент |
| Проекция | Build system | — | — | — | Pack→Downstream |
| Распределение (MCP) | Package mgr | knowledge-mcp | + guides-mcp | + digital-twin-mcp | + personal MCP |
| Версионирование | VCS | — | — | ЦД (managed) | Git (full) |
| Шаблон | Project tmpl | — | — | — | fork template |
| Исполнение (Bot) | Runtime | Бот | Бот + программы | Бот + ЦД + Claude | Бот + Claude Code |

Вопрос: эта таблица корректна? Что сдвинуть, что убрать? Есть ли компоненты, которые нужно дать раньше (например, проекцию на T3)?

**B6. Multi-tenant инфраструктура для T3**
Если на T3 экзокортекс cloud-managed, то платформе нужно:
- Хранить ЦД (цифровой двойник) для каждого пользователя (уже есть в Neon)
- Хранить personal-заметки, прогресс, настройки
- Возможно: personal memory/ как часть ЦД (не файлы, а записи в БД)
- Возможно: personal CLAUDE.md как шаблон в ЦД, подставляемый в system prompt бота

Вопросы:
- Сколько стоит хранение одного T3-пользователя (Neon rows + Claude tokens/мес)?
- При 1000 T3-пользователей — нужна ли шардинг-стратегия для ЦД, или одна Neon DB справится?
- Как изолировать данные пользователей (row-level security в Neon? Отдельные schemas?)?

**B7. Мотивация перехода между тирами: что триггерит апгрейд?**
Каждый переход должен быть мотивирован («пользователь упёрся в ограничение текущего тира»). Вопросы:
- T1→T2: какой конкретный момент? «Бот не может ответить на вопрос по программе» → «Подпишись на БР для доступа к полной программе»?
- T2→T3: «Хочу, чтобы бот помнил мой контекст между сессиями» → «Включи персонализацию (ЦД)»?
- T3→T4: «Хочу создавать свои знания, а не только потреблять» → «Разверни полную IWE»?
- Нужны ли автоматические триггеры (система предлагает апгрейд, когда видит определённое поведение) или только ручные?

**B8. Обратная совместимость: T4-пользователь продолжает использовать бот**
T4-пользователь имеет полную IWE (Claude Code + агенты + Git). Но он по-прежнему может взаимодействовать с ботом (заметки, команды, планы). Вопросы:
- Как синхронизировать состояние между Claude Code (локальный) и бот (облачный)? Через Git push → webhook → бот обновляет ЦД? Или ЦД = Git (единый source-of-truth)?
- Если пользователь редактирует CLAUDE.md локально — как это влияет на system prompt бота? Бот тянет personal CLAUDE.md из GitHub при каждом запросе?
- Конфликты: пользователь написал заметку в боте И в Claude Code одновременно. Кто побеждает?

---

## C. Общие архитектурные вопросы

### Контекст

Вопросы, которые пересекают A (бот) и B (IWE) — касаются платформы в целом.

### Вопросы

**C1. Топология деплоя: Railway + CF Workers + Neon + GitHub — правильная ли связка?**
Сейчас: Railway (бот, Python), Cloudflare Workers (3 MCP-сервера, TypeScript), Neon (PostgreSQL + pgvector), GitHub (код + шаблон экзокортекса). При росте до 10K пользователей — это правильная топология или нужна консолидация? Конкретно:
- Railway и CF Workers в разных регионах (Amsterdam vs. auto) — будет ли latency между ними проблемой?
- Нужен ли единый облачный провайдер (например, всё на Fly.io или всё на CF)?
- Vendor lock-in: насколько мы привязаны к каждому из провайдеров?

> → **Решение:** Топология **Railway + CF Workers + Neon + GitHub** — правильная. Позже заменить **Railway → Kubernetes** (когда потребуется больше контроля и масштаба).

**C2. MCP как архитектурный слой: 3 сервера — это правильные границы?**
Сейчас: knowledge-mcp (Pack + онтология), guides-mcp (руководства Aisystant), digital-twin-mcp (ЦД пользователя). Вопросы:
- При добавлении новых Pack (сейчас 4, план — 10+) — knowledge-mcp справится или нужна шардинг-стратегия?
- Нужен ли отдельный MCP для assessment / progress tracking?
- Как версионировать MCP API при обновлении схемы данных (Pack меняется → MCP меняется → бот ломается)?

> → **Решение:** **knowledge-mcp** (Pack + онтология) — **один публичный для всех**. **digital-twin-mcp** — для всего закрытого (персональные данные). Возможно, для личного репо «Моё стратегирование» будет отдельный MCP.

**C3. Pack → Downstream синхронизация: cron-скрипты vs. event-driven**
Сейчас синхронизация через launchd-скрипты (pack-project, template-sync, code-scan, mcp-reindex) — запускаются по расписанию. При масштабировании:
- Нужен ли event-driven подход (Pack commit → webhook → rebuild downstream)?
- GitHub Actions vs. собственный event bus?
- Как гарантировать консистентность (Pack обновился, но MCP ещё не переиндексировал)?

> → **Решение:** **Event-driven** через **GitHub Actions**. Pack commit → webhook → reindex MCP. Заменяет текущий cron-подход.

**C4. Стоимость владения: cost model по тирам**
Для бизнес-модели нужно понимать unit economics. Вопросы:
- Сколько стоит 1 пользователь/мес на каждом тире (T1: бот only, T2: + программы, T3: + ЦД + Claude, T4: + полная IWE)?
- Claude API = 96% стоимости (из A3). Остальная инфраструктура (Neon, Railway, CF) — сколько на пользователя?
- Break-even point при подписке через Telegram Stars: сколько T3-пользователей нужно, чтобы покрыть инфраструктуру?

**C5. Изоляция данных: единая стратегия для всех слоёв**
В B6 упоминается row-level security в Neon. Но изоляция нужна шире:
- Neon: RLS vs. отдельные schemas vs. отдельные DB?
- MCP: digital-twin-mcp уже фильтрует по user_id — достаточно ли этого при multi-tenant?
- Бот: session isolation (один пользователь не видит данные другого) — где потенциальные утечки?
- Логи и мониторинг: как исключить PII из Grafana / error_logs?

> → **Решение:** **Требуется отдельное решение.** Не ясно, как защититься от того, чтобы один пользователь не получал данные другого. Открытый вопрос.

**C6. Управление платформенными репозиториями: 25 репо — масштабируемо ли?**
Сейчас ~25 репозиториев платформы (4 PACK, 8 DS, 2 Framework, 1 Foundation, 2 Format, MCP и др.). При развитии (10+ Pack, новые агенты, новые MCP) число может удвоиться. Вопросы:
- Нужен ли monorepo для связанных компонентов (например, все MCP в одном репо, все агенты в одном)?
- Cross-repo зависимости (Pack → MCP → бот) — как управлять версиями? Git submodules, npm packages, или текущий подход (скрипты + conventions)?
- CI/CD: сейчас GitHub Actions per repo. При 50 репо — нужна ли централизованная CI-стратегия?
