#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ø–æ —Ö—Ä–∞–Ω–∏–ª–∏—â—É –∑–Ω–∞–Ω–∏–π.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 ops/build_report.py --report architecture-snapshot
    python3 ops/build_report.py --report content-completeness
    python3 ops/build_report.py --report technical-issues
    python3 ops/build_report.py --report all

    # –° AI-–∞–Ω–∞–ª–∏–∑–æ–º (—Ç—Ä–µ–±—É–µ—Ç ANTHROPIC_API_KEY)
    python3 ops/build_report.py --report terminology --ai-analysis
    python3 ops/build_report.py --report recommendations --ai-analysis

–¢–∏–ø—ã –æ—Ç—á—ë—Ç–æ–≤:
    architecture-snapshot   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    content-completeness    - –°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è
    technical-issues        - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    terminology             - –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (AI)
    recommendations         - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é (AI)
    links-map               - –ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    all                     - –í—Å–µ –æ—Ç—á—ë—Ç—ã

–§–ª–∞–≥–∏:
    --ai-analysis           - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Claude –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞
    --dry-run               - –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã, —Ç–æ–ª—å–∫–æ –≤—ã–≤–µ—Å—Ç–∏
    --output, -o            - –£–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
"""

import os
import re
import sys
import json
import yaml
import argparse
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Any

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
try:
    from dotenv import load_dotenv
    load_dotenv()  # –ó–∞–≥—Ä—É–∂–∞–µ—Ç .env –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
except ImportError:
    pass  # python-dotenv –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ AI-–∞–Ω–∞–ª–∏–∑–∞
try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
CONTENT_DIR = Path("content")
REPORTS_DIR = CONTENT_DIR / "0. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ" / "0.4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç—á—ë—Ç—ã –ò–ò"

# –°–µ–º–µ–π—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ F0-F9
FAMILIES = {
    "F0": {"name": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "level": "–ú–µ—Ç–∞—Å–∏—Å—Ç–µ–º–∞", "role": "-", "section": "0"},
    "F1": {"name": "–í–∏–¥–µ–Ω–∏–µ –∏ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ", "level": "–ú–∏—Ä", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "1.1"},
    "F2": {"name": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã", "level": "–ú–∏—Ä", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "1.2"},
    "F3": {"name": "–†–µ–ø—É—Ç–∞—Ü–∏—è –∏ –ø–∞—Ä—Ç–Ω—ë—Ä—Å—Ç–≤–∞", "level": "–ú–∏—Ä", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "1.3"},
    "F4": {"name": "–¶–µ–Ω–Ω–æ—Å—Ç—å –∏ –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª–∏", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "2.1"},
    "F5": {"name": "–ú–æ–¥–µ–ª—å –∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "2.2"},
    "F6": {"name": "–ü—É—Ç—å –∏ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "2.3"},
    "F7": {"name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "3.1"},
    "F8": {"name": "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "3.2"},
    "F9": {"name": "–ö–æ–º–∞–Ω–¥–∞ –∏ —Å–ª—É–∂–±—ã", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "3.3"},
}

# –ú–æ–¥–µ–ª—å –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞
AI_MODEL = "claude-sonnet-4-20250514"
AI_MAX_TOKENS = 4096


class AIAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Claude."""

    def __init__(self):
        if not HAS_ANTHROPIC:
            raise RuntimeError(
                "–î–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ anthropic.\n"
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install anthropic"
            )

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise RuntimeError(
                "–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è ANTHROPIC_API_KEY.\n"
                "–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞ https://console.anthropic.com/"
            )

        self.client = anthropic.Anthropic(api_key=api_key)

    def analyze(self, prompt: str, context: str, max_tokens: int = AI_MAX_TOKENS) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ AI-–∞–Ω–∞–ª–∏–∑–∞."""
        try:
            response = self.client.messages.create(
                model=AI_MODEL,
                max_tokens=max_tokens,
                messages=[
                    {
                        "role": "user",
                        "content": f"{prompt}\n\n---\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"
                    }
                ]
            )
            return response.content[0].text
        except Exception as e:
            return f"*–û—à–∏–±–∫–∞ AI-–∞–Ω–∞–ª–∏–∑–∞: {e}*"

    def analyze_terminology(self, documents: List['Document']) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏."""
        # –°–æ–±–∏—Ä–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        terms_context = self._extract_terms_context(documents)

        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π.

–ó–∞–¥–∞—á–∏:
1. –ù–∞–π–¥–∏ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
2. –ù–∞–π–¥–∏ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
3. –ù–∞–π–¥–∏ —Å–∏–Ω–æ–Ω–∏–º—ã/–≤–∞—Ä–∏–∞—Ü–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–æ–¥–Ω–æ –ø–æ–Ω—è—Ç–∏–µ - —Ä–∞–∑–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
4. –û—Ü–µ–Ω–∏ –æ–±—â—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –≤ Markdown:

## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°—Ç–∞—Ç—É—Å |
|-----------|------------|--------|
| –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã | N | üü¢ |
| –¢–µ—Ä–º–∏–Ω—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ | N | üü° |
| –¢–µ—Ä–º–∏–Ω—ã —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π | N | üî¥ |

## 1. –¢–µ—Ä–º–∏–Ω—ã —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

### 1.1. [TERM-001] ¬´–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞¬ª
**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:**
- –î–æ–∫—É–º–µ–Ω—Ç A: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 1
- –î–æ–∫—É–º–µ–Ω—Ç B: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 2

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** ...

## 2. –¢–µ—Ä–º–∏–Ω—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ (—Å–∏–Ω–æ–Ω–∏–º—ã)

| –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω | –í–∞—Ä–∏–∞—Ü–∏–∏ | –î–æ–∫—É–º–µ–Ω—Ç—ã |
|-----------------|----------|-----------|
| ... | ... | ... |

## 3. –¢–µ—Ä–º–∏–Ω—ã –±–µ–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

- –¢–µ—Ä–º–∏–Ω 1 (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤: –¥–æ–∫1, –¥–æ–∫2)
- –¢–µ—Ä–º–∏–Ω 2 (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤: –¥–æ–∫3)

## 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏

1. ...
2. ...
"""

        return self.analyze(prompt, terms_context)

    def analyze_recommendations(self, documents: List['Document'], by_family: Dict[str, List['Document']]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_context = self._build_stats_context(documents, by_family)

        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—é.

–ó–∞–¥–∞—á–∏:
1. –û—Ü–µ–Ω–∏ –ø–æ–ª–Ω–æ—Ç—É –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º –ø–æ –º–∞—Ç—Ä–∏—Ü–µ 3√ó3 (–ú–∏—Ä/–°–æ–∑–∏–¥–∞—Ç–µ–ª—å/–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ √ó –ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å/–ò–Ω–∂–µ–Ω–µ—Ä/–ú–µ–Ω–µ–¥–∂–µ—Ä)
2. –ù–∞–π–¥–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
3. –í—ã—è–≤–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
4. –ü—Ä–µ–¥–ª–æ–∂–∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –≤ Markdown:

## Executive Summary

–ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).

## 1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã üî¥

### 1.1. [GAP-001] –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–∞
**–û–ø–∏—Å–∞–Ω–∏–µ:** ...
**–í–ª–∏—è–Ω–∏–µ:** ...
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è:**
- –î–æ–∫—É–º–µ–Ω—Ç 1
- –î–æ–∫—É–º–µ–Ω—Ç 2

## 2. –í–∞–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è üü°

### 2.1. [IMP-001] –ù–∞–∑–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
**–û–ø–∏—Å–∞–Ω–∏–µ:** ...
**–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:**
- [[–î–æ–∫—É–º–µ–Ω—Ç]] ‚Äî —á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å

## 3. –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1 (–∫—Ä–∏—Ç–∏—á–Ω–æ)
1. ...
2. ...

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2 (–≤–∞–∂–Ω–æ)
1. ...
2. ...

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3 (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ)
1. ...

## 4. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª–µ–≤–æ–µ |
|---------|---------|---------|
| ... | ... | ... |
"""

        return self.analyze(prompt, stats_context)

    def _extract_terms_context(self, documents: List['Document']) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        context_parts = []

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ (–≥–ª–æ—Å—Å–∞—Ä–∏–∏, –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏)
        priority_patterns = ["–≥–ª–æ—Å—Å–∞—Ä–∏–π", "—Ç–µ—Ä–º–∏–Ω", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏", "–∫–æ–Ω—Ü–µ–ø—Ü", "–ø–æ–Ω—è—Ç–∏–µ"]

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        priority_docs = []
        other_docs = []

        for doc in documents:
            name_lower = doc.name.lower()
            if any(p in name_lower for p in priority_patterns):
                priority_docs.append(doc)
            else:
                other_docs.append(doc)

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        for doc in priority_docs[:10]:
            context_parts.append(f"### {doc.name}\n\n{doc.body[:3000]}")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        for doc in other_docs[:30]:
            # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "X ‚Äî —ç—Ç–æ Y" –∏–ª–∏ "X: Y")
            definitions = re.findall(
                r'(?:^|\n)([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s]+?)(?:\s*[‚Äî‚Äì-]\s*|\s*:\s*)([^\n]{20,200})',
                doc.body
            )
            if definitions:
                context_parts.append(f"### {doc.name}\n")
                for term, definition in definitions[:5]:
                    context_parts.append(f"- **{term.strip()}**: {definition.strip()}")

        return "\n\n".join(context_parts)[:15000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç

    def _build_stats_context(self, documents: List['Document'], by_family: Dict[str, List['Document']]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        context = f"## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n\n"
        context += f"- –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\n"

        for family, docs in sorted(by_family.items()):
            context += f"- {family}: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"

        context += "\n## –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º\n\n"

        for family, docs in sorted(by_family.items()):
            context += f"### {family}\n"
            for doc in docs[:10]:
                status = "‚úÖ" if not doc.is_empty else "‚ö†Ô∏è –ø—É—Å—Ç–æ–π"
                context += f"- {doc.name} ({status})\n"
            if len(docs) > 10:
                context += f"- ... –∏ –µ—â—ë {len(docs) - 10}\n"
            context += "\n"

        context += "\n## –ü—É—Å—Ç—ã–µ/–Ω–µ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n\n"
        empty_docs = [d for d in documents if d.is_empty]
        for doc in empty_docs[:20]:
            context += f"- {doc.name}\n"

        return context[:15000]


# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞–ø–æ–∫ —Å–µ–º–µ–π—Å—Ç–≤–∞–º
FOLDER_TO_FAMILY = {
    "0. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": "F0",
    "1. –ú–∏—Ä (–ù–∞–¥—Å–∏—Å—Ç–µ–º–∞)": None,  # –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ –ø–æ–¥–ø–∞–ø–∫–µ
    "1.1.": "F1",
    "1.2.": "F2",
    "1.3.": "F3",
    "2. –°–æ–∑–∏–¥–∞—Ç–µ–ª—å (–¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞)": None,
    "2.1.": "F4",
    "2.2.": "F5",
    "2.3.": "F6",
    "3. –≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ —Ä–∞–∑–≤–∏—Ç–∏—è (–°–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è)": None,
    "3.1.": "F7",
    "3.2.": "F8",
    "3.3.": "F9",
}


class Document:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""

    def __init__(self, path: Path):
        self.path = path
        self.relative_path = path.relative_to(CONTENT_DIR) if path.is_relative_to(CONTENT_DIR) else path
        self.name = path.stem
        self.content = ""
        self.frontmatter: Dict[str, Any] = {}
        self.body = ""
        self.wikilinks: List[str] = []
        self.headings: List[Tuple[int, str]] = []
        self.family: Optional[str] = None
        self.size = 0
        self._parse()

    def _parse(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞: frontmatter, –∫–æ–Ω—Ç–µ–Ω—Ç, —Å—Å—ã–ª–∫–∏."""
        try:
            self.content = self.path.read_text(encoding="utf-8")
            self.size = len(self.content)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {self.path}: {e}")
            return

        # –ü–∞—Ä—Å–∏–Ω–≥ frontmatter
        if self.content.startswith("---"):
            parts = self.content.split("---", 2)
            if len(parts) >= 3:
                try:
                    self.frontmatter = yaml.safe_load(parts[1]) or {}
                except yaml.YAMLError:
                    self.frontmatter = {}
                self.body = parts[2].strip()
            else:
                self.body = self.content
        else:
            self.body = self.content

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ wikilinks
        self.wikilinks = re.findall(r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]', self.body)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.headings = [(len(m.group(1)), m.group(2))
                         for m in re.finditer(r'^(#{1,6})\s+(.+)$', self.body, re.MULTILINE)]

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–∞
        self.family = self._detect_family()

    def _detect_family(self) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –ø—É—Ç–∏ –∏ frontmatter."""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: frontmatter
        if "family" in self.frontmatter:
            return self.frontmatter["family"]

        # –ü–æ –ø—É—Ç–∏
        path_str = str(self.relative_path)
        path_segments = path_str.split("/")

        for pattern, family in FOLDER_TO_FAMILY.items():
            if pattern in path_str:
                if family:
                    return family
                # –î–ª—è –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø–∞–ø–æ–∫ —Å–º–æ—Ç—Ä–∏–º –ø–æ–¥–ø–∞–ø–∫—É
                # –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞—Ç—Ç–µ—Ä–Ω –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –í –ù–ê–ß–ê–õ–ï —Å–µ–≥–º–µ–Ω—Ç–∞ –ø—É—Ç–∏,
                # –∞ –Ω–µ –ø—Ä–æ—Å—Ç–æ –∫–∞–∫ –ø–æ–¥—Å—Ç—Ä–æ–∫–∞ (–∏–Ω–∞—á–µ "1.1.3." –æ—à–∏–±–æ—á–Ω–æ –º–∞—Ç—á–∏—Ç—Å—è –Ω–∞ "1.3.")
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –≤ –æ–±—Ä–∞—Ç–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ –¥–ª—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞ (3.3. > 3.2. > 3.1.)
                sorted_patterns = sorted(FOLDER_TO_FAMILY.items(), key=lambda x: x[0], reverse=True)
                for sub_pattern, sub_family in sorted_patterns:
                    if sub_family:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–∞–∫–æ–π-—Ç–æ —Å–µ–≥–º–µ–Ω—Ç –ø—É—Ç–∏ –ù–ê–ß–ò–ù–ê–ï–¢–°–Ø —Å –ø–∞—Ç—Ç–µ—Ä–Ω–∞
                        for segment in path_segments:
                            if segment.startswith(sub_pattern):
                                return sub_family

        return None

    @property
    def is_empty(self) -> bool:
        """–î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç < 200 —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ TODO."""
        body_clean = re.sub(r'<!--.*?-->', '', self.body, flags=re.DOTALL)
        body_clean = re.sub(r'TODO|FIXME', '', body_clean, flags=re.IGNORECASE)
        return len(body_clean.strip()) < 200

    @property
    def is_full(self) -> bool:
        """
        –î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ª–Ω—ã–º —Å–æ–≥–ª–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω—ã–º –¢–ó:
        - >500 —Å–ª–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        - –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–ª–æ–∂–µ–Ω–∏–µ (‚â•3 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
        - –ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã (—á–∏—Å–ª–∞/–º–µ—Ç—Ä–∏–∫–∏) –ò–õ–ò –¥–∏–∞–≥—Ä–∞–º–º—ã/—Ç–∞–±–ª–∏—Ü—ã
        - –ù–µ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–æ–π (<10% TODO/TBD)
        - –ï—Å—Ç—å —Å–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        """
        # –ö—Ä–∏—Ç–µ—Ä–∏–π 1: –û–±—ä–µ–º >500 —Å–ª–æ–≤
        word_count = len(self.body.split())
        if word_count < 500:
            return False

        # –ö—Ä–∏—Ç–µ—Ä–∏–π 2: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ (‚â•3 –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤)
        if len(self.headings) < 3:
            return False

        # –ö—Ä–∏—Ç–µ—Ä–∏–π 3: –ï—Å—Ç—å –ø—Ä–∏–º–µ—Ä—ã (—á–∏—Å–ª–∞) –ò–õ–ò –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è (—Ç–∞–±–ª–∏—Ü—ã/–¥–∏–∞–≥—Ä–∞–º–º—ã)
        has_numbers = bool(re.search(r'\d+[.,]?\d*\s*(%|—Ä—É–±|USD|—Å–ª–æ–≤|–¥–æ–∫—É–º–µ–Ω—Ç–æ–≤|—á–µ–ª–æ–≤–µ–∫)', self.body))
        has_tables = bool(re.search(r'\|.*\|.*\|', self.body))  # Markdown —Ç–∞–±–ª–∏—Ü—ã
        has_diagrams = bool(re.search(r'```(mermaid|plantuml|graphviz)', self.body, re.IGNORECASE))
        has_examples = has_numbers or has_tables or has_diagrams
        if not has_examples:
            return False

        # –ö—Ä–∏—Ç–µ—Ä–∏–π 4: –ù–µ –∑–∞–≥–ª—É—à–∫–∞ (<10% TODO/TBD)
        todo_count = len(re.findall(r'TODO|TBD|FIXME|\.\.\.', self.body, re.IGNORECASE))
        total_lines = len(self.body.split('\n'))
        if total_lines > 0 and (todo_count / total_lines) > 0.1:
            return False

        # –ö—Ä–∏—Ç–µ—Ä–∏–π 5: –ï—Å—Ç—å —Å–≤—è–∑–∏ (wikilinks)
        if len(self.wikilinks) == 0:
            return False

        return True

    @property
    def status(self) -> str:
        return self.frontmatter.get("status", "unknown")

    @property
    def doc_type(self) -> str:
        return self.frontmatter.get("type", "unknown")


class ReportGenerator:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤."""

    def __init__(self, ai_analyzer: Optional[AIAnalyzer] = None):
        self.documents: List[Document] = []
        self.by_family: Dict[str, List[Document]] = defaultdict(list)
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.git_hash = self._get_git_hash()
        self.ai_analyzer = ai_analyzer

    def _get_git_hash(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ git commit hash."""
        try:
            import subprocess
            result = subprocess.run(
                ["git", "rev-parse", "--short", "HEAD"],
                capture_output=True, text=True, check=True
            )
            return result.stdout.strip()
        except:
            return "unknown"

    def scan_documents(self):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""
        print("üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for md_file in CONTENT_DIR.rglob("*.md"):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
            if any(skip in str(md_file) for skip in [".obsidian", "node_modules", ".git"]):
                continue

            doc = Document(md_file)
            self.documents.append(doc)

            if doc.family:
                self.by_family[doc.family].append(doc)

        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        for family, docs in sorted(self.by_family.items()):
            print(f"   {family}: {len(docs)}")

    def generate(self, report_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞."""
        generators = {
            "architecture-snapshot": self._generate_architecture_snapshot,
            "content-completeness": self._generate_content_completeness,
            "technical-issues": self._generate_technical_issues,
            "terminology": self._generate_terminology,
            "recommendations": self._generate_recommendations,
            "links-map": self._generate_links_map,
        }

        if report_type not in generators:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –æ—Ç—á—ë—Ç–∞: {report_type}")

        return generators[report_type]()

    def _header(self, title: str, extra: str = "") -> str:
        """–ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞."""
        return f"""# {title}

> –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: {self.timestamp}
> –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}
> –í–µ—Ä—Å–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {self.git_hash}
{extra}
---

"""

    # ==================== –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ô –°–õ–ï–ü–û–ö ====================

    def _generate_architecture_snapshot(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞'."""
        report = self._header(
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞",
            "\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞**: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–æ–≥–∏–∫–µ –æ–ø–∏—Å–∞–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –æ—Ç —Ü–µ–ª–µ–π –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.\n"
        )

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞–∑–¥–µ–ª–æ–≤
        report += self._architecture_heatmap()

        # –†–∞–∑–¥–µ–ª—ã
        report += self._architecture_section_1_mission()
        report += self._architecture_section_2_personas()
        report += self._architecture_section_3_goals()
        report += self._architecture_section_4_creator()
        report += self._architecture_section_5_functioning()
        report += self._architecture_section_6_platform()
        report += self._architecture_section_7_data()
        report += self._architecture_section_8_epistemic()
        report += self._architecture_section_9_economy()
        report += self._architecture_section_10_quality()
        report += self._architecture_section_11_metrics()
        report += self._architecture_section_12_stats()
        report += self._architecture_section_13_links()

        return report

    def _check_main_question_coverage(self, family_id: str, docs: list, main_question: str) -> bool:
        """
        –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —Ä–∞—Å–∫—Ä—ã—Ç –ª–∏ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å–µ–º–µ–π—Å—Ç–≤–∞ –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö.

        –ö—Ä–∏—Ç–µ—Ä–∏–π: —Ö–æ—Ç—è –±—ã –æ–¥–∏–Ω –ø–æ–ª–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å
        (>3 –∞–±–∑–∞—Ü–µ–≤ —Å –ø—Ä–∏–º–µ—Ä–∞–º–∏ –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–π)
        """
        if not main_question or not docs:
            return False

        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        question_keywords = set(re.findall(r'\w+', main_question.lower()))
        question_keywords -= {'–∫–∞–∫', '—á—Ç–æ', '–∑–∞—á–µ–º', '–¥–ª—è', '–∫–æ–≥–æ', '—ç—Ç–æ', '—É—Å—Ç—Ä–æ–µ–Ω', '—É—Å—Ç—Ä–æ–µ–Ω–∞', '—É—Å—Ç—Ä–æ–µ–Ω–æ'}

        for doc in docs:
            if not doc.is_full:
                continue

            # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –∞–±–∑–∞—Ü—ã
            paragraphs = [p.strip() for p in doc.body.split('\n\n') if len(p.strip()) > 100]
            if len(paragraphs) < 3:
                continue

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –≤–æ–ø—Ä–æ—Å–∞ –≤ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏–∏
            body_lower = doc.body.lower()
            matches = sum(1 for keyword in question_keywords if keyword in body_lower)

            # –ï—Å–ª–∏ —Ö–æ—Ç—è –±—ã 50% –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –Ω–∞–π–¥–µ–Ω—ã –∏ –µ—Å—Ç—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞
            if matches >= len(question_keywords) * 0.5 and len(doc.headings) >= 3:
                return True

        return False

    def _architecture_heatmap(self) -> str:
        """–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º F0-F9 —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ 0.4.1."""

        # –¢–∏–ø–∏—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–µ–º–µ–π—Å—Ç–≤—É (–∏–∑ –¢–ó)
        typical_docs = {
            "F0": ["–º–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≥–ª–æ—Å—Å–∞—Ä–∏–π", "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü"],
            "F1": ["–º–∞–Ω–∏—Ñ–µ—Å—Ç", "–ø—Ä–æ–±–ª–µ–º", "—Ü–µ–ª–µ–≤", "jtbd", "–∞—É–¥–∏—Ç–æ—Ä"],
            "F2": ["–∫–æ–Ω—Ü–µ–ø—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "—Å—Ü–µ–Ω–∞—Ä–∏", "–∫–æ–Ω—Ç–µ–∫—Å—Ç", "–¥–∏–∞–≥—Ä–∞–º–º"],
            "F3": ["–∫–æ–º–º—É–Ω–∏–∫–∞—Ü", "–ø–∞—Ä—Ç–Ω—ë—Ä", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "compliance"],
            "F4": ["—Ü–µ–Ω–Ω–æ—Å—Ç–Ω", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü", "–±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª", "–æ—Ñ—Ñ–µ—Ä"],
            "F5": ["—Å–æ–∑–∏–¥–∞—Ç–µ–ª", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü", "–º–∞—Å—Ç–µ—Ä—Å—Ç–≤", "–º–æ–¥–µ–ª—å"],
            "F6": ["–æ–Ω–±–æ—Ä–¥–∏–Ω–≥", "–º–∞—Ä—à—Ä—É—Ç", "–¥–µ–∫–∞–Ω–∞—Ç", "–º–µ—Ç—Ä–∏–∫"],
            "F7": ["—ç–∫–æ–Ω–æ–º–∏–∫", "—Ç–æ–∫–µ–Ω–æ–º–∏–∫", "–∏–Ω–≤–µ—Å—Ç–∏—Ü"],
            "F8": ["–ø–ª–∞—Ç—Ñ–æ—Ä–º", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "—Å–∏—Å—Ç–µ–º", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"],
            "F9": ["—Ä–æ–ª", "—Ä–∏—Ç–º", "—Å–ª—É–∂–±", "–∫–æ–º–∞–Ω–¥", "—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü"],
        }

        # –ì–ª–∞–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–µ–º–µ–π—Å—Ç–≤ (–∏–∑ –ú–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1)
        main_questions = {
            "F0": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª–∞ –∏ –æ–Ω—Ç–æ–ª–æ–≥–∏—è?",
            "F1": "–ó–∞—á–µ–º –º–∏—Ä—É —ç—Ç–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞?",
            "F2": "–ö–∞–∫ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –º–∏—Ä?",
            "F3": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º —Å –≤–Ω–µ—à–Ω–∏–º –º–∏—Ä–æ–º?",
            "F4": "–ö–∞–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?",
            "F5": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?",
            "F6": "–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—è?",
            "F7": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã?",
            "F8": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞?",
            "F9": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–º–∞–Ω–¥–∞?",
        }

        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
        heatmap += "| –°–µ–º–µ–π—Å—Ç–≤–æ | –ù–∞–∑–≤–∞–Ω–∏–µ | –°—Ç–∞—Ç—É—Å | –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n"
        heatmap += "|-----------|----------|--------|------------|-------------|\n"

        status_counts = {"üü¢": 0, "üü°": 0, "üî¥": 0}

        for family_id in ["F0", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            family = FAMILIES[family_id]
            docs = self.by_family.get(family_id, [])
            count = len(docs)

            # –ê–Ω–∞–ª–∏–∑ –°–û–î–ï–†–ñ–ê–ù–ò–Ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º—É –¢–ó 0.4.1
            # –ö—Ä–∏—Ç–µ—Ä–∏–∏ –ø–æ–ª–Ω–æ—Ç—ã: >500 —Å–ª–æ–≤ + —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + –ø—Ä–∏–º–µ—Ä—ã + –¥–∏–∞–≥—Ä–∞–º–º—ã + —Å–≤—è–∑–∏
            typical_patterns = typical_docs.get(family_id, [])
            full_docs_count = 0  # –î–æ–∫—É–º–µ–Ω—Ç—ã, —É–¥–æ–≤–ª–µ—Ç–≤–æ—Ä—è—é—â–∏–µ is_full
            typical_full_docs = 0  # –¢–∏–ø–∏—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–ª–Ω—ã–µ

            for doc in docs:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–æ–ª–Ω–æ—Ç—É –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–∏—Å–ø–æ–ª—å–∑—É–µ–º is_full property)
                if doc.is_full:
                    full_docs_count += 1

                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç —Ç–∏–ø–∏—á–Ω—ã–º –¥–ª—è —Å–µ–º–µ–π—Å—Ç–≤–∞
                    doc_name_lower = doc.name.lower()
                    for pattern in typical_patterns:
                        if pattern in doc_name_lower:
                            typical_full_docs += 1
                            break  # –û–¥–∏–Ω –¥–æ–∫—É–º–µ–Ω—Ç –º–æ–∂–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–æ–≤–∞—Ç—å —Ç–æ–ª—å–∫–æ –æ–¥–Ω–æ–º—É –ø–∞—Ç—Ç–µ—Ä–Ω—É

            # –û—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –û–ë–ù–û–í–õ–ï–ù–ù–û–ú–£ –¢–ó (–ø. 2.1)
            # –ì–ª–∞–≤–Ω—ã–π –∫—Ä–∏—Ç–µ—Ä–∏–π: –ø—Ä–æ—Ü–µ–Ω—Ç –ü–û–õ–ù–´–• —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            # üü¢ –ü–æ–ª–Ω—ã–π: ‚â•80% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ (>500 —Å–ª–æ–≤ + —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + –ø—Ä–∏–º–µ—Ä—ã + –¥–∏–∞–≥—Ä–∞–º–º—ã + —Å–≤—è–∑–∏)
            # üü° –ß–∞—Å—Ç–∏—á–Ω—ã–π: 50-79% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ
            # üî¥ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π (–ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ): <50% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ

            typical_count = len(typical_patterns)
            full_ratio = full_docs_count / count if count > 0 else 0
            typical_full_ratio = typical_full_docs / typical_count if typical_count > 0 else 0

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å–µ–º–µ–π—Å—Ç–≤–∞ (–∞–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è)
            main_question_covered = self._check_main_question_coverage(family_id, docs, main_questions.get(family_id, ""))

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ—Ü–µ–Ω—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–≤—è–∑—è–º–∏
            docs_with_links = sum(1 for doc in docs if len(doc.wikilinks) > 0)
            links_ratio = docs_with_links / count if count > 0 else 0

            # –ñ–ï–°–¢–ö–ò–ï –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó
            if (typical_full_ratio >= 0.8 and
                full_ratio >= 0.8 and
                main_question_covered and
                links_ratio >= 0.7):
                status = "üü¢"
                comment = f"{int(full_ratio*100)}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ, –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Ä–∞—Å–∫—Ä—ã—Ç"
            elif (typical_full_ratio >= 0.5 and
                  full_ratio >= 0.5):
                status = "üü°"
                comment = f"{int(full_ratio*100)}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ"
            else:
                # –ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ üî¥
                status = "üî¥"
                if count == 0:
                    comment = "–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
                elif full_docs_count == 0:
                    comment = "–ù–µ—Ç –ø–æ–ª–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (—Ç–æ–ª—å–∫–æ –∑–∞–≥–ª—É—à–∫–∏/TODO)"
                else:
                    comment = f"–¢–æ–ª—å–∫–æ {int(full_ratio*100)}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ (—Ç—Ä–µ–±—É–µ—Ç—Å—è ‚â•80%)"

            status_counts[status] += 1
            heatmap += f"| {family_id} | {family['name']} | {status} | {count} | {comment} |\n"

        heatmap += f"\n**–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å:** üü¢ {status_counts['üü¢']} | üü° {status_counts['üü°']} | üî¥ {status_counts['üî¥']}\n"

        return heatmap + "\n---\n\n"

    def _architecture_section_1_mission(self) -> str:
        """–†–∞–∑–¥–µ–ª 1: –ó–∞—á–µ–º, –¥–ª—è –∫–æ–≥–æ, —á—Ç–æ –º—ã –¥–µ–ª–∞–µ–º."""
        section = "## 1. –ó–∞—á–µ–º, –¥–ª—è –∫–æ–≥–æ, —á—Ç–æ –º—ã –¥–µ–ª–∞–µ–º\n\n"

        # –ò—â–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç
        manifesto = self._find_doc_by_pattern("–ú–∞–Ω–∏—Ñ–µ—Å—Ç")
        if manifesto:
            section += "### 1.1. –ú–∏—Å—Å–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã\n\n"
            section += self._extract_summary(manifesto, max_sentences=3)
            section += "\n\n"

        section += "### 1.2. –¢—Ä–∏ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–∞ (–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π)\n\n"
        section += "- **–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å** ‚Äî –ª—é–±–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å–ø–æ—Å–æ–±–µ–Ω –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏\n"
        section += "- **–°–∫–≤–æ–∑–Ω–∞—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å** ‚Äî –æ—Ç –ª–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–π –¥–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –µ–¥–∏–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞\n"
        section += "- **–î–∏–¥–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å** ‚Äî –∫–∞–∂–¥—ã–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤–æ—é –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—É—é –¥–æ—Ä–æ–∂–∫—É\n\n"

        section += "### 1.3. –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏\n\n"
        section += "- C4+ADR –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è\n"
        section += "- –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n"
        section += "- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ Method / MethodDescription / Work\n\n"

        sources = [d.name for d in self.by_family.get("F1", [])[:3]]
        section += f"**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** {', '.join(f'[[{s}]]' for s in sources)}\n\n"

        return section + "---\n\n"

    def _architecture_section_2_personas(self) -> str:
        """–†–∞–∑–¥–µ–ª 2: –ü–µ—Ä—Å–æ–Ω—ã –∏ —Ä–æ–ª–∏."""
        section = "## 2. –ü–µ—Ä—Å–æ–Ω—ã –∏ —Ä–æ–ª–∏\n\n"

        section += "### 2.1. –¶–µ–ª–µ–≤—ã–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏\n\n"
        ta_doc = self._find_doc_by_pattern("–¶–µ–ª–µ–≤—ã–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏")
        if ta_doc:
            section += self._extract_summary(ta_doc, max_sentences=3)
        else:
            section += "*–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω*\n"
        section += "\n\n"

        section += "### 2.2. –†–æ–ª–∏ –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ\n\n"
        section += "- –£—á–µ–Ω–∏–∫/–°—Ç–∞–∂—ë—Ä\n"
        section += "- –°–æ–∑–¥–∞—Ç–µ–ª—å/–ú–µ—Ç–æ–¥–∏—Å—Ç\n"
        section += "- –ö—É—Ä–∞—Ç–æ—Ä/–ú–µ–Ω—Ç–æ—Ä\n"
        section += "- –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- –û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä/–û—É–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞\n"
        section += "- –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç\n\n"

        return section + "---\n\n"

    def _architecture_section_3_goals(self) -> str:
        """–†–∞–∑–¥–µ–ª 3: –ü—Ä–æ–±–ª–µ–º—ã, –≥–∏–ø–æ—Ç–µ–∑—ã –∏ —Ü–µ–ª–∏."""
        section = "## 3. –ü—Ä–æ–±–ª–µ–º—ã, –≥–∏–ø–æ—Ç–µ–∑—ã –∏ —Ü–µ–ª–∏\n\n"

        section += "### 3.1. –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã\n\n"
        problems_doc = self._find_doc_by_pattern("–ü—Ä–æ–±–ª–µ–º—ã")
        if problems_doc:
            section += self._extract_summary(problems_doc, max_sentences=5)
        else:
            section += "- –°–≤–æ–±–æ–¥–∞ –±–µ–∑ ¬´–≥—Ä–∞–º–º–∞—Ç–∏–∫–∏¬ª –ª–∏—á–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏\n"
            section += "- ¬´–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ¬ª\n"
            section += "- –û–±—É—á–µ–Ω–∏–µ ¬´—Ä—è–¥–æ–º¬ª, –∞ –Ω–µ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞\n"
        section += "\n\n"

        section += "### 3.2. –ì–∏–ø–æ—Ç–µ–∑—ã —Ä–µ—à–µ–Ω–∏—è\n\n"
        hypotheses_doc = self._find_doc_by_pattern("–ì–∏–ø–æ—Ç–µ–∑—ã")
        if hypotheses_doc:
            section += self._extract_summary(hypotheses_doc, max_sentences=5)
        section += "\n\n"

        section += "### 3.3. –¶–µ–ª–∏ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º\n\n"
        section += "- **2026** ‚Äî –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —Å–±–æ—Ä–∫–∞ —è–¥—Ä–∞\n"
        section += "- **2027‚Äì2030** ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã\n"
        section += "- **–ü–æ—Å–ª–µ 2030** ‚Äî –∫—É–ª—å—Ç—É—Ä–Ω–∞—è –Ω–æ—Ä–º–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_4_creator(self) -> str:
        """–†–∞–∑–¥–µ–ª 4: –¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ - –°–æ–∑–∏–¥–∞—Ç–µ–ª—å."""
        section = "## 4. –¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: –°–æ–∑–∏–¥–∞—Ç–µ–ª—å\n\n"

        section += "### 4.1. –ß—Ç–æ —Ç–∞–∫–æ–µ –°–æ–∑–∏–¥–∞—Ç–µ–ª—å\n\n"
        creator_doc = self._find_doc_by_pattern("–ö–æ–Ω—Ü–µ–ø—Ü–∏—è —Å–æ–∑–∏–¥–∞—Ç–µ–ª—è")
        if creator_doc:
            section += self._extract_summary(creator_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.2. –ú–æ–¥–µ–ª—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏ —É—Ä–æ–≤–Ω–∏ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞\n\n"
        competencies_doc = self._find_doc_by_pattern("–ö–∞—Ä—Ç–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π|–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏")
        if competencies_doc:
            section += self._extract_summary(competencies_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.3. –¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n\n"
        value_doc = self._find_doc_by_pattern("–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
        if value_doc:
            section += self._extract_summary(value_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.4. –†–æ–ª–µ–≤–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è\n\n"
        section += "–£—á–µ–Ω–∏–∫ ‚Üí –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª ‚Üí –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª ‚Üí –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å ‚Üí –ü—Ä–æ—Å–≤–µ—Ç–∏—Ç–µ–ª—å\n\n"

        return section + "---\n\n"

    def _architecture_section_5_functioning(self) -> str:
        """–†–∞–∑–¥–µ–ª 5: –§—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã."""
        section = "## 5. –§—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã\n\n"

        section += "### 5.1. –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã (—Å–∫–≤–æ–∑–Ω—ã–µ —Ü–∏–∫–ª—ã)\n\n"
        section += "- **C1.** –û–Ω–±–æ—Ä–¥–∏–Ω–≥ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–æ–π–Ω–∏–∫–∞\n"
        section += "- **C2.** –û–±—É—á–µ–Ω–∏–µ ‚Üí –ê—Ä—Ç–µ—Ñ–∞–∫—Ç ‚Üí –ü–µ—Ä–µ–Ω–æ—Å\n"
        section += "- **C3.** –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤\n"
        section += "- **C4.** –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- **C5.** –ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ (–ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –ø—Ä–æ–¥–∞–∂–∏)\n"
        section += "- **C6.** –¢–æ–≤–∞—Ä–æ–æ–±–º–µ–Ω –∏ —Ä–∞—Å—á—ë—Ç—ã (—Ñ–∏–∞—Ç + —Ç–æ–∫–µ–Ω)\n"
        section += "- **C7.** –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞ (—Ç–æ–∫–µ–Ω)\n"
        section += "- **C8.** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ —Ñ—Ä–æ–Ω—Ç–∏—Ä–∞\n"
        section += "- **C9.** –ù–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ\n"
        section += "- **C10.** –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞\n\n"

        section += "### 5.2. –ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–æ–ª–µ–π\n\n"
        conops_doc = self._find_doc_by_pattern("–ö–æ–Ω—Ü–µ–ø—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        if conops_doc:
            section += self._extract_summary(conops_doc, max_sentences=5)
        section += "\n\n"

        return section + "---\n\n"

    def _architecture_section_6_platform(self) -> str:
        """–†–∞–∑–¥–µ–ª 6: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ò–ò-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã."""
        section = "## 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ò–ò-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã\n\n"

        section += "### 6.1. –ö–∞—Ä—Ç–∞ –ø–æ–¥—Å–∏—Å—Ç–µ–º\n\n"

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–¥—Å–∏—Å—Ç–µ–º
        subsystems = [d for d in self.by_family.get("F8", [])
                      if any(kw in d.name.lower() for kw in ["–ø–æ–¥—Å–∏—Å—Ç–µ–º", "—Å–∏—Å—Ç–µ–º–∞", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞"])]

        if subsystems:
            section += "| ‚Ññ | –ü–æ–¥—Å–∏—Å—Ç–µ–º–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |\n"
            section += "|---|------------|----------|\n"
            for i, doc in enumerate(subsystems[:10], 1):
                desc = self._extract_first_sentence(doc)
                section += f"| {i} | [[{doc.name}]] | {desc[:80]}... |\n"
        section += "\n"

        section += "### 6.2. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è –û–°: –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n\n"
        section += "- **Reasoning Core** ‚Äî –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n"
        section += "- **Memory Store** ‚Äî —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–Ω–∞–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
        section += "- **Tool/Action Interface** ‚Äî –¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (Apps SDK)\n"
        section += "- **Goal Manager** ‚Äî —Ü–µ–ª–∏, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –±—é–¥–∂–µ—Ç—ã\n"
        section += "- **Dialogue Layer** ‚Äî –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\n\n"

        section += "### 6.3. –°—Ç–∞–¥–∏–∏ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–≥–µ–Ω—Ç–∞\n\n"
        section += "1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** ‚Äî —Ü–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç\n"
        section += "2. **–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ** ‚Äî –≤—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–ª–∞–Ω –≤—ã–∑–æ–≤–æ–≤\n"
        section += "3. **–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ** ‚Äî –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
        section += "4. **–û—Ü–µ–Ω–∫–∞** ‚Äî –ø–µ—Ä–µ—Å—á—ë—Ç —Ü–µ–Ω–Ω–æ—Å—Ç–∏, —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_7_data(self) -> str:
        """–†–∞–∑–¥–µ–ª 7: –î–∞–Ω–Ω—ã–µ –∏ —Å—É—â–Ω–æ—Å—Ç–∏."""
        section = "## 7. –î–∞–Ω–Ω—ã–µ –∏ —Å—É—â–Ω–æ—Å—Ç–∏\n\n"

        section += "### 7.1. –°–∫–≤–æ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö\n\n"
        section += "- **Digital Twin** ‚Äî —Ü–µ–ª–∏, –Ω–∞–≤—ã–∫–∏, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ\n"
        section += "- **Epistemic Graph** ‚Äî —ç–ø–∏—Å—Ç–µ–º—ã –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å—ã/—Å–≤—è–∑–∏/–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞\n"
        section += "- **Activity Ledger** ‚Äî –≤—Å–µ —Å–æ–±—ã—Ç–∏—è (–æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–æ–µ–∫—Ç—ã, –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)\n"
        section += "- **Token Ledger** ‚Äî –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è/—Å–ø–∏—Å–∞–Ω–∏—è, –∑–∞–º–æ—Ä–æ–∑–∫–∏, —Ç–∞—Ä–∏—Ñ—ã\n\n"

        section += "### 7.2. –û—Å–Ω–æ–≤–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏\n\n"
        section += "`User`, `DigitalTwin`, `Program`, `Guide/Step/Task`, `Artifact`, "
        section += "`ActionEvent`, `Qualification`, `Episteme`, `Work`, `MethodDescription`, `Evidence`\n\n"

        section += "### 7.3. –ü–æ–ª–∏—Ç–∏–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n\n"
        section += "- **curated/** ‚Äî —Å—Ç—Ä–æ–≥–æ —á–µ—Ä–µ–∑ PR/—Ä–µ–≤—å—é\n"
        section += "- **derived/** ‚Äî —Ç–æ–ª—å–∫–æ –∞–≤—Ç–æ–º–∞—Ç–æ–º (–∞–≥–µ–Ω—Ç—ã/ETL)\n\n"

        return section + "---\n\n"

    def _architecture_section_8_epistemic(self) -> str:
        """–†–∞–∑–¥–µ–ª 8: –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å."""
        section = "## 8. –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞\n\n"

        section += "### 8.1. ESG (Epistemic Status Graph)\n\n"
        section += "**Draft** ‚Üí **PeerChecked** ‚Üí **Accepted** ‚Üí **Superseded**\n\n"

        section += "### 8.2. –°–∏–≥–Ω–∞–ª—ã –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞\n\n"
        section += "- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –∏—Ö –ø–µ—Ä–µ–Ω–æ—Å\n"
        section += "- –†–µ—Ü–µ–Ω–∑–∏–∏ –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–≤\n"
        section += "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏–º–∏\n"
        section += "- –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–æ–≤\n"
        section += "- Evidence-bindings –∫ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º\n\n"

        section += "### 8.3. –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n\n"
        section += "1. –°–±–æ—Ä —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ —Å–æ–±—ã—Ç–∏–π–Ω–æ–π —à–∏–Ω—ã\n"
        section += "2. –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏\n"
        section += "3. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥\n"
        section += "4. –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ (–ø–æ –ø–æ—Ä–æ–≥—É —Ä–∏—Å–∫–∞)\n"
        section += "5. –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏ –ª–æ–≥–∏–∫–∞ ¬´—Å—Ç–∞—Ä–µ–Ω–∏—è¬ª\n\n"

        return section + "---\n\n"

    def _architecture_section_9_economy(self) -> str:
        """–†–∞–∑–¥–µ–ª 9: –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞."""
        section = "## 9. –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞: Proof-of-Impact –∏ —Ç–æ–∫–µ–Ω–æ–º–∏–∫–∞\n\n"

        section += "### 9.1. –ò–¥–µ—è –∏ –ø–∞–π–ø–ª–∞–π–Ω –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è\n\n"
        section += "–°–æ–±—ã—Ç–∏—è ‚Üí Work ‚Üí Episteme ‚Üí Evidence ‚Üí –ù–∞—á–∏—Å–ª–µ–Ω–∏–µ ‚Üí –ó–∞–º–æ—Ä–æ–∑–∫–∞\n\n"

        section += "### 9.2. –ö–∞–∫ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ —Ç—Ä–∞—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω—ã\n\n"
        section += "**–ó–∞—Ä–∞–±–æ—Ç–æ–∫:**\n"
        section += "- –ó–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —É—Ä–æ–∫–∏/—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n"
        section += "- –ó–∞ —Ä–µ–≤—å—é —á—É–∂–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n"
        section += "- –ó–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ö–≤–∞—Ç–æ–º\n"
        section += "- –ó–∞ –≤–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç—ã —Å –∏–∑–º–µ—Ä–∏–º—ã–º —ç—Ñ—Ñ–µ–∫—Ç–æ–º\n\n"
        section += "**–†–∞—Å—Ö–æ–¥:**\n"
        section += "- –î–æ—Å—Ç—É–ø –∫ –ø—Ä–µ–º–∏—É–º-–∫—É—Ä—Å–∞–º\n"
        section += "- –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∫–≤–æ—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤\n"
        section += "- –°–µ—Å—Å–∏–∏ —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞–º–∏\n\n"

        section += "### 9.3. –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –±–∏—Ä–∂–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ treasury\n\n"
        tokenomics_doc = self._find_doc_by_pattern("–¢–æ–∫–µ–Ω–æ–º–∏–∫–∞")
        if tokenomics_doc:
            section += self._extract_summary(tokenomics_doc, max_sentences=3)
        section += "\n\n"

        return section + "---\n\n"

    def _architecture_section_10_quality(self) -> str:
        """–†–∞–∑–¥–µ–ª 10: –ö—É–ª—å—Ç—É—Ä–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞."""
        section = "## 10. –ö—É–ª—å—Ç—É—Ä–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞\n\n"

        section += "### 10.1. –ü—Ä–∏–Ω—Ü–∏–ø—ã –∫–∞—á–µ—Å—Ç–≤–∞\n\n"
        section += "- **–î–µ–ª–∞—Ç—å-–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å-–º–µ—Ä–∏—Ç—å-—É–ª—É—á—à–∞—Ç—å** ‚Äî –Ω–µ–¥–µ–ª—å–Ω—ã–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç—ã –∏ peer-review\n"
        section += "- **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å** ‚Äî –ª–æ–≥–∏, –≤–µ—Ä—Å–∏–∏, –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏\n"
        section += "- **Curated vs Derived** ‚Äî —Ä—É—á–Ω–∞—è –∑–æ–Ω–∞ –æ—Ç–¥–µ–ª–µ–Ω–∞ –æ—Ç –∞–≤—Ç–æ-–∑–æ–Ω—ã\n"
        section += "- **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–∏** ‚Äî Policy-as-code –≤ Apps SDK\n\n"

        section += "### 10.2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π\n\n"
        section += "- **C4 Model** ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã ‚Üí –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n"
        section += "- **ADR** ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí —Ä–µ—à–µ–Ω–∏–µ ‚Üí trade-offs ‚Üí —Å—Å—ã–ª–∫–∏\n\n"

        return section + "---\n\n"

    def _architecture_section_11_metrics(self) -> str:
        """–†–∞–∑–¥–µ–ª 11: –ú–µ—Ç—Ä–∏–∫–∏."""
        section = "## 11. –ú–µ—Ç—Ä–∏–∫–∏\n\n"

        section += "### 11.1. –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n\n"
        section += "- **Time-to-Master (TTM)** ‚Äî –≤—Ä–µ–º—è –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –º–∞—Å—Ç–µ—Ä–∞\n"
        section += "- **Cost-to-Master (CTM)** ‚Äî —Å—É–º–º–∞—Ä–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã (—Ñ–∏–∞—Ç + —Ç–æ–∫–µ–Ω—ã)\n"
        section += "- **–í—Ä–µ–º—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞** ‚Äî <30 –º–∏–Ω—É—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è\n"
        section += "- **–ö–∞—á–µ—Å—Ç–≤–æ –∏ –ø–µ—Ä–µ–Ω–æ—Å** ‚Äî –¥–æ–ª—è –∑–∞–¥–∞—á —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º (7‚Äì14 –¥–Ω–µ–π)\n"
        section += "- **–≠–∫–æ–Ω–æ–º–∏–∫–∞** ‚Äî MRR/NRR, –æ–±–æ—Ä–æ—Ç —Ç–æ–∫–µ–Ω–∞\n"
        section += "- **–†–µ–ø—É—Ç–∞—Ü–∏—è** ‚Äî –¥–∏–Ω–∞–º–∏–∫–∞ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_12_stats(self) -> str:
        """–†–∞–∑–¥–µ–ª 12: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        section = "## 12. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n\n"

        section += "| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |\n"
        section += "|---------|----------|\n"
        section += f"| –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {len(self.documents)} |\n"

        by_family_str = ", ".join(f"{f}: {len(docs)}" for f, docs in sorted(self.by_family.items()))
        section += f"| –ü–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º | {by_family_str} |\n"

        active = sum(1 for d in self.documents if d.status == "active")
        draft = sum(1 for d in self.documents if d.status == "draft")
        section += f"| –ê–∫—Ç–∏–≤–Ω—ã—Ö | {active} |\n"
        section += f"| –ß–µ—Ä–Ω–æ–≤–∏–∫–æ–≤ | {draft} |\n\n"

        return section + "---\n\n"

    def _architecture_section_13_links(self) -> str:
        """–†–∞–∑–¥–µ–ª 13: –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."""
        return """## 13. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è 0.4]]
- [[–ú–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1]]
"""

    # ==================== –°–û–î–ï–†–ñ–ê–¢–ï–õ–¨–ù–ê–Ø –ü–û–õ–ù–û–¢–ê ====================

    def _generate_content_completeness(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è'."""
        report = self._header("–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è")

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ 3x3
        report += self._completeness_heatmap()

        # Executive Summary
        report += self._completeness_summary()

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º
        report += self._completeness_by_cells()

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SoTA-–º–µ—Ç–æ–¥–æ–≤
        report += self._completeness_sota()

        # –ò–Ω—Ç–µ—Ä–µ—Å—ã —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–æ–≤
        report += self._completeness_stakeholders()

        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        report += self._completeness_gaps()

        # –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        report += self._completeness_links()

        return report

    def _completeness_heatmap(self) -> str:
        """
        –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ 3x3 –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã.
        –°–æ–≥–ª–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º—É –¢–ó 0.4.1, –ø—Ä–æ–≤–µ—Ä—è–µ–º:
        1. –ü—Ä–æ—Ü–µ–Ω—Ç –ü–û–õ–ù–´–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (>500 —Å–ª–æ–≤ + —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ + –ø—Ä–∏–º–µ—Ä—ã + –¥–∏–∞–≥—Ä–∞–º–º—ã)
        2. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SoTA-–º–µ—Ç–æ–¥–æ–≤
        3. –ù–∞–ª–∏—á–∏–µ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
        4. –ê–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç—å (–æ–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞ 6 –º–µ—Å—è—Ü–µ–≤)
        """
        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã\n\n"

        # SoTA-–º–µ—Ç–æ–¥—ã –¥–ª—è –∫–∞–∂–¥–æ–π —Ä–æ–ª–∏ (–∏–∑ –¢–ó)
        sota_methods = {
            "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å": ["jtbd", "business model", "value proposition", "–±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª", "—Ü–µ–Ω–Ω–æ—Å—Ç–Ω"],
            "–ò–Ω–∂–µ–Ω–µ—Ä": ["c4", "adr", "architecture", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "–¥–∏–∞–≥—Ä–∞–º–º"],
            "–ú–µ–Ω–µ–¥–∂–µ—Ä": ["conops", "okr", "–º–µ—Ç—Ä–∏–∫", "–ø—Ä–æ—Ü–µ—Å—Å", "—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü"],
        }

        def cell_status(family_id):
            """–û—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —è—á–µ–π–∫–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –ñ–ï–°–¢–ö–ò–ú –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¢–ó."""
            docs = self.by_family.get(family_id, [])
            if not docs:
                return "üî¥", 0

            # 1. –ü–æ–¥—Å—á–µ—Ç –ü–û–õ–ù–´–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            full_docs = [d for d in docs if d.is_full]
            full_ratio = len(full_docs) / len(docs)

            # 2. –ü—Ä–æ–≤–µ—Ä–∫–∞ SoTA-–º–µ—Ç–æ–¥–æ–≤ –¥–ª—è —Ä–æ–ª–∏
            family = FAMILIES[family_id]
            role = family['role']
            required_methods = sota_methods.get(role, [])
            methods_found = 0
            for doc in full_docs:
                body_lower = doc.body.lower()
                for method in required_methods:
                    if method in body_lower:
                        methods_found += 1
                        break
            sota_ratio = methods_found / len(full_docs) if full_docs else 0

            # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö —Å–≤—è–∑–µ–π
            docs_with_links = sum(1 for d in docs if len(d.wikilinks) > 0)
            links_ratio = docs_with_links / len(docs)

            # 4. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∞–∫—Ç—É–∞–ª—å–Ω–æ—Å—Ç–∏ (–æ–±–Ω–æ–≤–ª–µ–Ω—ã –∑–∞ 6 –º–µ—Å—è—Ü–µ–≤)
            import datetime
            six_months_ago = datetime.datetime.now() - datetime.timedelta(days=180)
            # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: frontmatter.get('updated') –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º created
            recent_docs = 0
            for doc in docs:
                doc_date_str = doc.frontmatter.get('updated') or doc.frontmatter.get('created')
                if doc_date_str:
                    try:
                        doc_date = datetime.datetime.fromisoformat(str(doc_date_str))
                        if doc_date >= six_months_ago:
                            recent_docs += 1
                    except:
                        pass
            actuality_ratio = recent_docs / len(docs) if len(docs) > 0 else 0

            # –ñ–ï–°–¢–ö–ò–ï –∫—Ä–∏—Ç–µ—Ä–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó –ø. 4.2
            # üü¢ –ü–æ–ª–Ω–æ (‚â•90%): –í–°–ï —É—Å–ª–æ–≤–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
            if (full_ratio >= 0.8 and
                sota_ratio >= 0.5 and
                links_ratio >= 0.7 and
                actuality_ratio >= 0.7):
                return "üü¢", int(full_ratio * 100)

            # üü° –ß–∞—Å—Ç–∏—á–Ω–æ (50‚Äì89%): –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∫—Ä–∏—Ç–µ—Ä–∏–µ–≤
            elif (full_ratio >= 0.5 and
                  (sota_ratio >= 0.3 or links_ratio >= 0.5)):
                return "üü°", int(full_ratio * 100)

            # üî¥ –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ (–ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ): <50% –ò–õ–ò –Ω–µ –≤—ã–ø–æ–ª–Ω–µ–Ω—ã –¥—Ä—É–≥–∏–µ –∫—Ä–∏—Ç–µ—Ä–∏–∏
            else:
                return "üî¥", int(full_ratio * 100)

        # –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã
        heatmap += "|                    | –ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å | –ò–Ω–∂–µ–Ω–µ—Ä | –ú–µ–Ω–µ–¥–∂–µ—Ä |\n"
        heatmap += "|                    | (–°–º—ã—Å–ª—ã)        | (–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞) | (–û–ø–µ—Ä–∞—Ü–∏–∏) |\n"
        heatmap += "|--------------------|-----------------|---------|----------|\n"

        f1_status, f1_pct = cell_status('F1')
        f2_status, f2_pct = cell_status('F2')
        f3_status, f3_pct = cell_status('F3')
        heatmap += f"| **–ú–∏—Ä (–ù–∞–¥—Å–∏—Å—Ç–µ–º–∞)** | {f1_status} F1 ({f1_pct}%) | {f2_status} F2 ({f2_pct}%) | {f3_status} F3 ({f3_pct}%) |\n"

        f4_status, f4_pct = cell_status('F4')
        f5_status, f5_pct = cell_status('F5')
        f6_status, f6_pct = cell_status('F6')
        heatmap += f"| **–°–æ–∑–∏–¥–∞—Ç–µ–ª—å (–¶–µ–ª–µ–≤–∞—è)** | {f4_status} F4 ({f4_pct}%) | {f5_status} F5 ({f5_pct}%) | {f6_status} F6 ({f6_pct}%) |\n"

        f7_status, f7_pct = cell_status('F7')
        f8_status, f8_pct = cell_status('F8')
        f9_status, f9_pct = cell_status('F9')
        heatmap += f"| **–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ (–°–æ–∑–¥–∞–Ω–∏—è)** | {f7_status} F7 ({f7_pct}%) | {f8_status} F8 ({f8_pct}%) | {f9_status} F9 ({f9_pct}%) |\n"

        return heatmap + "\n---\n\n"

    def _completeness_summary(self) -> str:
        """Executive Summary –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã."""
        total_docs = len(self.documents)
        expected_total = 71  # –°—É–º–º–∞ –æ–∂–∏–¥–∞–µ–º—ã—Ö
        completeness = min(100, int(total_docs / expected_total * 100))

        # –ù–∞–π—Ç–∏ —Å–∞–º—ã–µ –ø–æ–ª–Ω—ã–µ –∏ —Å–∞–º—ã–µ –ø—É—Å—Ç—ã–µ —Å–µ–º–µ–π—Å—Ç–≤–∞
        family_ratios = {}
        expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}
        for f, exp in expected.items():
            count = len(self.by_family.get(f, []))
            family_ratios[f] = count / exp

        best = max(family_ratios, key=family_ratios.get)
        worst = min(family_ratios, key=family_ratios.get)

        gaps = [f for f, ratio in family_ratios.items() if ratio < 0.4]

        summary = "## 1. Executive Summary\n\n"
        summary += f"- **–û–±—â–∞—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞:** {completeness}%\n"
        summary += f"- **–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ:** {best} ({FAMILIES[best]['name']}) ‚Äî {int(family_ratios[best]*100)}%\n"
        summary += f"- **–ù–∞–∏–º–µ–Ω–µ–µ –ø–æ–ª–Ω–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ:** {worst} ({FAMILIES[worst]['name']}) ‚Äî {int(family_ratios[worst]*100)}%\n"
        summary += f"- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã:** {', '.join(gaps) if gaps else '–Ω–µ—Ç'}\n\n"

        return summary + "---\n\n"

    def _completeness_by_cells(self) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º –º–∞—Ç—Ä–∏—Ü—ã 3x3."""
        cells = ""

        for family_id in ["F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            family = FAMILIES[family_id]
            docs = self.by_family.get(family_id, [])

            cells += f"### 2.{family_id[1]}. {family['level']} √ó {family['role']} ({family_id}: {family['name']})\n\n"

            expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}
            ratio = len(docs) / expected.get(family_id, 5)
            status = "üü¢" if ratio >= 0.8 else ("üü°" if ratio >= 0.4 else "üî¥")

            cells += f"**–°—Ç–∞—Ç—É—Å:** {status} ({len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\n\n"

            if docs:
                cells += "**–î–æ–∫—É–º–µ–Ω—Ç—ã:**\n"
                for doc in docs[:5]:
                    cells += f"- [[{doc.name}]]\n"
                if len(docs) > 5:
                    cells += f"- ... –∏ –µ—â—ë {len(docs) - 5}\n"
            else:
                cells += "*–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç*\n"

            cells += "\n"

        return "## 2. –ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º –º–∞—Ç—Ä–∏—Ü—ã 3√ó3\n\n" + cells + "---\n\n"

    def _completeness_sota(self) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SoTA-–º–µ—Ç–æ–¥–æ–≤."""
        sota = "## 3. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SoTA-–º–µ—Ç–æ–¥–æ–≤\n\n"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–≤
        methods = {
            "JTBD": self._find_doc_by_pattern("JTBD|Jobs.to.be.done"),
            "Value Proposition": self._find_doc_by_pattern("–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ|Value Proposition"),
            "C4 Model": self._find_doc_by_pattern("C4|–ö–æ–Ω—Ç–µ–∫—Å—Ç.*–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä|–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä"),
            "ADR": self._find_doc_by_pattern("ADR|Architecture Decision"),
            "ConOps": self._find_doc_by_pattern("ConOps|–ö–æ–Ω—Ü–µ–ø—Ü–∏—è.*–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è|Concept.*Operations"),
            "OKR": self._find_doc_by_pattern("OKR|Objectives.*Key.*Results|–¶–µ–ª–∏.*–∑–∞–¥–∞—á–∏"),
        }

        sota += "| –ú–µ—Ç–æ–¥ | –°—Ç–∞—Ç—É—Å | –î–æ–∫—É–º–µ–Ω—Ç |\n"
        sota += "|-------|--------|----------|\n"

        for method, doc in methods.items():
            if doc:
                sota += f"| {method} | ‚úÖ –ü—Ä–∏–º–µ–Ω—ë–Ω | [[{doc.name}]] |\n"
            else:
                sota += f"| {method} | ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω | ‚Äî |\n"

        return sota + "\n---\n\n"

    def _completeness_stakeholders(self) -> str:
        """–û—Ç–≤–µ—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü."""
        stakeholders = "## 4. –û—Ç–≤–µ—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü\n\n"

        questions = [
            ("–£—á–µ–Ω–∏–∫", "–ß—Ç–æ —è –ø–æ–ª—É—á—É?", "F4", "–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"),
            ("–£—á–µ–Ω–∏–∫", "–ö–∞–∫ –Ω–∞—á–∞—Ç—å?", "F6", "–û–Ω–±–æ—Ä–¥–∏–Ω–≥"),
            ("–ù–∞—Å—Ç–∞–≤–Ω–∏–∫", "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —É—á–µ–Ω–∏–∫–∞–º–∏?", "F6", "–î–µ–∫–∞–Ω–∞—Ç|–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫"),
            ("–ò–Ω–≤–µ—Å—Ç–æ—Ä", "–ö–∞–∫–æ–≤–∞ –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª—å?", "F7", "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫|–±–∏–∑–Ω–µ—Å.–º–æ–¥–µ–ª"),
            ("–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞?", "F8", "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä|–ø–ª–∞—Ç—Ñ–æ—Ä–º"),
            ("–ü–∞—Ä—Ç–Ω—ë—Ä", "–ö–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è?", "F2", "–ò–Ω—Ç–µ–≥—Ä–∞—Ü|–ø–∞—Ä—Ç–Ω—ë—Ä"),
        ]

        stakeholders += "| –°—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä | –í–æ–ø—Ä–æ—Å | –°—Ç–∞—Ç—É—Å | –ì–¥–µ –æ—Ç–≤–µ—Ç |\n"
        stakeholders += "|-------------|--------|--------|----------|\n"

        for stakeholder, question, family, pattern in questions:
            doc = self._find_doc_by_pattern(pattern)
            if doc:
                stakeholders += f"| {stakeholder} | {question} | ‚úÖ | [[{doc.name}]] |\n"
            else:
                stakeholders += f"| {stakeholder} | {question} | ‚ùå | *–ù–µ –Ω–∞–π–¥–µ–Ω* |\n"

        return stakeholders + "\n---\n\n"

    def _completeness_gaps(self) -> str:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã."""
        gaps = "## 5. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n\n"

        expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}

        critical = []
        important = []

        for f, exp in expected.items():
            count = len(self.by_family.get(f, []))
            ratio = count / exp
            if ratio < 0.4:
                critical.append((f, FAMILIES[f]['name'], int(ratio * 100)))
            elif ratio < 0.8:
                important.append((f, FAMILIES[f]['name'], int(ratio * 100)))

        gaps += "### 5.1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ üî¥\n\n"
        if critical:
            for f, name, pct in critical:
                gaps += f"- **{f} ({name})** ‚Äî {pct}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏\n"
        else:
            gaps += "*–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–µ—Ç*\n"
        gaps += "\n"

        gaps += "### 5.2. –í–∞–∂–Ω—ã–µ üü°\n\n"
        if important:
            for f, name, pct in important:
                gaps += f"- **{f} ({name})** ‚Äî {pct}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏\n"
        else:
            gaps += "*–í–∞–∂–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–µ—Ç*\n"

        return gaps + "\n---\n\n"

    def _completeness_links(self) -> str:
        return """## 6. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–ú–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1]]
- [[–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.1]]
"""

    # ==================== –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´ ====================

    def _generate_technical_issues(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞'."""
        report = self._header("–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")

        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        dup_folders = self._find_duplicate_folders()
        dup_docs = self._find_duplicate_documents()
        broken_links = self._find_broken_links()
        missing_metadata = self._find_missing_metadata()

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        report += self._technical_heatmap(dup_folders, dup_docs, broken_links, missing_metadata)

        # Executive Summary
        total = len(dup_folders) + len(dup_docs) + len(broken_links) + len(missing_metadata)
        critical = len(dup_folders) + len([d for d in dup_docs if d[2] == "exact"])

        report += "## 1. Executive Summary\n\n"
        report += f"- **–í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º:** {total}\n"
        report += f"- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö:** {critical}\n"
        report += f"- **–¢—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è:** {total - critical}\n\n"
        report += "---\n\n"

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        report += self._technical_dup_folders(dup_folders)

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        report += self._technical_dup_docs(dup_docs)

        # –ë–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏
        report += self._technical_broken_links(broken_links)

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        report += self._technical_missing_metadata(missing_metadata)

        # –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        report += """## 7. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å 0.4]]
- [[–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.1]]
"""

        return report

    def _technical_heatmap(self, dup_folders, dup_docs, broken_links, missing_metadata) -> str:
        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º\n\n"
        heatmap += "| –¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°—Ç–∞—Ç—É—Å |\n"
        heatmap += "|--------------|------------|--------|\n"

        def status(count, threshold_red=1, threshold_yellow=5):
            if count >= threshold_red:
                return "üî¥"
            elif count >= threshold_yellow:
                return "üü°"
            return "üü¢"

        heatmap += f"| –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–∞–ø–æ–∫ | {len(dup_folders)} | {status(len(dup_folders))} |\n"
        heatmap += f"| –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {len(dup_docs)} | {status(len(dup_docs), 3, 10)} |\n"
        heatmap += f"| –ë–∏—Ç—ã–µ wikilinks | {len(broken_links)} | {status(len(broken_links), 5, 15)} |\n"
        heatmap += f"| –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö | {len(missing_metadata)} | {status(len(missing_metadata), 10, 30)} |\n"

        total = len(dup_folders) + len(dup_docs) + len(broken_links) + len(missing_metadata)
        heatmap += f"| **–ò—Ç–æ–≥–æ –ø—Ä–æ–±–ª–µ–º** | **{total}** | ‚Äî |\n"

        return heatmap + "\n---\n\n"

    def _find_duplicate_folders(self) -> List[Tuple[str, List[str], str]]:
        """–ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä–∞–º–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: (–∫–ª—é—á_–¥—É–±–ª—è, [–ø—É—Ç–∏], —Ç–∏–ø_–¥—É–±–ª—è)
        –¢–∏–ø—ã: 'number' (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞), 'name' (–æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)
        """
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –í–°–ï –ø–∞–ø–∫–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        all_folders = set()
        for folder in CONTENT_DIR.rglob("*"):
            if folder.is_dir():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞–ø–∫–∏
                if any(skip in str(folder) for skip in [".obsidian", "node_modules", ".git"]):
                    continue
                if folder != CONTENT_DIR:
                    all_folders.add(folder)

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π
        folder_numbers = defaultdict(list)  # –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ -> –ø—É—Ç–∏
        folder_names = defaultdict(list)    # –Ω–∞–∑–≤–∞–Ω–∏–µ (–±–µ–∑ –Ω–æ–º–µ—Ä–∞) -> –ø—É—Ç–∏

        for folder in all_folders:
            rel_path = str(folder.relative_to(CONTENT_DIR))
            folder_name = folder.name

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "0.4.1." –∏–∑ "0.4.1. –ù–∞–∑–≤–∞–Ω–∏–µ")
            number_match = re.match(r'^(\d+(?:\.\d+)*\.?)\s*', folder_name)
            if number_match:
                section_number = number_match.group(1).rstrip('.')  # "0.4.1"
                folder_numbers[section_number].append(rel_path)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –Ω–æ–º–µ—Ä–∞
            name = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', folder_name).lower().strip()
            if name:
                folder_names[name].append(rel_path)

        duplicates = []

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤ (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
        for number, paths in folder_numbers.items():
            unique_paths = list(set(paths))
            if len(unique_paths) > 1:
                duplicates.append((f"–ù–æ–º–µ—Ä {number}", unique_paths, "number"))

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π
        for name, paths in folder_names.items():
            unique_paths = list(set(paths))
            if len(unique_paths) > 1:
                duplicates.append((name, unique_paths, "name"))

        return duplicates

    def _find_duplicate_documents(self) -> List[Tuple[str, List[str], str]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏."""
        doc_names = defaultdict(list)

        for doc in self.documents:
            # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            name = re.sub(r'\s*\d+\.\d+\.?$', '', doc.name).lower().strip()
            doc_names[name].append(str(doc.relative_path))

        duplicates = []
        for name, paths in doc_names.items():
            if len(paths) > 1:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥—É–±–ª—è
                dup_type = "exact" if len(set(paths)) == len(paths) else "similar"
                duplicates.append((name, paths, dup_type))

        return duplicates

    def _find_broken_links(self) -> List[Tuple[str, str, str]]:
        """–ü–æ–∏—Å–∫ –±–∏—Ç—ã—Ö wikilinks."""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–º–µ–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_names = {doc.name.lower(): doc.name for doc in self.documents}

        broken = []
        for doc in self.documents:
            for link in doc.wikilinks:
                link_lower = link.lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                if link_lower not in doc_names:
                    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
                    similar = self._find_similar_name(link, doc_names.values())
                    broken.append((str(doc.relative_path), link, similar or "–Ω–µ –Ω–∞–π–¥–µ–Ω"))

        return broken[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

    def _find_missing_metadata(self) -> List[Tuple[str, List[str]]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        required_fields = ["type", "status"]

        missing = []
        for doc in self.documents:
            absent = [f for f in required_fields if f not in doc.frontmatter]
            if absent:
                missing.append((str(doc.relative_path), absent))

        return missing[:30]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

    def _technical_dup_folders(self, duplicates) -> str:
        section = "## 2. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫\n\n"

        if not duplicates:
            return section + "*–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏–ø—É: —Å–Ω–∞—á–∞–ª–∞ –¥—É–±–ª–∏ –Ω–æ–º–µ—Ä–æ–≤ (–∫—Ä–∏—Ç–∏—á–Ω–µ–µ), –ø–æ—Ç–æ–º –Ω–∞–∑–≤–∞–Ω–∏–π
        number_dups = [(n, p, t) for n, p, t in duplicates if t == "number"]
        name_dups = [(n, p, t) for n, p, t in duplicates if t == "name"]

        idx = 1

        if number_dups:
            section += "### –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤ üî¥\n\n"
            for name, paths, _ in number_dups[:10]:
                section += f"#### 2.{idx}. [DUP-F{idx:03d}] {name}\n\n"
                section += "**–ù–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º:**\n"
                for path in paths[:5]:
                    section += f"- `{path}`\n"
                section += "\n**–ü—Ä–æ–±–ª–µ–º–∞:** –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –Ω–∞—Ä—É—à–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.\n"
                section += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –æ–¥–Ω—É –∏–∑ –ø–∞–ø–æ–∫ —Å –Ω–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º.\n\n"
                idx += 1

        if name_dups:
            section += "### –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–∞–ø–æ–∫ üü°\n\n"
            for name, paths, _ in name_dups[:10]:
                section += f"#### 2.{idx}. [DUP-F{idx:03d}] –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ¬´{name}¬ª\n\n"
                section += "**–ù–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏:**\n"
                for path in paths[:5]:
                    section += f"- `{path}`\n"
                section += "\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–ª–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å.\n\n"
                idx += 1

        return section + "---\n\n"

    def _technical_dup_docs(self, duplicates) -> str:
        section = "## 3. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"

        if not duplicates:
            return section + "*–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –ù–∞–∑–≤–∞–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—É—Ç–∏ |\n"
        section += "|---|----------|------------|------|\n"

        for i, (name, paths, dup_type) in enumerate(duplicates[:15], 1):
            paths_str = "; ".join(paths[:3])
            if len(paths) > 3:
                paths_str += f" –∏ –µ—â—ë {len(paths) - 3}"
            section += f"| {i} | {name[:40]} | {len(paths)} | {paths_str[:60]}... |\n"

        return section + "\n---\n\n"

    def _technical_broken_links(self, broken) -> str:
        section = "## 4. –ë–∏—Ç—ã–µ wikilinks\n\n"

        if not broken:
            return section + "*–ë–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –°—Å—ã–ª–∫–∞ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n"
        section += "|---|----------|--------|-------------|\n"

        for i, (doc_path, link, suggestion) in enumerate(broken[:20], 1):
            doc_short = doc_path.split("/")[-1][:30]
            section += f"| {i} | {doc_short} | `[[{link[:30]}]]` | {suggestion[:30]} |\n"

        if len(broken) > 20:
            section += f"\n*... –∏ –µ—â—ë {len(broken) - 20} –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫*\n"

        return section + "\n---\n\n"

    def _technical_missing_metadata(self, missing) -> str:
        section = "## 5. –î–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n\n"

        if not missing:
            return section + "*–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–º–µ—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è |\n"
        section += "|---|----------|------------------|\n"

        for i, (doc_path, fields) in enumerate(missing[:20], 1):
            doc_short = doc_path.split("/")[-1][:40]
            section += f"| {i} | {doc_short} | {', '.join(fields)} |\n"

        if len(missing) > 20:
            section += f"\n*... –∏ –µ—â—ë {len(missing) - 20} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤*\n"

        return section + "\n---\n\n"

    # ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –û–¢–ß–Å–¢–´ ====================

    def _generate_terminology(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏."""
        report = self._header("–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")

        if not self.ai_analyzer:
            report += "*–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç —Ç—Ä–µ–±—É–µ—Ç AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤.*\n\n"
            report += "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å —Ñ–ª–∞–≥–æ–º `--ai-analysis` –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n"
            report += "```bash\n"
            report += "python3 ops/build_report.py --report terminology --ai-analysis\n"
            report += "```\n\n"
            report += "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n"
            report += "- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: `pip install anthropic`\n"
            report += "- –ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `ANTHROPIC_API_KEY`\n"
            return report

        print("   ü§ñ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è AI-–∞–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏...")
        ai_analysis = self.ai_analyzer.analyze_terminology(self.documents)
        report += ai_analysis

        return report

    def _generate_recommendations(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é.
        –†–∞–±–æ—Ç–∞–µ—Ç –ë–ï–ó AI-–∞–Ω–∞–ª–∏–∑–∞, –∞–≥—Ä–µ–≥–∏—Ä—É—è –¥–∞–Ω–Ω—ã–µ –∏–∑ –≤—Å–µ—Ö –æ—Ç—á–µ—Ç–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó 0.4.1.
        """
        report = self._header("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é")

        # –ê–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ (–±–µ–∑ AI)
        report += self._recommendations_heatmap()
        report += self._recommendations_metrics()
        report += self._recommendations_critical_issues()
        report += self._recommendations_priorities()

        # –ï—Å–ª–∏ –µ—Å—Ç—å AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä, –¥–æ–±–∞–≤–ª—è–µ–º AI-—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
        if self.ai_analyzer:
            print("   ü§ñ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è AI-–∞–Ω–∞–ª–∏–∑ –¥–ª—è –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
            ai_analysis = self.ai_analyzer.analyze_recommendations(self.documents, self.by_family)
            report += "\n---\n\n## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ AI\n\n"
            report += ai_analysis

        return report

    def _recommendations_heatmap(self) -> str:
        """–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó."""
        # –†–∞—Å—á–µ—Ç –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π
        full_docs = [d for d in self.documents if d.is_full]
        full_docs_count = len(full_docs)
        full_ratio = full_docs_count / len(self.documents) if self.documents else 0

        docs_with_links = sum(1 for d in self.documents if len(d.wikilinks) > 0)
        links_ratio = docs_with_links / len(self.documents) if self.documents else 0

        # –ü–æ–¥—Å—á–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º (—Å–µ–º–µ–π—Å—Ç–≤–∞ —Å üî¥ —Å—Ç–∞—Ç—É—Å–æ–º)
        critical_families = 0
        for family_id in ["F0", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            docs = self.by_family.get(family_id, [])
            if not docs:
                critical_families += 1
                continue
            full_count = sum(1 for d in docs if d.is_full)
            full_fam_ratio = full_count / len(docs)
            if full_fam_ratio < 0.5:
                critical_families += 1

        # –í–∑–≤–µ—à–µ–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è (—Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó)
        # –í–µ—Å –ø–æ–∫–∞–∑–∞—Ç–µ–ª–µ–π: –ø–æ–ª–Ω–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 40%, —Å–≤—è–∑–Ω–æ—Å—Ç—å 30%, –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º 30%
        health_score = (
            (full_ratio * 40) +  # 40% –≤–µ—Å
            (links_ratio * 30) +  # 30% –≤–µ—Å
            ((1 - critical_families / 10) * 30)  # 30% –≤–µ—Å
        )

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó –ø. 2
        if (full_docs_count >= 90 and
            health_score >= 90 and
            links_ratio >= 0.8 and
            critical_families < 10):
            overall_status = "üü¢"
            status_desc = "–ó–¥–æ—Ä–æ–≤–æ–µ"
        elif (full_docs_count >= 60 and
              health_score >= 75 and
              links_ratio >= 0.4 and
              critical_families <= 30):
            overall_status = "üü°"
            status_desc = "–¢—Ä–µ–±—É–µ—Ç –≤–Ω–∏–º–∞–Ω–∏—è"
        else:
            overall_status = "üî¥"
            status_desc = "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ"

        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –∑–¥–æ—Ä–æ–≤—å—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n\n"
        heatmap += f"**–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å:** {overall_status} {status_desc}\n\n"

        heatmap += "| –ò–∑–º–µ—Ä–µ–Ω–∏–µ | –û—Ü–µ–Ω–∫–∞ | –í–µ—Å | –í–∫–ª–∞–¥ | –°—Ç–∞—Ç—É—Å |\n"
        heatmap += "|-----------|--------|-----|-------|--------|\n"
        heatmap += f"| –ü–æ–ª–Ω–æ—Ç–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {full_docs_count}/{len(self.documents)} ({int(full_ratio*100)}%) | 40% | {full_ratio*40:.1f} | {'üü¢' if full_ratio >= 0.8 else 'üü°' if full_ratio >= 0.5 else 'üî¥'} |\n"
        heatmap += f"| –°–≤—è–∑–Ω–æ—Å—Ç—å | {docs_with_links}/{len(self.documents)} ({int(links_ratio*100)}%) | 30% | {links_ratio*30:.1f} | {'üü¢' if links_ratio >= 0.7 else 'üü°' if links_ratio >= 0.4 else 'üî¥'} |\n"
        heatmap += f"| –û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –ø—Ä–æ–±–ª–µ–º | {10-critical_families}/10 —Å–µ–º–µ–π—Å—Ç–≤ | 30% | {(1-critical_families/10)*30:.1f} | {'üü¢' if critical_families < 3 else 'üü°' if critical_families <= 5 else 'üî¥'} |\n"
        heatmap += f"| **–ò—Ç–æ–≥–æ** | ‚Äî | 100% | **{health_score:.1f}** | {overall_status} |\n\n"

        heatmap += f"**–ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:** {'‚úÖ –•—Ä–∞–Ω–∏–ª–∏—â–µ –≤ —Ö–æ—Ä–æ—à–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏' if overall_status == 'üü¢' else '‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è –≤–Ω–∏–º–∞–Ω–∏–µ –∏ —É–ª—É—á—à–µ–Ω–∏—è' if overall_status == 'üü°' else 'üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ, —Ç—Ä–µ–±—É–µ—Ç—Å—è —Å—Ä–æ—á–Ω–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ'}\n\n"

        return heatmap + "---\n\n"

    def _recommendations_metrics(self) -> str:
        """–î–µ—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ –ø–æ –æ—Ç—á–µ—Ç–∞–º."""
        metrics = "## 1. –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏\n\n"

        # –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞
        metrics += "### 1.1. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ (–ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º F0-F9)\n\n"
        metrics += "| –°–µ–º–µ–π—Å—Ç–≤–æ | –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ | –ü–æ–ª–Ω—ã—Ö | % | –°—Ç–∞—Ç—É—Å |\n"
        metrics += "|-----------|------------|--------|---|--------|\n"

        for family_id in ["F0", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            family = FAMILIES[family_id]
            docs = self.by_family.get(family_id, [])
            count = len(docs)
            full_count = sum(1 for d in docs if d.is_full)
            full_pct = int(full_count / count * 100) if count > 0 else 0
            status = "üü¢" if full_pct >= 80 else "üü°" if full_pct >= 50 else "üî¥"
            metrics += f"| {family_id} | {count} | {full_count} | {full_pct}% | {status} |\n"

        metrics += "\n### 1.2. –°–≤—è–∑–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
        docs_with_links = sum(1 for d in self.documents if len(d.wikilinks) > 0)
        isolated = len(self.documents) - docs_with_links
        metrics += f"- **–î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å–≤—è–∑—è–º–∏:** {docs_with_links} ({int(docs_with_links/len(self.documents)*100)}%)\n"
        metrics += f"- **–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {isolated} ({int(isolated/len(self.documents)*100)}%)\n"
        metrics += f"- **–°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç:** {sum(len(d.wikilinks) for d in self.documents) / len(self.documents):.1f}\n\n"

        return metrics + "---\n\n"

    def _recommendations_critical_issues(self) -> str:
        """–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã."""
        issues = "## 2. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã üî¥\n\n"

        critical_found = False

        # –°–µ–º–µ–π—Å—Ç–≤–∞ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —Å—Ç–∞—Ç—É—Å–æ–º
        critical_families = []
        for family_id in ["F0", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            docs = self.by_family.get(family_id, [])
            if not docs:
                critical_families.append((family_id, FAMILIES[family_id]['name'], 0, "–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"))
                continue
            full_count = sum(1 for d in docs if d.is_full)
            full_ratio = full_count / len(docs)
            if full_ratio < 0.5:
                critical_families.append((family_id, FAMILIES[family_id]['name'], int(full_ratio*100), f"–¢–æ–ª—å–∫–æ {int(full_ratio*100)}% –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ–ª–Ω—ã–µ"))

        if critical_families:
            critical_found = True
            issues += "### 2.1. –°–µ–º–µ–π—Å—Ç–≤–∞ —Å –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–º —É—Ä–æ–≤–Ω–µ–º –ø–æ–ª–Ω–æ—Ç—ã (<50%)\n\n"
            issues += "| –°–µ–º–µ–π—Å—Ç–≤–æ | –ù–∞–∑–≤–∞–Ω–∏–µ | % –ø–æ–ª–Ω—ã—Ö | –ü—Ä–æ–±–ª–µ–º–∞ |\n"
            issues += "|-----------|----------|----------|----------|\n"
            for fid, fname, pct, problem in critical_families[:5]:
                issues += f"| {fid} | {fname} | {pct}% | {problem} |\n"
            issues += "\n"

        # –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        isolated = [d for d in self.documents if len(d.wikilinks) == 0]
        if len(isolated) > 50:
            critical_found = True
            issues += f"### 2.2. –ú–∞—Å—Å–æ–≤–∞—è –∏–∑–æ–ª—è—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
            issues += f"**{len(isolated)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({int(len(isolated)/len(self.documents)*100)}%) –Ω–µ –∏–º–µ—é—Ç —Å–≤—è–∑–µ–π —Å –¥—Ä—É–≥–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.**\n\n"
            issues += "–≠—Ç–æ –∑–∞—Ç—Ä—É–¥–Ω—è–µ—Ç –Ω–∞–≤–∏–≥–∞—Ü–∏—é –∏ –ø–æ–Ω–∏–º–∞–Ω–∏–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.\n\n"

        if not critical_found:
            issues += "*–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ.*\n\n"

        return issues + "---\n\n"

    def _recommendations_priorities(self) -> str:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏."""
        rec = "## 3. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º\n\n"

        rec += "### 3.1. –°—Ä–æ—á–Ω—ã–µ (—ç—Ç–∞ –Ω–µ–¥–µ–ª—è)\n\n"

        urgent = []

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–µ–º–µ–π—Å—Ç–≤ —Å 0% –ø–æ–ª–Ω–æ—Ç—ã
        for family_id in ["F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            docs = self.by_family.get(family_id, [])
            if not docs:
                continue
            full_count = sum(1 for d in docs if d.is_full)
            if full_count == 0:
                urgent.append(f"**{family_id} ({FAMILIES[family_id]['name']}):** –ù–∞–ø–æ–ª–Ω–∏—Ç—å —Å–µ–º–µ–π—Å—Ç–≤–æ –ø–æ–ª–Ω—ã–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ (—Å–µ–π—á–∞—Å 0/{len(docs)} –ø–æ–ª–Ω—ã—Ö)")

        if urgent:
            for item in urgent[:3]:
                rec += f"1. {item}\n"
        else:
            rec += "1. –ü—Ä–æ–¥–æ–ª–∂–∏—Ç—å —Ä–∞–∑–≤–∏—Ç–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ —Å–æ–≥–ª–∞—Å–Ω–æ –ø–ª–∞–Ω—É\n"

        rec += "\n### 3.2. –í–∞–∂–Ω—ã–µ (—ç—Ç–æ—Ç –º–µ—Å—è—Ü)\n\n"

        important = []

        # –°–µ–º–µ–π—Å—Ç–≤–∞ —Å 1-49% –ø–æ–ª–Ω–æ—Ç—ã
        for family_id in ["F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            docs = self.by_family.get(family_id, [])
            if not docs:
                continue
            full_count = sum(1 for d in docs if d.is_full)
            full_ratio = full_count / len(docs)
            if 0 < full_ratio < 0.5:
                important.append(f"**{family_id}:** –î–æ–≤–µ—Å—Ç–∏ –ø–æ–ª–Ω–æ—Ç—É –¥–æ 50%+ (—Å–µ–π—á–∞—Å {int(full_ratio*100)}%)")

        # –°–≤—è–∑–Ω–æ—Å—Ç—å
        isolated_pct = sum(1 for d in self.documents if len(d.wikilinks) == 0) / len(self.documents)
        if isolated_pct > 0.5:
            important.append(f"**–°–≤—è–∑–Ω–æ—Å—Ç—å:** –î–æ–±–∞–≤–∏—Ç—å wikilinks –≤ –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (—Å–µ–π—á–∞—Å {int(isolated_pct*100)}% –±–µ–∑ —Å–≤—è–∑–µ–π)")

        if important:
            for i, item in enumerate(important[:3], 1):
                rec += f"{i}. {item}\n"
        else:
            rec += "1. –£–ª—É—á—à–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"

        rec += "\n### 3.3. –ñ–µ–ª–∞—Ç–µ–ª—å–Ω—ã–µ (–±—ç–∫–ª–æ–≥)\n\n"
        rec += "1. –î–æ–±–∞–≤–∏—Ç—å –¥–∏–∞–≥—Ä–∞–º–º—ã –∏ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n"
        rec += "2. –û–±–Ω–æ–≤–∏—Ç—å —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (>6 –º–µ—Å—è—Ü–µ–≤)\n"
        rec += "3. –î–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏–º–µ—Ä—ã –∏ –º–µ—Ç—Ä–∏–∫–∏ –≤ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n\n"

        return rec + "---\n\n"

    def _generate_links_map(self) -> str:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç—ã —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏.
        –°–æ–≥–ª–∞—Å–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–Ω–æ–º—É –¢–ó 0.4.1:
        - –î–µ—Ç–∞–ª—å–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –¥–ª—è –í–°–ï–• –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        - –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–ø–æ–ª–Ω—ã–µ –∏–ª–∏ –∑–∞–≥–ª—É—à–∫–∏)
        - –°—Ç–∞—Ç—É—Å —Å–≤—è–∑–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è
        """
        report = self._header("–ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")

        # –ü–æ–¥—Å—á–µ—Ç –≤—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫
        incoming = defaultdict(int)
        for doc in self.documents:
            for link in doc.wikilinks:
                incoming[link.lower()] += 1

        # –ò–Ω–¥–µ–∫—Å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –∏–º–µ–Ω–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –ø–æ–∏—Å–∫–∞
        doc_by_name = {d.name.lower(): d for d in self.documents}

        # –ê–Ω–∞–ª–∏–∑ –∫–∞–∂–¥–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
        doc_stats = []
        for doc in self.documents:
            # –ò—Å—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏
            outgoing = len(doc.wikilinks)

            # –í—Ö–æ–¥—è—â–∏–µ —Å–≤—è–∑–∏
            incoming_count = incoming.get(doc.name.lower(), 0)

            # –í—Å–µ–≥–æ —Å–≤—è–∑–µ–π
            total_links = incoming_count + outgoing

            # –¢–µ–∫—Å—Ç–æ–≤—ã–µ —Å–≤—è–∑–∏ (wikilinks –≤ —Ç–µ–ª–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞, –Ω–µ –≤ frontmatter)
            text_links = len(doc.wikilinks)  # –í—Å–µ wikilinks —É–∂–µ –∏–∑ —Ç–µ–∫—Å—Ç–∞

            # –ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–ª–Ω—ã—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            linked_full_count = 0
            linked_total = 0
            for link in doc.wikilinks:
                linked_doc = doc_by_name.get(link.lower())
                if linked_doc:
                    linked_total += 1
                    if linked_doc.is_full:
                        linked_full_count += 1

            full_linked_ratio = (linked_full_count / linked_total * 100) if linked_total > 0 else 0

            # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –ñ–ï–°–¢–ö–ò–ú –∫—Ä–∏—Ç–µ—Ä–∏—è–º –¢–ó
            # üü¢ –•–æ—Ä–æ—à–æ —Å–≤—è–∑–∞–Ω: ‚â•5 —Å–≤—è–∑–µ–π + ‚â•70% —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö + ‚â•70% —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª–Ω—ã–µ
            # üü° –°–ª–∞–±–æ —Å–≤—è–∑–∞–Ω: 3-4 —Å–≤—è–∑–∏ + ‚â•40% —Ç–µ–∫—Å—Ç–æ–≤—ã—Ö + ‚â•50% —Å–≤—è–∑–∞–Ω–Ω—ã—Ö –ø–æ–ª–Ω—ã–µ
            # üî¥ –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω (–ü–û –£–ú–û–õ–ß–ê–ù–ò–Æ): ‚â§2 —Å–≤—è–∑–µ–π –ò–õ–ò —Å–≤—è–∑–∏ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã/–Ω–µ–ø–æ–ª–Ω—ã–µ

            if (total_links >= 5 and
                text_links >= total_links * 0.7 and
                full_linked_ratio >= 70):
                status = "üü¢"
            elif (total_links >= 3 and
                  text_links >= total_links * 0.4 and
                  full_linked_ratio >= 50):
                status = "üü°"
            else:
                status = "üî¥"

            doc_stats.append({
                'name': doc.name,
                'total': total_links,
                'text': text_links,
                'incoming': incoming_count,
                'outgoing': outgoing,
                'full_linked_pct': int(full_linked_ratio),
                'status': status
            })

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É —Å–≤—è–∑–µ–π (—É–±—ã–≤–∞–Ω–∏–µ)
        doc_stats.sort(key=lambda x: x['total'], reverse=True)

        # –ü–æ–¥—Å—á–µ—Ç —Å—Ç–∞—Ç—É—Å–æ–≤
        status_counts = {"üü¢": 0, "üü°": 0, "üî¥": 0}
        for stat in doc_stats:
            status_counts[stat['status']] += 1

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–≤—è–∑–Ω–æ—Å—Ç–∏ (—Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó –ø. 2)
        report += "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π\n\n"
        report += "**–°–≤–æ–¥–∫–∞ –ø–æ —Å—Ç–∞—Ç—É—Å–∞–º:**\n"
        report += f"- üü¢ –•–æ—Ä–æ—à–æ —Å–≤—è–∑–∞–Ω—ã: {status_counts['üü¢']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({int(status_counts['üü¢']/len(doc_stats)*100)}%)\n"
        report += f"- üü° –°–ª–∞–±–æ —Å–≤—è–∑–∞–Ω—ã: {status_counts['üü°']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({int(status_counts['üü°']/len(doc_stats)*100)}%)\n"
        report += f"- üî¥ –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω—ã: {status_counts['üî¥']} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({int(status_counts['üî¥']/len(doc_stats)*100)}%)\n\n"

        report += "**–î–µ—Ç–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (–ø–µ—Ä–≤—ã–µ 20, –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–º. –Ω–∏–∂–µ):**\n\n"
        report += "| –î–æ–∫—É–º–µ–Ω—Ç | –í—Å–µ–≥–æ —Å–≤—è–∑–µ–π | –¢–µ–∫—Å—Ç–æ–≤—ã—Ö | –í—Ö–æ–¥—è—â–∏—Ö | –ò—Å—Ö–æ–¥—è—â–∏—Ö | % –ø–æ–ª–Ω—ã—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö | –°—Ç–∞—Ç—É—Å |\n"
        report += "|----------|--------------|-----------|----------|-----------|-------------------|--------|\n"

        # –ü–µ—Ä–≤—ã–µ 20 –¥–ª—è preview
        for stat in doc_stats[:20]:
            name_short = stat['name'][:60]
            report += f"| {name_short} | {stat['total']} | {stat['text']} ({int(stat['text']/stat['total']*100) if stat['total'] > 0 else 0}%) | {stat['incoming']} | {stat['outgoing']} | {stat['full_linked_pct']}% | {stat['status']} |\n"

        report += f"\n*–ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ —Å–æ –≤—Å–µ–º–∏ {len(doc_stats)} –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∞ –≤ —Ä–∞–∑–¥–µ–ª–µ 3.*\n\n"
        report += "---\n\n"

        # –°–µ–∫—Ü–∏—è 1: Executive Summary
        total_links = sum(stat['total'] for stat in doc_stats)
        avg_links = total_links / len(doc_stats) if doc_stats else 0

        report += "## 1. Executive Summary\n\n"
        report += f"- **–í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {len(doc_stats)}\n"
        report += f"- **–í—Å–µ–≥–æ —Å–≤—è–∑–µ–π:** {total_links}\n"
        report += f"- **–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:** {status_counts['üî¥']} ({int(status_counts['üî¥']/len(doc_stats)*100)}%)\n"
        report += f"- **–°—Ä–µ–¥–Ω–µ–µ —Å–≤—è–∑–µ–π –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç:** {avg_links:.1f}\n\n"
        report += "---\n\n"

        # –°–µ–∫—Ü–∏—è 2: –¢–æ–ø-10 —Ö–∞–±–æ–≤
        report += "## 2. –¢–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å–≤—è–∑—è–º (—Ö–∞–±—ã)\n\n"
        report += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –í—Ö–æ–¥—è—â–∏—Ö | –ò—Å—Ö–æ–¥—è—â–∏—Ö | –í—Å–µ–≥–æ |\n"
        report += "|---|----------|----------|-----------|-------|\n"

        top_incoming = sorted(doc_stats, key=lambda x: x['incoming'], reverse=True)[:10]
        for i, stat in enumerate(top_incoming, 1):
            report += f"| {i} | [[{stat['name']}]] | {stat['incoming']} | {stat['outgoing']} | {stat['total']} |\n"

        report += "\n---\n\n"

        # –°–µ–∫—Ü–∏—è 3: –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        report += f"## 3. –ü–æ–ª–Ω–∞—è —Ç–∞–±–ª–∏—Ü–∞ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ ({len(doc_stats)})\n\n"
        report += "| –î–æ–∫—É–º–µ–Ω—Ç | –í—Å–µ–≥–æ —Å–≤—è–∑–µ–π | –¢–µ–∫—Å—Ç–æ–≤—ã—Ö | –í—Ö–æ–¥—è—â–∏—Ö | –ò—Å—Ö–æ–¥—è—â–∏—Ö | % –ø–æ–ª–Ω—ã—Ö —Å–≤—è–∑–∞–Ω–Ω—ã—Ö | –°—Ç–∞—Ç—É—Å |\n"
        report += "|----------|--------------|-----------|----------|-----------|-------------------|--------|\n"

        for stat in doc_stats:
            name_short = stat['name'][:60]
            text_pct = int(stat['text']/stat['total']*100) if stat['total'] > 0 else 0
            report += f"| {name_short} | {stat['total']} | {stat['text']} ({text_pct}%) | {stat['incoming']} | {stat['outgoing']} | {stat['full_linked_pct']}% | {stat['status']} |\n"

        report += "\n---\n\n"

        # –°–µ–∫—Ü–∏—è 4: –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        isolated = [stat for stat in doc_stats if stat['status'] == 'üî¥']
        report += f"## 4. –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã üî¥ ({len(isolated)})\n\n"

        if isolated:
            report += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –í—Å–µ–≥–æ —Å–≤—è–∑–µ–π | –ü—Ä–∏—á–∏–Ω–∞ –∏–∑–æ–ª—è—Ü–∏–∏ |\n"
            report += "|---|----------|--------------|------------------|\n"

            for i, stat in enumerate(isolated[:20], 1):
                reason = "–ù–µ—Ç —Å–≤—è–∑–µ–π" if stat['total'] == 0 else f"–¢–æ–ª—å–∫–æ {stat['total']} —Å–≤—è–∑."
                if stat['full_linked_pct'] < 50:
                    reason += f", —Å–≤—è–∑–∞–Ω–Ω—ã–µ –Ω–µ–ø–æ–ª–Ω—ã–µ ({stat['full_linked_pct']}%)"
                report += f"| {i} | {stat['name'][:50]} | {stat['total']} | {reason} |\n"

            if len(isolated) > 20:
                report += f"\n*... –∏ –µ—â—ë {len(isolated) - 20} –∏–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤*\n"
        else:
            report += "*–ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–µ—Ç*\n"

        report += "\n---\n\n"

        return report

    # ==================== –£–¢–ò–õ–ò–¢–´ ====================

    def _find_doc_by_pattern(self, pattern: str) -> Optional[Document]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏."""
        regex = re.compile(pattern, re.IGNORECASE)
        for doc in self.documents:
            if regex.search(doc.name):
                return doc
        return None

    def _extract_summary(self, doc: Document, max_sentences: int = 3) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        text = re.sub(r'^#.*$', '', doc.body, flags=re.MULTILINE)
        text = re.sub(r'\|.*\|', '', text)  # –£–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # –£–±–∏—Ä–∞–µ–º –∫–æ–¥
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]', r'\1', text)  # –£–±–∏—Ä–∞–µ–º wikilinks

        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

        return " ".join(sentences[:max_sentences])

    def _extract_first_sentence(self, doc: Document) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        summary = self._extract_summary(doc, max_sentences=1)
        return summary[:100] if summary else ""

    def _find_similar_name(self, name: str, candidates: List[str], threshold: float = 0.8) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è (–ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º)."""
        name_lower = name.lower()
        for candidate in candidates:
            candidate_lower = candidate.lower()
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            if name_lower in candidate_lower or candidate_lower in name_lower:
                return candidate
        return None


def save_report(content: str, filename: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ —Ñ–∞–π–ª."""
    output_path = REPORTS_DIR / filename
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content, encoding="utf-8")
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤")
    parser.add_argument(
        "--report", "-r",
        required=True,
        choices=["architecture-snapshot", "content-completeness", "technical-issues",
                 "terminology", "recommendations", "links-map", "all"],
        help="–¢–∏–ø –æ—Ç—á—ë—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    )
    parser.add_argument(
        "--output", "-o",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤ –ø–∞–ø–∫—É –æ—Ç—á—ë—Ç–æ–≤)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç—á—ë—Ç, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å"
    )
    parser.add_argument(
        "--ai-analysis",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI (Claude) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
    )

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—É—â–µ–Ω–æ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
    if not CONTENT_DIR.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {CONTENT_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞.")
        sys.exit(1)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    ai_analyzer = None
    if args.ai_analysis:
        try:
            ai_analyzer = AIAnalyzer()
            print("‚úÖ AI-–∞–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è  {e}")
            print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ AI-–∞–Ω–∞–ª–∏–∑–∞...")

    generator = ReportGenerator(ai_analyzer=ai_analyzer)
    generator.scan_documents()

    report_files = {
        "architecture-snapshot": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.4.md",
        "content-completeness": "–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è 0.4.md",
        "technical-issues": "–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ 0.4.md",
        "terminology": "–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å 0.4.md",
        "recommendations": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é 0.4.md",
        "links-map": "–ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ 0.4.md",
    }

    if args.report == "all":
        reports_to_generate = list(report_files.keys())
    else:
        reports_to_generate = [args.report]

    for report_type in reports_to_generate:
        print(f"\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞: {report_type}")

        try:
            content = generator.generate(report_type)

            if args.dry_run:
                print("\n" + "=" * 60)
                print(content[:2000])
                print("..." if len(content) > 2000 else "")
                print("=" * 60)
            else:
                filename = args.output if args.output and args.report != "all" else report_files[report_type]
                save_report(content, filename)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {report_type}: {e}")
            import traceback
            traceback.print_exc()

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
