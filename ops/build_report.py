#!/usr/bin/env python3
"""
–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ø–æ —Ö—Ä–∞–Ω–∏–ª–∏—â—É –∑–Ω–∞–Ω–∏–π.

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python3 ops/build_report.py --report architecture-snapshot
    python3 ops/build_report.py --report content-completeness
    python3 ops/build_report.py --report technical-issues
    python3 ops/build_report.py --report all

    # –° AI-–∞–Ω–∞–ª–∏–∑–æ–º (—Ç—Ä–µ–±—É–µ—Ç ANTHROPIC_API_KEY)
    python3 ops/build_report.py --report terminology --ai-analysis
    python3 ops/build_report.py --report recommendations --ai-analysis

–¢–∏–ø—ã –æ—Ç—á—ë—Ç–æ–≤:
    architecture-snapshot   - –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    content-completeness    - –°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è
    technical-issues        - –ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞
    terminology             - –¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å (AI)
    recommendations         - –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é (AI)
    links-map               - –ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
    all                     - –í—Å–µ –æ—Ç—á—ë—Ç—ã

–§–ª–∞–≥–∏:
    --ai-analysis           - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å Claude –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞
    --dry-run               - –ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ñ–∞–π–ª—ã, —Ç–æ–ª—å–∫–æ –≤—ã–≤–µ—Å—Ç–∏
    --output, -o            - –£–∫–∞–∑–∞—Ç—å –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
"""

import os
import re
import sys
import json
import yaml
import argparse
import hashlib
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Optional, Tuple, Any

# –û–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–∞—è –ø–æ–¥–¥–µ—Ä–∂–∫–∞ AI-–∞–Ω–∞–ª–∏–∑–∞
try:
    import anthropic
    HAS_ANTHROPIC = True
except ImportError:
    HAS_ANTHROPIC = False

# –ö–æ–Ω—Å—Ç–∞–Ω—Ç—ã
CONTENT_DIR = Path("content")
REPORTS_DIR = CONTENT_DIR / "0. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ" / "0.4. –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ –æ—Ç—á—ë—Ç—ã –ò–ò"

# –°–µ–º–µ–π—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ F0-F9
FAMILIES = {
    "F0": {"name": "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ", "level": "–ú–µ—Ç–∞—Å–∏—Å—Ç–µ–º–∞", "role": "-", "section": "0"},
    "F1": {"name": "–í–∏–¥–µ–Ω–∏–µ –∏ –ø—Ä–∏–≤–ª–µ—á–µ–Ω–∏–µ", "level": "–ú–∏—Ä", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "1.1"},
    "F2": {"name": "–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏ –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã", "level": "–ú–∏—Ä", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "1.2"},
    "F3": {"name": "–†–µ–ø—É—Ç–∞—Ü–∏—è –∏ –ø–∞—Ä—Ç–Ω—ë—Ä—Å—Ç–≤–∞", "level": "–ú–∏—Ä", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "1.3"},
    "F4": {"name": "–¶–µ–Ω–Ω–æ—Å—Ç—å –∏ –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª–∏", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "2.1"},
    "F5": {"name": "–ú–æ–¥–µ–ª—å –∏ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "2.2"},
    "F6": {"name": "–ü—É—Ç—å –∏ —Å–æ–ø—Ä–æ–≤–æ–∂–¥–µ–Ω–∏–µ", "level": "–°–æ–∑–∏–¥–∞—Ç–µ–ª—å", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "2.3"},
    "F7": {"name": "–≠–∫–æ–Ω–æ–º–∏–∫–∞ –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å", "section": "3.1"},
    "F8": {"name": "–ü–ª–∞—Ç—Ñ–æ—Ä–º–∞ –∏ –ø–æ–¥—Å–∏—Å—Ç–µ–º—ã", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ò–Ω–∂–µ–Ω–µ—Ä", "section": "3.2"},
    "F9": {"name": "–ö–æ–º–∞–Ω–¥–∞ –∏ —Å–ª—É–∂–±—ã", "level": "–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞", "role": "–ú–µ–Ω–µ–¥–∂–µ—Ä", "section": "3.3"},
}

# –ú–æ–¥–µ–ª—å –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞
AI_MODEL = "claude-sonnet-4-20250514"
AI_MAX_TOKENS = 4096


class AIAnalyzer:
    """–ö–ª–∞—Å—Å –¥–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Claude."""

    def __init__(self):
        if not HAS_ANTHROPIC:
            raise RuntimeError(
                "–î–ª—è AI-–∞–Ω–∞–ª–∏–∑–∞ —Ç—Ä–µ–±—É–µ—Ç—Å—è –±–∏–±–ª–∏–æ—Ç–µ–∫–∞ anthropic.\n"
                "–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: pip install anthropic"
            )

        api_key = os.environ.get("ANTHROPIC_API_KEY")
        if not api_key:
            raise RuntimeError(
                "–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è ANTHROPIC_API_KEY.\n"
                "–ü–æ–ª—É—á–∏—Ç–µ –∫–ª—é—á –Ω–∞ https://console.anthropic.com/"
            )

        self.client = anthropic.Anthropic(api_key=api_key)

    def analyze(self, prompt: str, context: str, max_tokens: int = AI_MAX_TOKENS) -> str:
        """–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ AI-–∞–Ω–∞–ª–∏–∑–∞."""
        try:
            response = self.client.messages.create(
                model=AI_MODEL,
                max_tokens=max_tokens,
                messages=[
                    {
                        "role": "user",
                        "content": f"{prompt}\n\n---\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}"
                    }
                ]
            )
            return response.content[0].text
        except Exception as e:
            return f"*–û—à–∏–±–∫–∞ AI-–∞–Ω–∞–ª–∏–∑–∞: {e}*"

    def analyze_terminology(self, documents: List['Document']) -> str:
        """–ê–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏."""
        # –°–æ–±–∏—Ä–∞–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        terms_context = self._extract_terms_context(documents)

        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π.

–ó–∞–¥–∞—á–∏:
1. –ù–∞–π–¥–∏ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è —Å —Ä–∞–∑–Ω—ã–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ –≤ —Ä–∞–∑–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ö
2. –ù–∞–π–¥–∏ —Ç–µ—Ä–º–∏–Ω—ã, –∫–æ—Ç–æ—Ä—ã–µ —É–ø–æ–º–∏–Ω–∞—é—Ç—Å—è –±–µ–∑ —è–≤–Ω–æ–≥–æ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è
3. –ù–∞–π–¥–∏ —Å–∏–Ω–æ–Ω–∏–º—ã/–≤–∞—Ä–∏–∞—Ü–∏–∏ —Ç–µ—Ä–º–∏–Ω–æ–≤ (–æ–¥–Ω–æ –ø–æ–Ω—è—Ç–∏–µ - —Ä–∞–∑–Ω—ã–µ –Ω–∞–∑–≤–∞–Ω–∏—è)
4. –û—Ü–µ–Ω–∏ –æ–±—â—É—é —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫—É—é —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –≤ Markdown:

## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏

| –ö–∞—Ç–µ–≥–æ—Ä–∏—è | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°—Ç–∞—Ç—É—Å |
|-----------|------------|--------|
| –°–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã | N | üü¢ |
| –¢–µ—Ä–º–∏–Ω—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ | N | üü° |
| –¢–µ—Ä–º–∏–Ω—ã —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π | N | üî¥ |

## 1. –¢–µ—Ä–º–∏–Ω—ã —Å –∫–æ–Ω—Ñ–ª–∏–∫—Ç–∞–º–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

### 1.1. [TERM-001] ¬´–ù–∞–∑–≤–∞–Ω–∏–µ —Ç–µ—Ä–º–∏–Ω–∞¬ª
**–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è:**
- –î–æ–∫—É–º–µ–Ω—Ç A: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 1
- –î–æ–∫—É–º–µ–Ω—Ç B: –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ 2

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** ...

## 2. –¢–µ—Ä–º–∏–Ω—ã —Å –≤–∞—Ä–∏–∞—Ü–∏—è–º–∏ (—Å–∏–Ω–æ–Ω–∏–º—ã)

| –û—Å–Ω–æ–≤–Ω–æ–π —Ç–µ—Ä–º–∏–Ω | –í–∞—Ä–∏–∞—Ü–∏–∏ | –î–æ–∫—É–º–µ–Ω—Ç—ã |
|-----------------|----------|-----------|
| ... | ... | ... |

## 3. –¢–µ—Ä–º–∏–Ω—ã –±–µ–∑ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π

- –¢–µ—Ä–º–∏–Ω 1 (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤: –¥–æ–∫1, –¥–æ–∫2)
- –¢–µ—Ä–º–∏–Ω 2 (—É–ø–æ–º–∏–Ω–∞–µ—Ç—Å—è –≤: –¥–æ–∫3)

## 4. –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–Ω–∏—Ñ–∏–∫–∞—Ü–∏–∏

1. ...
2. ...
"""

        return self.analyze(prompt, terms_context)

    def analyze_recommendations(self, documents: List['Document'], by_family: Dict[str, List['Document']]) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        # –°–æ–±–∏—Ä–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        stats_context = self._build_stats_context(documents, by_family)

        prompt = """–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ –∑–Ω–∞–Ω–∏–π –∏ —Å—Ñ–æ—Ä–º–∏—Ä—É–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –µ–≥–æ —Ä–∞–∑–≤–∏—Ç–∏—é.

–ó–∞–¥–∞—á–∏:
1. –û—Ü–µ–Ω–∏ –ø–æ–ª–Ω–æ—Ç—É –ø–æ–∫—Ä—ã—Ç–∏—è —Ç–µ–º –ø–æ –º–∞—Ç—Ä–∏—Ü–µ 3√ó3 (–ú–∏—Ä/–°–æ–∑–∏–¥–∞—Ç–µ–ª—å/–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ √ó –ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å/–ò–Ω–∂–µ–Ω–µ—Ä/–ú–µ–Ω–µ–¥–∂–µ—Ä)
2. –ù–∞–π–¥–∏ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã –≤ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏
3. –í—ã—è–≤–∏ –¥–æ–∫—É–º–µ–Ω—Ç—ã, —Ç—Ä–µ–±—É—é—â–∏–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –∏–ª–∏ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
4. –ü—Ä–µ–¥–ª–æ–∂–∏ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è

–§–æ—Ä–º–∞—Ç –æ—Ç–≤–µ—Ç–∞ –≤ Markdown:

## Executive Summary

–ö—Ä–∞—Ç–∫–∏–π –æ–±–∑–æ—Ä —Å–æ—Å—Ç–æ—è–Ω–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è).

## 1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã üî¥

### 1.1. [GAP-001] –ù–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–±–µ–ª–∞
**–û–ø–∏—Å–∞–Ω–∏–µ:** ...
**–í–ª–∏—è–Ω–∏–µ:** ...
**–†–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è:**
- –î–æ–∫—É–º–µ–Ω—Ç 1
- –î–æ–∫—É–º–µ–Ω—Ç 2

## 2. –í–∞–∂–Ω—ã–µ —É–ª—É—á—à–µ–Ω–∏—è üü°

### 2.1. [IMP-001] –ù–∞–∑–≤–∞–Ω–∏–µ —É–ª—É—á—à–µ–Ω–∏—è
**–û–ø–∏—Å–∞–Ω–∏–µ:** ...
**–î–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è:**
- [[–î–æ–∫—É–º–µ–Ω—Ç]] ‚Äî —á—Ç–æ –¥–æ–±–∞–≤–∏—Ç—å

## 3. –ü–ª–∞–Ω —Ä–∞–∑–≤–∏—Ç–∏—è –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1 (–∫—Ä–∏—Ç–∏—á–Ω–æ)
1. ...
2. ...

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2 (–≤–∞–∂–Ω–æ)
1. ...
2. ...

### –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 3 (–∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ)
1. ...

## 4. –ú–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è

| –ú–µ—Ç—Ä–∏–∫–∞ | –¢–µ–∫—É—â–µ–µ | –¶–µ–ª–µ–≤–æ–µ |
|---------|---------|---------|
| ... | ... | ... |
"""

        return self.analyze(prompt, stats_context)

    def _extract_terms_context(self, documents: List['Document']) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Ç–µ—Ä–º–∏–Ω–æ–≤ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤."""
        context_parts = []

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è–º–∏ (–≥–ª–æ—Å—Å–∞—Ä–∏–∏, –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏)
        priority_patterns = ["–≥–ª–æ—Å—Å–∞—Ä–∏–π", "—Ç–µ—Ä–º–∏–Ω", "–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏", "–∫–æ–Ω—Ü–µ–ø—Ü", "–ø–æ–Ω—è—Ç–∏–µ"]

        # –°–Ω–∞—á–∞–ª–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        priority_docs = []
        other_docs = []

        for doc in documents:
            name_lower = doc.name.lower()
            if any(p in name_lower for p in priority_patterns):
                priority_docs.append(doc)
            else:
                other_docs.append(doc)

        # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –ø–æ–ª–Ω–æ—Å—Ç—å—é
        for doc in priority_docs[:10]:
            context_parts.append(f"### {doc.name}\n\n{doc.body[:3000]}")

        # –î–æ–±–∞–≤–ª—è–µ–º –≤—ã–¥–µ—Ä–∂–∫–∏ –∏–∑ –æ—Å—Ç–∞–ª—å–Ω—ã—Ö
        for doc in other_docs[:30]:
            # –ò—â–µ–º –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è (–ø–∞—Ç—Ç–µ—Ä–Ω—ã —Ç–∏–ø–∞ "X ‚Äî —ç—Ç–æ Y" –∏–ª–∏ "X: Y")
            definitions = re.findall(
                r'(?:^|\n)([–ê-–Ø–ÅA-Z][–∞-—è—ëa-zA-Z\s]+?)(?:\s*[‚Äî‚Äì-]\s*|\s*:\s*)([^\n]{20,200})',
                doc.body
            )
            if definitions:
                context_parts.append(f"### {doc.name}\n")
                for term, definition in definitions[:5]:
                    context_parts.append(f"- **{term.strip()}**: {definition.strip()}")

        return "\n\n".join(context_parts)[:15000]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç

    def _build_stats_context(self, documents: List['Document'], by_family: Dict[str, List['Document']]) -> str:
        """–ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π."""
        context = f"## –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n\n"
        context += f"- –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(documents)}\n"

        for family, docs in sorted(by_family.items()):
            context += f"- {family}: {len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n"

        context += "\n## –î–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º\n\n"

        for family, docs in sorted(by_family.items()):
            context += f"### {family}\n"
            for doc in docs[:10]:
                status = "‚úÖ" if not doc.is_empty else "‚ö†Ô∏è –ø—É—Å—Ç–æ–π"
                context += f"- {doc.name} ({status})\n"
            if len(docs) > 10:
                context += f"- ... –∏ –µ—â—ë {len(docs) - 10}\n"
            context += "\n"

        context += "\n## –ü—É—Å—Ç—ã–µ/–Ω–µ–∑–∞–ø–æ–ª–Ω–µ–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã\n\n"
        empty_docs = [d for d in documents if d.is_empty]
        for doc in empty_docs[:20]:
            context += f"- {doc.name}\n"

        return context[:15000]


# –°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ –ø–∞–ø–æ–∫ —Å–µ–º–µ–π—Å—Ç–≤–∞–º
FOLDER_TO_FAMILY = {
    "0. –£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ": "F0",
    "1. –ú–∏—Ä (–ù–∞–¥—Å–∏—Å—Ç–µ–º–∞)": None,  # –û–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –ø–æ –ø–æ–¥–ø–∞–ø–∫–µ
    "1.1.": "F1",
    "1.2.": "F2",
    "1.3.": "F3",
    "2. –°–æ–∑–∏–¥–∞—Ç–µ–ª—å (–¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞)": None,
    "2.1.": "F4",
    "2.2.": "F5",
    "2.3.": "F6",
    "3. –≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ —Ä–∞–∑–≤–∏—Ç–∏—è (–°–∏—Å—Ç–µ–º–∞ —Å–æ–∑–¥–∞–Ω–∏—è)": None,
    "3.1.": "F7",
    "3.2.": "F8",
    "3.3.": "F9",
}


class Document:
    """–ü—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""

    def __init__(self, path: Path):
        self.path = path
        self.relative_path = path.relative_to(CONTENT_DIR) if path.is_relative_to(CONTENT_DIR) else path
        self.name = path.stem
        self.content = ""
        self.frontmatter: Dict[str, Any] = {}
        self.body = ""
        self.wikilinks: List[str] = []
        self.headings: List[Tuple[int, str]] = []
        self.family: Optional[str] = None
        self.size = 0
        self._parse()

    def _parse(self):
        """–ü–∞—Ä—Å–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–∞: frontmatter, –∫–æ–Ω—Ç–µ–Ω—Ç, —Å—Å—ã–ª–∫–∏."""
        try:
            self.content = self.path.read_text(encoding="utf-8")
            self.size = len(self.content)
        except Exception as e:
            print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {self.path}: {e}")
            return

        # –ü–∞—Ä—Å–∏–Ω–≥ frontmatter
        if self.content.startswith("---"):
            parts = self.content.split("---", 2)
            if len(parts) >= 3:
                try:
                    self.frontmatter = yaml.safe_load(parts[1]) or {}
                except yaml.YAMLError:
                    self.frontmatter = {}
                self.body = parts[2].strip()
            else:
                self.body = self.content
        else:
            self.body = self.content

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ wikilinks
        self.wikilinks = re.findall(r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]', self.body)

        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        self.headings = [(len(m.group(1)), m.group(2))
                         for m in re.finditer(r'^(#{1,6})\s+(.+)$', self.body, re.MULTILINE)]

        # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–∞
        self.family = self._detect_family()

    def _detect_family(self) -> Optional[str]:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å–µ–º–µ–π—Å—Ç–≤–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –ø—É—Ç–∏ –∏ frontmatter."""
        # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: frontmatter
        if "family" in self.frontmatter:
            return self.frontmatter["family"]

        # –ü–æ –ø—É—Ç–∏
        path_str = str(self.relative_path)
        for pattern, family in FOLDER_TO_FAMILY.items():
            if pattern in path_str:
                if family:
                    return family
                # –î–ª—è –∫–æ—Ä–Ω–µ–≤—ã—Ö –ø–∞–ø–æ–∫ —Å–º–æ—Ç—Ä–∏–º –ø–æ–¥–ø–∞–ø–∫—É
                for sub_pattern, sub_family in FOLDER_TO_FAMILY.items():
                    if sub_pattern in path_str and sub_family:
                        return sub_family

        return None

    @property
    def is_empty(self) -> bool:
        """–î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø—É—Å—Ç—ã–º, –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç < 200 —Å–∏–º–≤–æ–ª–æ–≤ –∏–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ç–æ–ª—å–∫–æ TODO."""
        body_clean = re.sub(r'<!--.*?-->', '', self.body, flags=re.DOTALL)
        body_clean = re.sub(r'TODO|FIXME', '', body_clean, flags=re.IGNORECASE)
        return len(body_clean.strip()) < 200

    @property
    def status(self) -> str:
        return self.frontmatter.get("status", "unknown")

    @property
    def doc_type(self) -> str:
        return self.frontmatter.get("type", "unknown")


class ReportGenerator:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç—á—ë—Ç–æ–≤."""

    def __init__(self, ai_analyzer: Optional[AIAnalyzer] = None):
        self.documents: List[Document] = []
        self.by_family: Dict[str, List[Document]] = defaultdict(list)
        self.timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        self.git_hash = self._get_git_hash()
        self.ai_analyzer = ai_analyzer

    def _get_git_hash(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ git commit hash."""
        try:
            import subprocess
            result = subprocess.run(
                ["git", "rev-parse", "--short", "HEAD"],
                capture_output=True, text=True, check=True
            )
            return result.stdout.strip()
        except:
            return "unknown"

    def scan_documents(self):
        """–°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ —Ö—Ä–∞–Ω–∏–ª–∏—â–µ."""
        print("üìÇ –°–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")

        for md_file in CONTENT_DIR.rglob("*.md"):
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ñ–∞–π–ª—ã
            if any(skip in str(md_file) for skip in [".obsidian", "node_modules", ".git"]):
                continue

            doc = Document(md_file)
            self.documents.append(doc)

            if doc.family:
                self.by_family[doc.family].append(doc)

        print(f"   –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}")
        for family, docs in sorted(self.by_family.items()):
            print(f"   {family}: {len(docs)}")

    def generate(self, report_type: str) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞."""
        generators = {
            "architecture-snapshot": self._generate_architecture_snapshot,
            "content-completeness": self._generate_content_completeness,
            "technical-issues": self._generate_technical_issues,
            "terminology": self._generate_terminology,
            "recommendations": self._generate_recommendations,
            "links-map": self._generate_links_map,
        }

        if report_type not in generators:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –æ—Ç—á—ë—Ç–∞: {report_type}")

        return generators[report_type]()

    def _header(self, title: str, extra: str = "") -> str:
        """–ó–∞–≥–æ–ª–æ–≤–æ–∫ –æ—Ç—á—ë—Ç–∞."""
        return f"""# {title}

> –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω: {self.timestamp}
> –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(self.documents)}
> –í–µ—Ä—Å–∏—è —Ö—Ä–∞–Ω–∏–ª–∏—â–∞: {self.git_hash}
{extra}
---

"""

    # ==================== –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–ô –°–õ–ï–ü–û–ö ====================

    def _generate_architecture_snapshot(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞'."""
        report = self._header(
            "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞",
            "\n**–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞**: –ò–µ—Ä–∞—Ä—Ö–∏—á–µ—Å–∫–∞—è ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ª–æ–≥–∏–∫–µ –æ–ø–∏—Å–∞–Ω–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã –æ—Ç —Ü–µ–ª–µ–π –∫ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏.\n"
        )

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ä–∞–∑–¥–µ–ª–æ–≤
        report += self._architecture_heatmap()

        # –†–∞–∑–¥–µ–ª—ã
        report += self._architecture_section_1_mission()
        report += self._architecture_section_2_personas()
        report += self._architecture_section_3_goals()
        report += self._architecture_section_4_creator()
        report += self._architecture_section_5_functioning()
        report += self._architecture_section_6_platform()
        report += self._architecture_section_7_data()
        report += self._architecture_section_8_epistemic()
        report += self._architecture_section_9_economy()
        report += self._architecture_section_10_quality()
        report += self._architecture_section_11_metrics()
        report += self._architecture_section_12_stats()
        report += self._architecture_section_13_links()

        return report

    def _architecture_heatmap(self) -> str:
        """–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º F0-F9 —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ 0.4.1."""

        # –¢–∏–ø–∏—á–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ –∫–∞–∂–¥–æ–º—É —Å–µ–º–µ–π—Å—Ç–≤—É (–∏–∑ –¢–ó)
        typical_docs = {
            "F0": ["–º–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç", "–≥–ª–æ—Å—Å–∞—Ä–∏–π", "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü"],
            "F1": ["–º–∞–Ω–∏—Ñ–µ—Å—Ç", "–ø—Ä–æ–±–ª–µ–º", "—Ü–µ–ª–µ–≤", "jtbd", "–∞—É–¥–∏—Ç–æ—Ä"],
            "F2": ["–∫–æ–Ω—Ü–µ–ø—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è", "—Å—Ü–µ–Ω–∞—Ä–∏", "–∫–æ–Ω—Ç–µ–∫—Å—Ç", "–¥–∏–∞–≥—Ä–∞–º–º"],
            "F3": ["–∫–æ–º–º—É–Ω–∏–∫–∞—Ü", "–ø–∞—Ä—Ç–Ω—ë—Ä", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "compliance"],
            "F4": ["—Ü–µ–Ω–Ω–æ—Å—Ç–Ω", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü", "–±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª", "–æ—Ñ—Ñ–µ—Ä"],
            "F5": ["—Å–æ–∑–∏–¥–∞—Ç–µ–ª", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü", "–º–∞—Å—Ç–µ—Ä—Å—Ç–≤", "–º–æ–¥–µ–ª—å"],
            "F6": ["–æ–Ω–±–æ—Ä–¥–∏–Ω–≥", "–º–∞—Ä—à—Ä—É—Ç", "–¥–µ–∫–∞–Ω–∞—Ç", "–º–µ—Ç—Ä–∏–∫"],
            "F7": ["—ç–∫–æ–Ω–æ–º–∏–∫", "—Ç–æ–∫–µ–Ω–æ–º–∏–∫", "–∏–Ω–≤–µ—Å—Ç–∏—Ü"],
            "F8": ["–ø–ª–∞—Ç—Ñ–æ—Ä–º", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "—Å–∏—Å—Ç–µ–º", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç"],
            "F9": ["—Ä–æ–ª", "—Ä–∏—Ç–º", "—Å–ª—É–∂–±", "–∫–æ–º–∞–Ω–¥", "—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü"],
        }

        # –ì–ª–∞–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã —Å–µ–º–µ–π—Å—Ç–≤ (–∏–∑ –ú–æ–¥–µ–ª–∏ —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1)
        main_questions = {
            "F0": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª–∞ –∏ –æ–Ω—Ç–æ–ª–æ–≥–∏—è?",
            "F1": "–ó–∞—á–µ–º –º–∏—Ä—É —ç—Ç–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞?",
            "F2": "–ö–∞–∫ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –º–∏—Ä?",
            "F3": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º —Å –≤–Ω–µ—à–Ω–∏–º –º–∏—Ä–æ–º?",
            "F4": "–ö–∞–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?",
            "F5": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?",
            "F6": "–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—è?",
            "F7": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã?",
            "F8": "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞?",
            "F9": "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–º–∞–Ω–¥–∞?",
        }

        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"
        heatmap += "| –°–µ–º–µ–π—Å—Ç–≤–æ | –ù–∞–∑–≤–∞–Ω–∏–µ | –°—Ç–∞—Ç—É—Å | –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ | –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π |\n"
        heatmap += "|-----------|----------|--------|------------|-------------|\n"

        status_counts = {"üü¢": 0, "üü°": 0, "üî¥": 0}

        for family_id in ["F0", "F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            family = FAMILIES[family_id]
            docs = self.by_family.get(family_id, [])
            count = len(docs)

            # –ê–Ω–∞–ª–∏–∑ —Å–æ–¥–µ—Ä–∂–∞–Ω–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            typical_patterns = typical_docs.get(family_id, [])
            found_typical = 0
            meaningful_docs = 0  # –¥–æ–∫—É–º–µ–Ω—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∑–∞–≥–ª—É—à–∫–∞–º–∏
            complete_docs = 0    # –¥–æ–∫—É–º–µ–Ω—Ç—ã, –æ—Ç–≤–µ—á–∞—é—â–∏–µ –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å

            for doc in docs:
                doc_name_lower = doc.name.lower()
                doc_body_lower = doc.body.lower()

                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–µ —Ç–∏–ø–∏—á–Ω—ã–º –¥–æ–∫—É–º–µ–Ω—Ç–∞–º (–≤ –Ω–∞–∑–≤–∞–Ω–∏–∏ –ò–õ–ò —Ç–µ–ª–µ)
                is_typical = False
                for pattern in typical_patterns:
                    if pattern in doc_name_lower or pattern in doc_body_lower:
                        found_typical += 1
                        is_typical = True
                        break

                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                word_count = len(doc.body.split())
                is_stub = self._is_stub_document(doc)
                answers_main_question = self._answers_main_question(doc, main_questions[family_id])

                # –î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –æ—Å–º—ã—Å–ª–µ–Ω–Ω—ã–º, –µ—Å–ª–∏:
                # - –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –∑–∞–≥–ª—É—à–∫–æ–π
                # - –∏–º–µ–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–ª–∏–Ω—É (> 200 —Å–ª–æ–≤) –ò–õ–ò –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å
                if not is_stub and (word_count >= 200 or answers_main_question):
                    meaningful_docs += 1

                # –î–æ–∫—É–º–µ–Ω—Ç —Å—á–∏—Ç–∞–µ—Ç—Å—è –ø–æ–ª–Ω—ã–º, –µ—Å–ª–∏ –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å–µ–º–µ–π—Å—Ç–≤–∞
                if answers_main_question:
                    complete_docs += 1

            # –û—Ü–µ–Ω–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–æ–≥–ª–∞—Å–Ω–æ –¢–ó (–ø. 2.3)
            # üü¢ –ü–æ–ª–Ω—ã–π: ‚â• 70% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ò –æ—Ç–≤–µ—á–∞—é—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å
            # üü° –ß–∞—Å—Ç–∏—á–Ω—ã–π: 30‚Äì69% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –ò–õ–ò –¥–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ–ø–æ–ª–Ω—ã–µ
            # üî¥ –ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–π: < 30% —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ò–õ–ò —Ç–æ–ª—å–∫–æ –∑–∞–≥–ª—É—à–∫–∏

            typical_ratio = found_typical / len(typical_patterns) if typical_patterns else 0
            meaningful_ratio = meaningful_docs / count if count > 0 else 0
            complete_ratio = complete_docs / count if count > 0 else 0

            if typical_ratio >= 0.7 and complete_ratio >= 0.5:
                status = "üü¢"
                comment = "–ö–ª—é—á–µ–≤—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç"
            elif typical_ratio >= 0.3 or meaningful_ratio >= 0.5:
                status = "üü°"
                if complete_ratio < 0.3:
                    comment = f"{int(complete_ratio*100)}% –æ—Ç–≤–µ—á–∞—é—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å"
                elif meaningful_ratio < 0.5:
                    comment = f"{int(meaningful_ratio*100)}% —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"
                else:
                    comment = f"–ù–∞–π–¥–µ–Ω–æ {found_typical}/{len(typical_patterns)} —Ç–∏–ø–∏—á–Ω—ã—Ö"
            else:
                status = "üî¥"
                if count == 0:
                    comment = "–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç"
                elif found_typical == 0:
                    comment = f"–ù–µ—Ç —Ç–∏–ø–∏—á–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ (–µ—Å—Ç—å {count} –¥—Ä—É–≥–∏—Ö)"
                elif meaningful_docs == 0:
                    comment = f"–¢–æ–ª—å–∫–æ –∑–∞–≥–ª—É—à–∫–∏ ({count} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)"
                else:
                    comment = f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"

            status_counts[status] += 1
            heatmap += f"| {family_id} | {family['name']} | {status} | {count} | {comment} |\n"

        heatmap += f"\n**–û–±—â–∏–π —Å—Ç–∞—Ç—É—Å:** üü¢ {status_counts['üü¢']} | üü° {status_counts['üü°']} | üî¥ {status_counts['üî¥']}\n"

        return heatmap + "\n---\n\n"

    def _is_stub_document(self, doc) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥–ª—É—à–∫–æ–π (stub)."""
        body_lower = doc.body.lower()

        # –ü—Ä–∏–∑–Ω–∞–∫–∏ –∑–∞–≥–ª—É—à–∫–∏
        stub_indicators = [
            "todo:", "tbd", "–∑–∞–≥–ª—É—à–∫–∞", "–≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ", "–¥–æ–∫—É–º–µ–Ω—Ç –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ",
            "placeholder", "stub", "–ø—É—Å—Ç–æ–π", "–Ω–µ –∑–∞–ø–æ–ª–Ω–µ–Ω"
        ]

        # –ï—Å–ª–∏ > 30% —Ç–µ–∫—Å—Ç–∞ - –∑–∞–≥–ª—É—à–∫–∏, —Å—á–∏—Ç–∞–µ–º –¥–æ–∫—É–º–µ–Ω—Ç –∑–∞–≥–ª—É—à–∫–æ–π
        words = doc.body.split()
        if len(words) < 50:  # —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç
            return True

        stub_words = 0
        for word in words:
            word_lower = word.lower().strip('.,!?;:')
            if any(indicator in word_lower for indicator in stub_indicators):
                stub_words += 1

        return (stub_words / len(words)) > 0.3

    def _answers_main_question(self, doc, main_question: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –æ—Ç–≤–µ—á–∞–µ—Ç –ª–∏ –¥–æ–∫—É–º–µ–Ω—Ç –Ω–∞ –≥–ª–∞–≤–Ω—ã–π –≤–æ–ø—Ä–æ—Å —Å–µ–º–µ–π—Å—Ç–≤–∞."""
        # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞: –¥–æ–∫—É–º–µ–Ω—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –≤–æ–ø—Ä–æ—Å–∞
        # –∏ –∏–º–µ–µ—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—É—é –¥–ª–∏–Ω—É/—Å—Ç—Ä—É–∫—Ç—É—Ä—É
        question_keywords = {
            "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω—ã –ø—Ä–∞–≤–∏–ª–∞ –∏ –æ–Ω—Ç–æ–ª–æ–≥–∏—è?": ["–ø—Ä–∞–≤–∏–ª", "–æ–Ω—Ç–æ–ª–æ–≥–∏", "–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü", "–≥–ª–æ—Å—Å–∞—Ä–∏", "—Å—Ç–∞–Ω–¥–∞—Ä—Ç"],
            "–ó–∞—á–µ–º –º–∏—Ä—É —ç—Ç–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º–∞?": ["–º–∏—Å—Å–∏", "–ø—Ä–æ–±–ª–µ–º", "—Ü–µ–ª—å", "—Ü–µ–Ω–Ω–æ—Å—Ç—å", "–º–∞–Ω–∏—Ñ–µ—Å—Ç"],
            "–ö–∞–∫ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å –≤—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç—Å—è –≤ –º–∏—Ä?": ["–∫–æ–Ω—Ü–µ–ø—Ü–∏", "—Å—Ü–µ–Ω–∞—Ä–∏", "–∫–æ–Ω—Ç–µ–∫—Å—Ç", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω"],
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ–º —Å –≤–Ω–µ—à–Ω–∏–º –º–∏—Ä–æ–º?": ["–∫–æ–º–º—É–Ω–∏–∫–∞—Ü", "–ø–∞—Ä—Ç–Ω—ë—Ä", "—Ä–µ–≥—É–ª—è—Ç–æ—Ä", "–≤–Ω–µ—à–Ω", "—Å–≤—è–∑"],
            "–ö–∞–∫—É—é —Ü–µ–Ω–Ω–æ—Å—Ç—å –ø–æ–ª—É—á–∞–µ—Ç —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?": ["—Ü–µ–Ω–Ω–æ—Å—Ç", "–ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞", "–±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª", "—Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü"],
            "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω —Å–æ–∑–∏–¥–∞—Ç–µ–ª—å?": ["–º–æ–¥–µ–ª—å", "–∫–æ–º–ø–µ—Ç–µ–Ω—Ü", "–º–∞—Å—Ç–µ—Ä—Å—Ç–≤", "—Å–æ–∑–∏–¥–∞—Ç–µ–ª"],
            "–ö–∞–∫ –ø—Ä–æ–∏—Å—Ö–æ–¥–∏—Ç —Ä–∞–∑–≤–∏—Ç–∏–µ —Å–æ–∑–∏–¥–∞—Ç–µ–ª—è?": ["–æ–Ω–±–æ—Ä–¥–∏–Ω–≥", "–º–∞—Ä—à—Ä—É—Ç", "—Ä–∞–∑–≤–∏—Ç–∏", "–æ–±—É—á–µ–Ω", "–º–µ—Ç—Ä–∏–∫"],
            "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ —ç–∫–æ–Ω–æ–º–∏–∫–∞ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã?": ["—ç–∫–æ–Ω–æ–º–∏–∫", "—Ç–æ–∫–µ–Ω–æ–º–∏–∫", "–∏–Ω–≤–µ—Å—Ç–∏—Ü", "—Ñ–∏–Ω–∞–Ω—Å"],
            "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞?": ["–ø–ª–∞—Ç—Ñ–æ—Ä–º", "–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä", "—Å–∏—Å—Ç–µ–º", "–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç", "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏"],
            "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞–µ—Ç –∫–æ–º–∞–Ω–¥–∞?": ["–∫–æ–º–∞–Ω–¥", "—Ä–æ–ª", "—Å–ª—É–∂–±", "—Ä–∏—Ç–º", "—ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü"]
        }

        keywords = question_keywords.get(main_question, [])
        body_lower = doc.body.lower()

        # –î–æ–∫—É–º–µ–Ω—Ç –æ—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –≤–æ–ø—Ä–æ—Å, –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ö–æ—Ç—è –±—ã 2 –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤–∞
        found_keywords = sum(1 for keyword in keywords if keyword in body_lower)
        return found_keywords >= 2

    def _architecture_section_1_mission(self) -> str:
        """–†–∞–∑–¥–µ–ª 1: –ó–∞—á–µ–º, –¥–ª—è –∫–æ–≥–æ, —á—Ç–æ –º—ã –¥–µ–ª–∞–µ–º."""
        section = "## 1. –ó–∞—á–µ–º, –¥–ª—è –∫–æ–≥–æ, —á—Ç–æ –º—ã –¥–µ–ª–∞–µ–º\n\n"

        # –ò—â–µ–º –º–∞–Ω–∏—Ñ–µ—Å—Ç
        manifesto = self._find_doc_by_pattern("–ú–∞–Ω–∏—Ñ–µ—Å—Ç")
        if manifesto:
            section += "### 1.1. –ú–∏—Å—Å–∏—è —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã\n\n"
            section += self._extract_summary(manifesto, max_sentences=3)
            section += "\n\n"

        section += "### 1.2. –¢—Ä–∏ –∏–Ω–≤–∞—Ä–∏–∞–Ω—Ç–∞ (–∫–æ–Ω—Å—Ç–∏—Ç—É—Ü–∏–æ–Ω–Ω—ã–π —Å–ª–æ–π)\n\n"
        section += "- **–≠–≤–æ–ª—é—Ü–∏–æ–Ω–Ω–æ—Å—Ç—å** ‚Äî –ª—é–±–æ–π –ø—Ä–æ—Ü–µ—Å—Å —Å–ø–æ—Å–æ–±–µ–Ω –∫ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º—É —É–ª—É—á—à–µ–Ω–∏—é —á–µ—Ä–µ–∑ —Ü–∏–∫–ª—ã –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏\n"
        section += "- **–°–∫–≤–æ–∑–Ω–∞—è —Ü–µ–ª–æ—Å—Ç–Ω–æ—Å—Ç—å** ‚Äî –æ—Ç –ª–∏—á–Ω—ã—Ö —Ü–µ–ª–µ–π –¥–æ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö —Å–∏—Å—Ç–µ–º –µ–¥–∏–Ω–∞—è —Ç—Ä–∞—Å—Å–∏—Ä–æ–≤–∫–∞\n"
        section += "- **–î–∏–¥–∞–∫—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç—å** ‚Äî –∫–∞–∂–¥—ã–π –∞—Ä—Ç–µ—Ñ–∞–∫—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —Å–≤–æ—é –ø—Ä–∏—á–∏–Ω–Ω–æ-—Å–ª–µ–¥—Å—Ç–≤–µ–Ω–Ω—É—é –¥–æ—Ä–æ–∂–∫—É\n\n"

        section += "### 1.3. –ü—Ä–∏–Ω—Ü–∏–ø—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏\n\n"
        section += "- C4+ADR –¥–ª—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω–æ–≥–æ –æ–ø–∏—Å–∞–Ω–∏—è\n"
        section += "- –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–∞—è –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è\n"
        section += "- –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ Method / MethodDescription / Work\n\n"

        sources = [d.name for d in self.by_family.get("F1", [])[:3]]
        section += f"**–ò—Å—Ç–æ—á–Ω–∏–∫–∏:** {', '.join(f'[[{s}]]' for s in sources)}\n\n"

        return section + "---\n\n"

    def _architecture_section_2_personas(self) -> str:
        """–†–∞–∑–¥–µ–ª 2: –ü–µ—Ä—Å–æ–Ω—ã –∏ —Ä–æ–ª–∏."""
        section = "## 2. –ü–µ—Ä—Å–æ–Ω—ã –∏ —Ä–æ–ª–∏\n\n"

        section += "### 2.1. –¶–µ–ª–µ–≤—ã–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏\n\n"
        ta_doc = self._find_doc_by_pattern("–¶–µ–ª–µ–≤—ã–µ –∞—É–¥–∏—Ç–æ—Ä–∏–∏")
        if ta_doc:
            section += self._extract_summary(ta_doc, max_sentences=3)
        else:
            section += "*–î–æ–∫—É–º–µ–Ω—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω*\n"
        section += "\n\n"

        section += "### 2.2. –†–æ–ª–∏ –≤ —ç–∫–æ—Å–∏—Å—Ç–µ–º–µ\n\n"
        section += "- –£—á–µ–Ω–∏–∫/–°—Ç–∞–∂—ë—Ä\n"
        section += "- –°–æ–∑–¥–∞—Ç–µ–ª—å/–ú–µ—Ç–æ–¥–∏—Å—Ç\n"
        section += "- –ö—É—Ä–∞—Ç–æ—Ä/–ú–µ–Ω—Ç–æ—Ä\n"
        section += "- –†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫ –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- –û—Ä–≥–∞–Ω–∏–∑–∞—Ç–æ—Ä/–û—É–Ω–µ—Ä –ø—Ä–æ–µ–∫—Ç–∞\n"
        section += "- –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å–æ–≤–µ—Ç\n\n"

        return section + "---\n\n"

    def _architecture_section_3_goals(self) -> str:
        """–†–∞–∑–¥–µ–ª 3: –ü—Ä–æ–±–ª–µ–º—ã, –≥–∏–ø–æ—Ç–µ–∑—ã –∏ —Ü–µ–ª–∏."""
        section = "## 3. –ü—Ä–æ–±–ª–µ–º—ã, –≥–∏–ø–æ—Ç–µ–∑—ã –∏ —Ü–µ–ª–∏\n\n"

        section += "### 3.1. –ö–ª—é—á–µ–≤—ã–µ –ø—Ä–æ–±–ª–µ–º—ã\n\n"
        problems_doc = self._find_doc_by_pattern("–ü—Ä–æ–±–ª–µ–º—ã")
        if problems_doc:
            section += self._extract_summary(problems_doc, max_sentences=5)
        else:
            section += "- –°–≤–æ–±–æ–¥–∞ –±–µ–∑ ¬´–≥—Ä–∞–º–º–∞—Ç–∏–∫–∏¬ª –ª–∏—á–Ω–æ–π —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏\n"
            section += "- ¬´–ò–Ω—Ç–µ–ª–ª–µ–∫—Ç –∫–∞–∫ —É–∑–∫–æ–µ –º–µ—Å—Ç–æ¬ª\n"
            section += "- –û–±—É—á–µ–Ω–∏–µ ¬´—Ä—è–¥–æ–º¬ª, –∞ –Ω–µ –≤–Ω—É—Ç—Ä–∏ –ø—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–∞\n"
        section += "\n\n"

        section += "### 3.2. –ì–∏–ø–æ—Ç–µ–∑—ã —Ä–µ—à–µ–Ω–∏—è\n\n"
        hypotheses_doc = self._find_doc_by_pattern("–ì–∏–ø–æ—Ç–µ–∑—ã")
        if hypotheses_doc:
            section += self._extract_summary(hypotheses_doc, max_sentences=5)
        section += "\n\n"

        section += "### 3.3. –¶–µ–ª–∏ –ø–æ –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–º\n\n"
        section += "- **2026** ‚Äî –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞ –∏ —Å–±–æ—Ä–∫–∞ —è–¥—Ä–∞\n"
        section += "- **2027‚Äì2030** ‚Äî –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã\n"
        section += "- **–ü–æ—Å–ª–µ 2030** ‚Äî –∫—É–ª—å—Ç—É—Ä–Ω–∞—è –Ω–æ—Ä–º–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_4_creator(self) -> str:
        """–†–∞–∑–¥–µ–ª 4: –¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞ - –°–æ–∑–∏–¥–∞—Ç–µ–ª—å."""
        section = "## 4. –¶–µ–ª–µ–≤–∞—è —Å–∏—Å—Ç–µ–º–∞: –°–æ–∑–∏–¥–∞—Ç–µ–ª—å\n\n"

        section += "### 4.1. –ß—Ç–æ —Ç–∞–∫–æ–µ –°–æ–∑–∏–¥–∞—Ç–µ–ª—å\n\n"
        creator_doc = self._find_doc_by_pattern("–ö–æ–Ω—Ü–µ–ø—Ü–∏—è —Å–æ–∑–∏–¥–∞—Ç–µ–ª—è")
        if creator_doc:
            section += self._extract_summary(creator_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.2. –ú–æ–¥–µ–ª—å –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π –∏ —É—Ä–æ–≤–Ω–∏ –º–∞—Å—Ç–µ—Ä—Å—Ç–≤–∞\n\n"
        competencies_doc = self._find_doc_by_pattern("–ö–∞—Ä—Ç–∞ –∫–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–π|–ö–æ–º–ø–µ—Ç–µ–Ω—Ü–∏–∏")
        if competencies_doc:
            section += self._extract_summary(competencies_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.3. –¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ\n\n"
        value_doc = self._find_doc_by_pattern("–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ")
        if value_doc:
            section += self._extract_summary(value_doc, max_sentences=3)
        section += "\n\n"

        section += "### 4.4. –†–æ–ª–µ–≤–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è\n\n"
        section += "–£—á–µ–Ω–∏–∫ ‚Üí –ò–Ω—Ç–µ–ª–ª–µ–∫—Ç—É–∞–ª ‚Üí –ü—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª ‚Üí –ò—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å ‚Üí –ü—Ä–æ—Å–≤–µ—Ç–∏—Ç–µ–ª—å\n\n"

        return section + "---\n\n"

    def _architecture_section_5_functioning(self) -> str:
        """–†–∞–∑–¥–µ–ª 5: –§—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã."""
        section = "## 5. –§—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ —ç–∫–æ—Å–∏—Å—Ç–µ–º—ã\n\n"

        section += "### 5.1. –ì–ª–∞–≤–Ω—ã–µ –ø—Ä–æ—Ü–µ—Å—Å—ã (—Å–∫–≤–æ–∑–Ω—ã–µ —Ü–∏–∫–ª—ã)\n\n"
        section += "- **C1.** –û–Ω–±–æ—Ä–¥–∏–Ω–≥ –∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –¥–≤–æ–π–Ω–∏–∫–∞\n"
        section += "- **C2.** –û–±—É—á–µ–Ω–∏–µ ‚Üí –ê—Ä—Ç–µ—Ñ–∞–∫—Ç ‚Üí –ü–µ—Ä–µ–Ω–æ—Å\n"
        section += "- **C3.** –ü—Ä–æ–∏–∑–≤–æ–¥—Å—Ç–≤–æ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤\n"
        section += "- **C4.** –ö–æ–Ω—Å—Ç—Ä—É–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏—è –ò–ò-–∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- **C5.** –ü—Ä–æ–¥–≤–∏–∂–µ–Ω–∏–µ (–ø—Ä–æ—Å–≤–µ—â–µ–Ω–∏–µ, –º–∞—Ä–∫–µ—Ç–∏–Ω–≥, –ø—Ä–æ–¥–∞–∂–∏)\n"
        section += "- **C6.** –¢–æ–≤–∞—Ä–æ–æ–±–º–µ–Ω –∏ —Ä–∞—Å—á—ë—Ç—ã (—Ñ–∏–∞—Ç + —Ç–æ–∫–µ–Ω)\n"
        section += "- **C7.** –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞ (—Ç–æ–∫–µ–Ω)\n"
        section += "- **C8.** –ò—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–µ –Ω–∞—É—á–Ω–æ–≥–æ —Ñ—Ä–æ–Ω—Ç–∏—Ä–∞\n"
        section += "- **C9.** –ù–∞–±–ª—é–¥–∞–µ–º–æ—Å—Ç—å/–∫–∞—á–µ—Å—Ç–≤–æ\n"
        section += "- **C10.** –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞\n\n"

        section += "### 5.2. –ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –¥–ª—è —Ä–æ–ª–µ–π\n\n"
        conops_doc = self._find_doc_by_pattern("–ö–æ–Ω—Ü–µ–ø—Ü–∏—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        if conops_doc:
            section += self._extract_summary(conops_doc, max_sentences=5)
        section += "\n\n"

        return section + "---\n\n"

    def _architecture_section_6_platform(self) -> str:
        """–†–∞–∑–¥–µ–ª 6: –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ò–ò-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã."""
        section = "## 6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ò–ò-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã\n\n"

        section += "### 6.1. –ö–∞—Ä—Ç–∞ –ø–æ–¥—Å–∏—Å—Ç–µ–º\n\n"

        # –ò—â–µ–º –¥–æ–∫—É–º–µ–Ω—Ç—ã –ø–æ–¥—Å–∏—Å—Ç–µ–º
        subsystems = [d for d in self.by_family.get("F8", [])
                      if any(kw in d.name.lower() for kw in ["–ø–æ–¥—Å–∏—Å—Ç–µ–º", "—Å–∏—Å—Ç–µ–º–∞", "–ø–ª–∞—Ç—Ñ–æ—Ä–º–∞"])]

        if subsystems:
            section += "| ‚Ññ | –ü–æ–¥—Å–∏—Å—Ç–µ–º–∞ | –û–ø–∏—Å–∞–Ω–∏–µ |\n"
            section += "|---|------------|----------|\n"
            for i, doc in enumerate(subsystems[:10], 1):
                desc = self._extract_first_sentence(doc)
                section += f"| {i} | [[{doc.name}]] | {desc[:80]}... |\n"
        section += "\n"

        section += "### 6.2. –ú—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–∞—è –û–°: –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n\n"
        section += "- **Reasoning Core** ‚Äî –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –≤—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤\n"
        section += "- **Memory Store** ‚Äî —Ö—Ä–∞–Ω–∏–ª–∏—â–µ –∑–Ω–∞–Ω–∏–π –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞\n"
        section += "- **Tool/Action Interface** ‚Äî –¥–µ–∫–ª–∞—Ä–∞—Ç–∏–≤–Ω—ã–µ –¥–µ–π—Å—Ç–≤–∏—è (Apps SDK)\n"
        section += "- **Goal Manager** ‚Äî —Ü–µ–ª–∏, –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è, –±—é–¥–∂–µ—Ç—ã\n"
        section += "- **Dialogue Layer** ‚Äî –∫–æ–º–º—É–Ω–∏–∫–∞—Ü–∏—è —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º\n\n"

        section += "### 6.3. –°—Ç–∞–¥–∏–∏ –∂–∏–∑–Ω–µ–Ω–Ω–æ–≥–æ —Ü–∏–∫–ª–∞ –∞–≥–µ–Ω—Ç–∞\n\n"
        section += "1. **–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è** ‚Äî —Ü–µ–ª—å, –º–µ—Ç—Ä–∏–∫–∏, –∫–æ–Ω—Ç–µ–∫—Å—Ç\n"
        section += "2. **–ü–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ** ‚Äî –≤—ã–±–æ—Ä –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø–ª–∞–Ω –≤—ã–∑–æ–≤–æ–≤\n"
        section += "3. **–í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ** ‚Äî –≤—ã–∑–æ–≤—ã –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤, –ø—Ä–æ—Ç–æ–∫–æ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ\n"
        section += "4. **–û—Ü–µ–Ω–∫–∞** ‚Äî –ø–µ—Ä–µ—Å—á—ë—Ç —Ü–µ–Ω–Ω–æ—Å—Ç–∏, —Å–∞–º–æ–∫—Ä–∏—Ç–∏–∫–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_7_data(self) -> str:
        """–†–∞–∑–¥–µ–ª 7: –î–∞–Ω–Ω—ã–µ –∏ —Å—É—â–Ω–æ—Å—Ç–∏."""
        section = "## 7. –î–∞–Ω–Ω—ã–µ –∏ —Å—É—â–Ω–æ—Å—Ç–∏\n\n"

        section += "### 7.1. –°–∫–≤–æ–∑–Ω—ã–µ –º–æ–¥–µ–ª–∏ –¥–∞–Ω–Ω—ã—Ö\n\n"
        section += "- **Digital Twin** ‚Äî —Ü–µ–ª–∏, –Ω–∞–≤—ã–∫–∏, –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è, —Ä–∞—Å–ø–∏—Å–∞–Ω–∏–µ\n"
        section += "- **Epistemic Graph** ‚Äî —ç–ø–∏—Å—Ç–µ–º—ã –∏ –∏—Ö —Å—Ç–∞—Ç—É—Å—ã/—Å–≤—è–∑–∏/–¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞\n"
        section += "- **Activity Ledger** ‚Äî –≤—Å–µ —Å–æ–±—ã—Ç–∏—è (–æ–±—É—á–µ–Ω–∏–µ, –ø—Ä–æ–µ–∫—Ç—ã, –ø—É–±–ª–∏–∫–∞—Ü–∏–∏)\n"
        section += "- **Token Ledger** ‚Äî –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è/—Å–ø–∏—Å–∞–Ω–∏—è, –∑–∞–º–æ—Ä–æ–∑–∫–∏, —Ç–∞—Ä–∏—Ñ—ã\n\n"

        section += "### 7.2. –û—Å–Ω–æ–≤–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏\n\n"
        section += "`User`, `DigitalTwin`, `Program`, `Guide/Step/Task`, `Artifact`, "
        section += "`ActionEvent`, `Qualification`, `Episteme`, `Work`, `MethodDescription`, `Evidence`\n\n"

        section += "### 7.3. –ü–æ–ª–∏—Ç–∏–∫–∞ –±–∏–±–ª–∏–æ—Ç–µ–∫–∏\n\n"
        section += "- **curated/** ‚Äî —Å—Ç—Ä–æ–≥–æ —á–µ—Ä–µ–∑ PR/—Ä–µ–≤—å—é\n"
        section += "- **derived/** ‚Äî —Ç–æ–ª—å–∫–æ –∞–≤—Ç–æ–º–∞—Ç–æ–º (–∞–≥–µ–Ω—Ç—ã/ETL)\n\n"

        return section + "---\n\n"

    def _architecture_section_8_epistemic(self) -> str:
        """–†–∞–∑–¥–µ–ª 8: –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å."""
        section = "## 8. –≠–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–∏–π —Å—Ç–∞—Ç—É—Å –∏ –¥–æ–∫–∞–∑–∞—Ç–µ–ª—å—Å—Ç–≤–∞\n\n"

        section += "### 8.1. ESG (Epistemic Status Graph)\n\n"
        section += "**Draft** ‚Üí **PeerChecked** ‚Üí **Accepted** ‚Üí **Superseded**\n\n"

        section += "### 8.2. –°–∏–≥–Ω–∞–ª—ã –¥–ª—è —Å—Ç–∞—Ç—É—Å–∞\n\n"
        section += "- –ü–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –∏ –∏—Ö –ø–µ—Ä–µ–Ω–æ—Å\n"
        section += "- –†–µ—Ü–µ–Ω–∑–∏–∏ –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–æ–≤\n"
        section += "- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–∏–º–∏\n"
        section += "- –°—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç—å –Ω–µ–¥–µ–ª—å–Ω—ã—Ö –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç–æ–≤\n"
        section += "- Evidence-bindings –∫ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è–º\n\n"

        section += "### 8.3. –ü—Ä–æ—Ü–µ–¥—É—Ä–∞ –ø—Ä–∏—Å–≤–æ–µ–Ω–∏—è –∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è\n\n"
        section += "1. –°–±–æ—Ä —Å–∏–≥–Ω–∞–ª–æ–≤ –∏–∑ —Å–æ–±—ã—Ç–∏–π–Ω–æ–π —à–∏–Ω—ã\n"
        section += "2. –ù–æ—Ä–º–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –∑–∞–¥–∞—á–∏\n"
        section += "3. –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ä–µ–π—Ç–∏–Ω–≥\n"
        section += "4. –ß–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ (–ø–æ –ø–æ—Ä–æ–≥—É —Ä–∏—Å–∫–∞)\n"
        section += "5. –ü—É–±–ª–∏–∫–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ –∏ –ª–æ–≥–∏–∫–∞ ¬´—Å—Ç–∞—Ä–µ–Ω–∏—è¬ª\n\n"

        return section + "---\n\n"

    def _architecture_section_9_economy(self) -> str:
        """–†–∞–∑–¥–µ–ª 9: –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞."""
        section = "## 9. –≠–∫–æ–Ω–æ–º–∏–∫–∞ –≤–∫–ª–∞–¥–∞: Proof-of-Impact –∏ —Ç–æ–∫–µ–Ω–æ–º–∏–∫–∞\n\n"

        section += "### 9.1. –ò–¥–µ—è –∏ –ø–∞–π–ø–ª–∞–π–Ω –Ω–∞—á–∏—Å–ª–µ–Ω–∏—è\n\n"
        section += "–°–æ–±—ã—Ç–∏—è ‚Üí Work ‚Üí Episteme ‚Üí Evidence ‚Üí –ù–∞—á–∏—Å–ª–µ–Ω–∏–µ ‚Üí –ó–∞–º–æ—Ä–æ–∑–∫–∞\n\n"

        section += "### 9.2. –ö–∞–∫ –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å –∏ —Ç—Ä–∞—Ç–∏—Ç—å —Ç–æ–∫–µ–Ω—ã\n\n"
        section += "**–ó–∞—Ä–∞–±–æ—Ç–æ–∫:**\n"
        section += "- –ó–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥—ë–Ω–Ω—ã–µ —É—Ä–æ–∫–∏/—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏–∏\n"
        section += "- –ó–∞ —Ä–µ–≤—å—é —á—É–∂–∏—Ö –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤\n"
        section += "- –ó–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ —Å –¥–æ–∫–∞–∑–∞–Ω–Ω—ã–º –æ—Ö–≤–∞—Ç–æ–º\n"
        section += "- –ó–∞ –≤–∫–ª–∞–¥ –≤ –ø—Ä–æ–µ–∫—Ç—ã —Å –∏–∑–º–µ—Ä–∏–º—ã–º —ç—Ñ—Ñ–µ–∫—Ç–æ–º\n\n"
        section += "**–†–∞—Å—Ö–æ–¥:**\n"
        section += "- –î–æ—Å—Ç—É–ø –∫ –ø—Ä–µ–º–∏—É–º-–∫—É—Ä—Å–∞–º\n"
        section += "- –í—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã–µ –∫–≤–æ—Ç—ã –∞–≥–µ–Ω—Ç–æ–≤\n"
        section += "- –ú–∞—Ä–∫–µ—Ç–ø–ª–µ–π—Å –ò–ò-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–≤\n"
        section += "- –°–µ—Å—Å–∏–∏ —Å –Ω–∞—Å—Ç–∞–≤–Ω–∏–∫–∞–º–∏\n\n"

        section += "### 9.3. –í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –±–∏—Ä–∂–∞ –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ treasury\n\n"
        tokenomics_doc = self._find_doc_by_pattern("–¢–æ–∫–µ–Ω–æ–º–∏–∫–∞")
        if tokenomics_doc:
            section += self._extract_summary(tokenomics_doc, max_sentences=3)
        section += "\n\n"

        return section + "---\n\n"

    def _architecture_section_10_quality(self) -> str:
        """–†–∞–∑–¥–µ–ª 10: –ö—É–ª—å—Ç—É—Ä–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞."""
        section = "## 10. –ö—É–ª—å—Ç—É—Ä–∞ –∏ —Å—Ç–∞–Ω–¥–∞—Ä—Ç—ã –∫–∞—á–µ—Å—Ç–≤–∞\n\n"

        section += "### 10.1. –ü—Ä–∏–Ω—Ü–∏–ø—ã –∫–∞—á–µ—Å—Ç–≤–∞\n\n"
        section += "- **–î–µ–ª–∞—Ç—å-–ø–æ–∫–∞–∑—ã–≤–∞—Ç—å-–º–µ—Ä–∏—Ç—å-—É–ª—É—á—à–∞—Ç—å** ‚Äî –Ω–µ–¥–µ–ª—å–Ω—ã–µ –∏–Ω–∫—Ä–µ–º–µ–Ω—Ç—ã –∏ peer-review\n"
        section += "- **–í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å** ‚Äî –ª–æ–≥–∏, –≤–µ—Ä—Å–∏–∏, –∏—Å—Ç–æ—á–Ω–∏–∫–∏, –∫—Ä–∏—Ç–µ—Ä–∏–∏ –ø—Ä–∏—ë–º–∫–∏\n"
        section += "- **Curated vs Derived** ‚Äî —Ä—É—á–Ω–∞—è –∑–æ–Ω–∞ –æ—Ç–¥–µ–ª–µ–Ω–∞ –æ—Ç –∞–≤—Ç–æ-–∑–æ–Ω—ã\n"
        section += "- **–ú–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ –ø—Ä–∏–≤–∏–ª–µ–≥–∏–∏** ‚Äî Policy-as-code –≤ Apps SDK\n\n"

        section += "### 10.2. –ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∏ –ø—Ä–∏–Ω—è—Ç–∏–µ —Ä–µ—à–µ–Ω–∏–π\n\n"
        section += "- **C4 Model** ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä—ã ‚Üí –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã\n"
        section += "- **ADR** ‚Äî –∫–æ–Ω—Ç–µ–∫—Å—Ç ‚Üí —Ä–µ—à–µ–Ω–∏–µ ‚Üí trade-offs ‚Üí —Å—Å—ã–ª–∫–∏\n\n"

        return section + "---\n\n"

    def _architecture_section_11_metrics(self) -> str:
        """–†–∞–∑–¥–µ–ª 11: –ú–µ—Ç—Ä–∏–∫–∏."""
        section = "## 11. –ú–µ—Ç—Ä–∏–∫–∏\n\n"

        section += "### 11.1. –ö–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏\n\n"
        section += "- **Time-to-Master (TTM)** ‚Äî –≤—Ä–µ–º—è –¥–æ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —É—Ä–æ–≤–Ω—è –º–∞—Å—Ç–µ—Ä–∞\n"
        section += "- **Cost-to-Master (CTM)** ‚Äî —Å—É–º–º–∞—Ä–Ω—ã–µ –∑–∞—Ç—Ä–∞—Ç—ã (—Ñ–∏–∞—Ç + —Ç–æ–∫–µ–Ω—ã)\n"
        section += "- **–í—Ä–µ–º—è –æ–Ω–±–æ—Ä–¥–∏–Ω–≥–∞** ‚Äî <30 –º–∏–Ω—É—Ç –¥–æ –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ –¥–µ–π—Å—Ç–≤–∏—è\n"
        section += "- **–ö–∞—á–µ—Å—Ç–≤–æ –∏ –ø–µ—Ä–µ–Ω–æ—Å** ‚Äî –¥–æ–ª—è –∑–∞–¥–∞—á —Å –ø–µ—Ä–µ–Ω–æ—Å–æ–º (7‚Äì14 –¥–Ω–µ–π)\n"
        section += "- **–≠–∫–æ–Ω–æ–º–∏–∫–∞** ‚Äî MRR/NRR, –æ–±–æ—Ä–æ—Ç —Ç–æ–∫–µ–Ω–∞\n"
        section += "- **–†–µ–ø—É—Ç–∞—Ü–∏—è** ‚Äî –¥–∏–Ω–∞–º–∏–∫–∞ —ç–ø–∏—Å—Ç–µ–º–∏—á–µ—Å–∫–æ–≥–æ —Å—Ç–∞—Ç—É—Å–∞\n\n"

        return section + "---\n\n"

    def _architecture_section_12_stats(self) -> str:
        """–†–∞–∑–¥–µ–ª 12: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞."""
        section = "## 12. –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞\n\n"

        section += "| –ú–µ—Ç—Ä–∏–∫–∞ | –ó–Ω–∞—á–µ–Ω–∏–µ |\n"
        section += "|---------|----------|\n"
        section += f"| –í—Å–µ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {len(self.documents)} |\n"

        by_family_str = ", ".join(f"{f}: {len(docs)}" for f, docs in sorted(self.by_family.items()))
        section += f"| –ü–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º | {by_family_str} |\n"

        active = sum(1 for d in self.documents if d.status == "active")
        draft = sum(1 for d in self.documents if d.status == "draft")
        section += f"| –ê–∫—Ç–∏–≤–Ω—ã—Ö | {active} |\n"
        section += f"| –ß–µ—Ä–Ω–æ–≤–∏–∫–æ–≤ | {draft} |\n\n"

        return section + "---\n\n"

    def _architecture_section_13_links(self) -> str:
        """–†–∞–∑–¥–µ–ª 13: –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã."""
        return """## 13. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è 0.4]]
- [[–ú–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1]]
"""

    # ==================== –°–û–î–ï–†–ñ–ê–¢–ï–õ–¨–ù–ê–Ø –ü–û–õ–ù–û–¢–ê ====================

    def _generate_content_completeness(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è'."""
        report = self._header("–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è")

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ 3x3
        report += self._completeness_heatmap()

        # Executive Summary
        report += self._completeness_summary()

        # –ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º
        report += self._completeness_by_cells()

        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SoTA-–º–µ—Ç–æ–¥–æ–≤
        report += self._completeness_sota()

        # –ò–Ω—Ç–µ—Ä–µ—Å—ã —Å—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä–æ–≤
        report += self._completeness_stakeholders()

        # –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã
        report += self._completeness_gaps()

        # –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        report += self._completeness_links()

        return report

    def _completeness_heatmap(self) -> str:
        """–¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ 3x3 –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã."""
        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã\n\n"

        # –û–∂–∏–¥–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ —Å–µ–º–µ–π—Å—Ç–≤–∞–º
        expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}

        def status(family):
            count = len(self.by_family.get(family, []))
            exp = expected.get(family, 5)
            ratio = count / exp
            return "üü¢" if ratio >= 0.8 else ("üü°" if ratio >= 0.4 else "üî¥")

        heatmap += "|                    | –ü—Ä–µ–¥–ø—Ä–∏–Ω–∏–º–∞—Ç–µ–ª—å | –ò–Ω–∂–µ–Ω–µ—Ä | –ú–µ–Ω–µ–¥–∂–µ—Ä |\n"
        heatmap += "|                    | (–°–º—ã—Å–ª—ã)        | (–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞) | (–û–ø–µ—Ä–∞—Ü–∏–∏) |\n"
        heatmap += "|--------------------|-----------------|---------|----------|\n"
        heatmap += f"| **–ú–∏—Ä (–ù–∞–¥—Å–∏—Å—Ç–µ–º–∞)** | {status('F1')} F1 ({len(self.by_family.get('F1', []))}) | {status('F2')} F2 ({len(self.by_family.get('F2', []))}) | {status('F3')} F3 ({len(self.by_family.get('F3', []))}) |\n"
        heatmap += f"| **–°–æ–∑–∏–¥–∞—Ç–µ–ª—å (–¶–µ–ª–µ–≤–∞—è)** | {status('F4')} F4 ({len(self.by_family.get('F4', []))}) | {status('F5')} F5 ({len(self.by_family.get('F5', []))}) | {status('F6')} F6 ({len(self.by_family.get('F6', []))}) |\n"
        heatmap += f"| **–≠–∫–æ—Å–∏—Å—Ç–µ–º–∞ (–°–æ–∑–¥–∞–Ω–∏—è)** | {status('F7')} F7 ({len(self.by_family.get('F7', []))}) | {status('F8')} F8 ({len(self.by_family.get('F8', []))}) | {status('F9')} F9 ({len(self.by_family.get('F9', []))}) |\n"

        return heatmap + "\n---\n\n"

    def _completeness_summary(self) -> str:
        """Executive Summary –¥–ª—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–π –ø–æ–ª–Ω–æ—Ç—ã."""
        total_docs = len(self.documents)
        expected_total = 71  # –°—É–º–º–∞ –æ–∂–∏–¥–∞–µ–º—ã—Ö
        completeness = min(100, int(total_docs / expected_total * 100))

        # –ù–∞–π—Ç–∏ —Å–∞–º—ã–µ –ø–æ–ª–Ω—ã–µ –∏ —Å–∞–º—ã–µ –ø—É—Å—Ç—ã–µ —Å–µ–º–µ–π—Å—Ç–≤–∞
        family_ratios = {}
        expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}
        for f, exp in expected.items():
            count = len(self.by_family.get(f, []))
            family_ratios[f] = count / exp

        best = max(family_ratios, key=family_ratios.get)
        worst = min(family_ratios, key=family_ratios.get)

        gaps = [f for f, ratio in family_ratios.items() if ratio < 0.4]

        summary = "## 1. Executive Summary\n\n"
        summary += f"- **–û–±—â–∞—è —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞:** {completeness}%\n"
        summary += f"- **–ù–∞–∏–±–æ–ª–µ–µ –ø–æ–ª–Ω–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ:** {best} ({FAMILIES[best]['name']}) ‚Äî {int(family_ratios[best]*100)}%\n"
        summary += f"- **–ù–∞–∏–º–µ–Ω–µ–µ –ø–æ–ª–Ω–æ–µ —Å–µ–º–µ–π—Å—Ç–≤–æ:** {worst} ({FAMILIES[worst]['name']}) ‚Äî {int(family_ratios[worst]*100)}%\n"
        summary += f"- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–µ–ª—ã:** {', '.join(gaps) if gaps else '–Ω–µ—Ç'}\n\n"

        return summary + "---\n\n"

    def _completeness_by_cells(self) -> str:
        """–ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º –º–∞—Ç—Ä–∏—Ü—ã 3x3."""
        cells = ""

        for family_id in ["F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "F9"]:
            family = FAMILIES[family_id]
            docs = self.by_family.get(family_id, [])

            cells += f"### 2.{family_id[1]}. {family['level']} √ó {family['role']} ({family_id}: {family['name']})\n\n"

            expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}
            ratio = len(docs) / expected.get(family_id, 5)
            status = "üü¢" if ratio >= 0.8 else ("üü°" if ratio >= 0.4 else "üî¥")

            cells += f"**–°—Ç–∞—Ç—É—Å:** {status} ({len(docs)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)\n\n"

            if docs:
                cells += "**–î–æ–∫—É–º–µ–Ω—Ç—ã:**\n"
                for doc in docs[:5]:
                    cells += f"- [[{doc.name}]]\n"
                if len(docs) > 5:
                    cells += f"- ... –∏ –µ—â—ë {len(docs) - 5}\n"
            else:
                cells += "*–î–æ–∫—É–º–µ–Ω—Ç—ã –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç*\n"

            cells += "\n"

        return "## 2. –ê–Ω–∞–ª–∏–∑ –ø–æ —è—á–µ–π–∫–∞–º –º–∞—Ç—Ä–∏—Ü—ã 3√ó3\n\n" + cells + "---\n\n"

    def _completeness_sota(self) -> str:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è SoTA-–º–µ—Ç–æ–¥–æ–≤."""
        sota = "## 3. –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ SoTA-–º–µ—Ç–æ–¥–æ–≤\n\n"

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–µ—Ç–æ–¥–æ–≤
        methods = {
            "JTBD": self._find_doc_by_pattern("JTBD|Jobs.to.be.done"),
            "Value Proposition": self._find_doc_by_pattern("–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ|Value Proposition"),
            "C4 Model": self._find_doc_by_pattern("C4|–ö–æ–Ω—Ç–µ–∫—Å—Ç.*–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä|–∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä"),
            "ADR": self._find_doc_by_pattern("ADR|Architecture Decision"),
            "ConOps": self._find_doc_by_pattern("ConOps|–ö–æ–Ω—Ü–µ–ø—Ü–∏—è.*–∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è|Concept.*Operations"),
            "OKR": self._find_doc_by_pattern("OKR|Objectives.*Key.*Results|–¶–µ–ª–∏.*–∑–∞–¥–∞—á–∏"),
        }

        sota += "| –ú–µ—Ç–æ–¥ | –°—Ç–∞—Ç—É—Å | –î–æ–∫—É–º–µ–Ω—Ç |\n"
        sota += "|-------|--------|----------|\n"

        for method, doc in methods.items():
            if doc:
                sota += f"| {method} | ‚úÖ –ü—Ä–∏–º–µ–Ω—ë–Ω | [[{doc.name}]] |\n"
            else:
                sota += f"| {method} | ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω | ‚Äî |\n"

        return sota + "\n---\n\n"

    def _completeness_stakeholders(self) -> str:
        """–û—Ç–≤–µ—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü."""
        stakeholders = "## 4. –û—Ç–≤–µ—Ç—ã –Ω–∞ –∏–Ω—Ç–µ—Ä–µ—Å—ã –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω—ã—Ö –ª–∏—Ü\n\n"

        questions = [
            ("–£—á–µ–Ω–∏–∫", "–ß—Ç–æ —è –ø–æ–ª—É—á—É?", "F4", "–¶–µ–Ω–Ω–æ—Å—Ç–Ω–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ"),
            ("–£—á–µ–Ω–∏–∫", "–ö–∞–∫ –Ω–∞—á–∞—Ç—å?", "F6", "–û–Ω–±–æ—Ä–¥–∏–Ω–≥"),
            ("–ù–∞—Å—Ç–∞–≤–Ω–∏–∫", "–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å —É—á–µ–Ω–∏–∫–∞–º–∏?", "F6", "–î–µ–∫–∞–Ω–∞—Ç|–Ω–∞—Å—Ç–∞–≤–Ω–∏–∫"),
            ("–ò–Ω–≤–µ—Å—Ç–æ—Ä", "–ö–∞–∫–æ–≤–∞ –±–∏–∑–Ω–µ—Å-–º–æ–¥–µ–ª—å?", "F7", "–≠–∫–æ–Ω–æ–º–∏—á–µ—Å–∫|–±–∏–∑–Ω–µ—Å.–º–æ–¥–µ–ª"),
            ("–†–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫", "–ö–∞–∫ —É—Å—Ç—Ä–æ–µ–Ω–∞ –ø–ª–∞—Ç—Ñ–æ—Ä–º–∞?", "F8", "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä|–ø–ª–∞—Ç—Ñ–æ—Ä–º"),
            ("–ü–∞—Ä—Ç–Ω—ë—Ä", "–ö–∞–∫ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å—Å—è?", "F2", "–ò–Ω—Ç–µ–≥—Ä–∞—Ü|–ø–∞—Ä—Ç–Ω—ë—Ä"),
        ]

        stakeholders += "| –°—Ç–µ–π–∫—Ö–æ–ª–¥–µ—Ä | –í–æ–ø—Ä–æ—Å | –°—Ç–∞—Ç—É—Å | –ì–¥–µ –æ—Ç–≤–µ—Ç |\n"
        stakeholders += "|-------------|--------|--------|----------|\n"

        for stakeholder, question, family, pattern in questions:
            doc = self._find_doc_by_pattern(pattern)
            if doc:
                stakeholders += f"| {stakeholder} | {question} | ‚úÖ | [[{doc.name}]] |\n"
            else:
                stakeholders += f"| {stakeholder} | {question} | ‚ùå | *–ù–µ –Ω–∞–π–¥–µ–Ω* |\n"

        return stakeholders + "\n---\n\n"

    def _completeness_gaps(self) -> str:
        """–ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã."""
        gaps = "## 5. –ü—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã\n\n"

        expected = {"F1": 8, "F2": 6, "F3": 6, "F4": 6, "F5": 8, "F6": 6, "F7": 6, "F8": 15, "F9": 10}

        critical = []
        important = []

        for f, exp in expected.items():
            count = len(self.by_family.get(f, []))
            ratio = count / exp
            if ratio < 0.4:
                critical.append((f, FAMILIES[f]['name'], int(ratio * 100)))
            elif ratio < 0.8:
                important.append((f, FAMILIES[f]['name'], int(ratio * 100)))

        gaps += "### 5.1. –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ üî¥\n\n"
        if critical:
            for f, name, pct in critical:
                gaps += f"- **{f} ({name})** ‚Äî {pct}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏\n"
        else:
            gaps += "*–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–µ—Ç*\n"
        gaps += "\n"

        gaps += "### 5.2. –í–∞–∂–Ω—ã–µ üü°\n\n"
        if important:
            for f, name, pct in important:
                gaps += f"- **{f} ({name})** ‚Äî {pct}% –∑–∞–ø–æ–ª–Ω–µ–Ω–Ω–æ—Å—Ç–∏\n"
        else:
            gaps += "*–í–∞–∂–Ω—ã—Ö –ø—Ä–æ–±–µ–ª–æ–≤ –Ω–µ—Ç*\n"

        return gaps + "\n---\n\n"

    def _completeness_links(self) -> str:
        return """## 6. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–ú–æ–¥–µ–ª—å —Å–µ–º–µ–π—Å—Ç–≤ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ 0.1]]
- [[–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.1]]
"""

    # ==================== –¢–ï–•–ù–ò–ß–ï–°–ö–ò–ï –ü–†–û–ë–õ–ï–ú–´ ====================

    def _generate_technical_issues(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ '–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞'."""
        report = self._header("–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞")

        # –°–æ–±–∏—Ä–∞–µ–º –ø—Ä–æ–±–ª–µ–º—ã
        dup_folders = self._find_duplicate_folders()
        dup_docs = self._find_duplicate_documents()
        broken_links = self._find_broken_links()
        missing_metadata = self._find_missing_metadata()

        # –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞
        report += self._technical_heatmap(dup_folders, dup_docs, broken_links, missing_metadata)

        # Executive Summary
        total = len(dup_folders) + len(dup_docs) + len(broken_links) + len(missing_metadata)
        critical = len(dup_folders) + len([d for d in dup_docs if d[2] == "exact"])

        report += "## 1. Executive Summary\n\n"
        report += f"- **–í—Å–µ–≥–æ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º:** {total}\n"
        report += f"- **–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏—Ö:** {critical}\n"
        report += f"- **–¢—Ä–µ–±—É—é—Ç –≤–Ω–∏–º–∞–Ω–∏—è:** {total - critical}\n\n"
        report += "---\n\n"

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫
        report += self._technical_dup_folders(dup_folders)

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        report += self._technical_dup_docs(dup_docs)

        # –ë–∏—Ç—ã–µ —Å—Å—ã–ª–∫–∏
        report += self._technical_broken_links(broken_links)

        # –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
        report += self._technical_missing_metadata(missing_metadata)

        # –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        report += """## 7. –°–≤—è–∑–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã

- [[–ö–æ–Ω—Ü–µ–ø—Ü–∏—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤ –ò–ò 0.4.1]]
- [[–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å 0.4]]
- [[–°—Ç—Ä—É–∫—Ç—É—Ä–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.1]]
"""

        return report

    def _technical_heatmap(self, dup_folders, dup_docs, broken_links, missing_metadata) -> str:
        heatmap = "## –¢–µ–ø–ª–æ–≤–∞—è –∫–∞—Ä—Ç–∞ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏—Ö –ø—Ä–æ–±–ª–µ–º\n\n"
        heatmap += "| –¢–∏–ø –ø—Ä–æ–±–ª–µ–º—ã | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –°—Ç–∞—Ç—É—Å |\n"
        heatmap += "|--------------|------------|--------|\n"

        def status(count, threshold_red=1, threshold_yellow=5):
            if count >= threshold_red:
                return "üî¥"
            elif count >= threshold_yellow:
                return "üü°"
            return "üü¢"

        heatmap += f"| –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–∞–ø–æ–∫ | {len(dup_folders)} | {status(len(dup_folders))} |\n"
        heatmap += f"| –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ | {len(dup_docs)} | {status(len(dup_docs), 3, 10)} |\n"
        heatmap += f"| –ë–∏—Ç—ã–µ wikilinks | {len(broken_links)} | {status(len(broken_links), 5, 15)} |\n"
        heatmap += f"| –ù–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö | {len(missing_metadata)} | {status(len(missing_metadata), 10, 30)} |\n"

        total = len(dup_folders) + len(dup_docs) + len(broken_links) + len(missing_metadata)
        heatmap += f"| **–ò—Ç–æ–≥–æ –ø—Ä–æ–±–ª–µ–º** | **{total}** | ‚Äî |\n"

        return heatmap + "\n---\n\n"

    def _find_duplicate_folders(self) -> List[Tuple[str, List[str], str]]:
        """–ü–æ–∏—Å–∫ –ø–∞–ø–æ–∫ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∏–ª–∏ –Ω–æ–º–µ—Ä–∞–º–∏.

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π: (–∫–ª—é—á_–¥—É–±–ª—è, [–ø—É—Ç–∏], —Ç–∏–ø_–¥—É–±–ª—è)
        –¢–∏–ø—ã: 'number' (–æ–¥–∏–Ω–∞–∫–æ–≤—ã–π –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞), 'name' (–æ–¥–∏–Ω–∞–∫–æ–≤–æ–µ –Ω–∞–∑–≤–∞–Ω–∏–µ)
        """
        # –°–∫–∞–Ω–∏—Ä—É–µ–º –í–°–ï –ø–∞–ø–∫–∏ —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ, –∞ –Ω–µ —Ç–æ–ª—å–∫–æ —Ä–æ–¥–∏—Ç–µ–ª–µ–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        all_folders = set()
        for folder in CONTENT_DIR.rglob("*"):
            if folder.is_dir():
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ –ø–∞–ø–∫–∏
                if any(skip in str(folder) for skip in [".obsidian", "node_modules", ".git"]):
                    continue
                if folder != CONTENT_DIR:
                    all_folders.add(folder)

        # –°–ª–æ–≤–∞—Ä–∏ –¥–ª—è –ø–æ–∏—Å–∫–∞ –¥—É–±–ª–µ–π
        folder_numbers = defaultdict(list)  # –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ -> –ø—É—Ç–∏
        folder_names = defaultdict(list)    # –Ω–∞–∑–≤–∞–Ω–∏–µ (–±–µ–∑ –Ω–æ–º–µ—Ä–∞) -> –ø—É—Ç–∏

        for folder in all_folders:
            rel_path = str(folder.relative_to(CONTENT_DIR))
            folder_name = folder.name

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, "0.4.1." –∏–∑ "0.4.1. –ù–∞–∑–≤–∞–Ω–∏–µ")
            number_match = re.match(r'^(\d+(?:\.\d+)*\.?)\s*', folder_name)
            if number_match:
                section_number = number_match.group(1).rstrip('.')  # "0.4.1"
                folder_numbers[section_number].append(rel_path)

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–∞–∑–≤–∞–Ω–∏–µ –±–µ–∑ –Ω–æ–º–µ—Ä–∞
            name = re.sub(r'^\d+(?:\.\d+)*\.?\s*', '', folder_name).lower().strip()
            if name:
                folder_names[name].append(rel_path)

        duplicates = []

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤ (–∫—Ä–∏—Ç–∏—á–Ω–æ!)
        for number, paths in folder_numbers.items():
            unique_paths = list(set(paths))
            if len(unique_paths) > 1:
                duplicates.append((f"–ù–æ–º–µ—Ä {number}", unique_paths, "number"))

        # –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π
        for name, paths in folder_names.items():
            unique_paths = list(set(paths))
            if len(unique_paths) > 1:
                duplicates.append((name, unique_paths, "name"))

        return duplicates

    def _find_duplicate_documents(self) -> List[Tuple[str, List[str], str]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏."""
        doc_names = defaultdict(list)

        for doc in self.documents:
            # –£–±–∏—Ä–∞–µ–º –Ω–æ–º–µ—Ä —Ä–∞–∑–¥–µ–ª–∞ –∏–∑ –Ω–∞–∑–≤–∞–Ω–∏—è
            name = re.sub(r'\s*\d+\.\d+\.?$', '', doc.name).lower().strip()
            doc_names[name].append(str(doc.relative_path))

        duplicates = []
        for name, paths in doc_names.items():
            if len(paths) > 1:
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø –¥—É–±–ª—è
                dup_type = "exact" if len(set(paths)) == len(paths) else "similar"
                duplicates.append((name, paths, dup_type))

        return duplicates

    def _find_broken_links(self) -> List[Tuple[str, str, str]]:
        """–ü–æ–∏—Å–∫ –±–∏—Ç—ã—Ö wikilinks."""
        # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∏–º–µ–Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        doc_names = {doc.name.lower(): doc.name for doc in self.documents}

        broken = []
        for doc in self.documents:
            for link in doc.wikilinks:
                link_lower = link.lower()
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ
                if link_lower not in doc_names:
                    # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ
                    similar = self._find_similar_name(link, doc_names.values())
                    broken.append((str(doc.relative_path), link, similar or "–Ω–µ –Ω–∞–π–¥–µ–Ω"))

        return broken[:50]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

    def _find_missing_metadata(self) -> List[Tuple[str, List[str]]]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö."""
        required_fields = ["type", "status"]

        missing = []
        for doc in self.documents:
            absent = [f for f in required_fields if f not in doc.frontmatter]
            if absent:
                missing.append((str(doc.relative_path), absent))

        return missing[:30]  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –≤—ã–≤–æ–¥

    def _technical_dup_folders(self, duplicates) -> str:
        section = "## 2. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞–ø–æ–∫\n\n"

        if not duplicates:
            return section + "*–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        # –†–∞–∑–¥–µ–ª—è–µ–º –ø–æ —Ç–∏–ø—É: —Å–Ω–∞—á–∞–ª–∞ –¥—É–±–ª–∏ –Ω–æ–º–µ—Ä–æ–≤ (–∫—Ä–∏—Ç–∏—á–Ω–µ–µ), –ø–æ—Ç–æ–º –Ω–∞–∑–≤–∞–Ω–∏–π
        number_dups = [(n, p, t) for n, p, t in duplicates if t == "number"]
        name_dups = [(n, p, t) for n, p, t in duplicates if t == "name"]

        idx = 1

        if number_dups:
            section += "### –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–æ–≤ —Ä–∞–∑–¥–µ–ª–æ–≤ üî¥\n\n"
            for name, paths, _ in number_dups[:10]:
                section += f"#### 2.{idx}. [DUP-F{idx:03d}] {name}\n\n"
                section += "**–ù–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏ —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º:**\n"
                for path in paths[:5]:
                    section += f"- `{path}`\n"
                section += "\n**–ü—Ä–æ–±–ª–µ–º–∞:** –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–º–µ—Ä–∞ —Ä–∞–∑–¥–µ–ª–∞ –Ω–∞—Ä—É—à–∞–µ—Ç –∏–µ—Ä–∞—Ä—Ö–∏—é —Ö—Ä–∞–Ω–∏–ª–∏—â–∞.\n"
                section += "**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å –æ–¥–Ω—É –∏–∑ –ø–∞–ø–æ–∫ —Å –Ω–æ–≤—ã–º –Ω–æ–º–µ—Ä–æ–º.\n\n"
                idx += 1

        if name_dups:
            section += "### –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –ø–∞–ø–æ–∫ üü°\n\n"
            for name, paths, _ in name_dups[:10]:
                section += f"#### 2.{idx}. [DUP-F{idx:03d}] –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ ¬´{name}¬ª\n\n"
                section += "**–ù–∞–π–¥–µ–Ω—ã –ø–∞–ø–∫–∏:**\n"
                for path in paths[:5]:
                    section += f"- `{path}`\n"
                section += "\n**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è:** –û–±—ä–µ–¥–∏–Ω–∏—Ç—å –∏–ª–∏ –ø–µ—Ä–µ–∏–º–µ–Ω–æ–≤–∞—Ç—å.\n\n"
                idx += 1

        return section + "---\n\n"

    def _technical_dup_docs(self, duplicates) -> str:
        section = "## 3. –î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞–∑–≤–∞–Ω–∏–π –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤\n\n"

        if not duplicates:
            return section + "*–î—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –ù–∞–∑–≤–∞–Ω–∏–µ | –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ | –ü—É—Ç–∏ |\n"
        section += "|---|----------|------------|------|\n"

        for i, (name, paths, dup_type) in enumerate(duplicates[:15], 1):
            paths_str = "; ".join(paths[:3])
            if len(paths) > 3:
                paths_str += f" –∏ –µ—â—ë {len(paths) - 3}"
            section += f"| {i} | {name[:40]} | {len(paths)} | {paths_str[:60]}... |\n"

        return section + "\n---\n\n"

    def _technical_broken_links(self, broken) -> str:
        section = "## 4. –ë–∏—Ç—ã–µ wikilinks\n\n"

        if not broken:
            return section + "*–ë–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫ –Ω–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –°—Å—ã–ª–∫–∞ | –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è |\n"
        section += "|---|----------|--------|-------------|\n"

        for i, (doc_path, link, suggestion) in enumerate(broken[:20], 1):
            doc_short = doc_path.split("/")[-1][:30]
            section += f"| {i} | {doc_short} | `[[{link[:30]}]]` | {suggestion[:30]} |\n"

        if len(broken) > 20:
            section += f"\n*... –∏ –µ—â—ë {len(broken) - 20} –±–∏—Ç—ã—Ö —Å—Å—ã–ª–æ–∫*\n"

        return section + "\n---\n\n"

    def _technical_missing_metadata(self, missing) -> str:
        section = "## 5. –î–æ–∫—É–º–µ–Ω—Ç—ã –±–µ–∑ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö\n\n"

        if not missing:
            return section + "*–í—Å–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏–º–µ—é—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ* üü¢\n\n---\n\n"

        section += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–æ–ª—è |\n"
        section += "|---|----------|------------------|\n"

        for i, (doc_path, fields) in enumerate(missing[:20], 1):
            doc_short = doc_path.split("/")[-1][:40]
            section += f"| {i} | {doc_short} | {', '.join(fields)} |\n"

        if len(missing) > 20:
            section += f"\n*... –∏ –µ—â—ë {len(missing) - 20} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤*\n"

        return section + "\n---\n\n"

    # ==================== –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –û–¢–ß–Å–¢–´ ====================

    def _generate_terminology(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ –ø–æ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏."""
        report = self._header("–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å")

        if not self.ai_analyzer:
            report += "*–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç —Ç—Ä–µ–±—É–µ—Ç AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π —Ç–µ—Ä–º–∏–Ω–æ–≤.*\n\n"
            report += "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å —Ñ–ª–∞–≥–æ–º `--ai-analysis` –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n"
            report += "```bash\n"
            report += "python3 ops/build_report.py --report terminology --ai-analysis\n"
            report += "```\n\n"
            report += "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n"
            report += "- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: `pip install anthropic`\n"
            report += "- –ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `ANTHROPIC_API_KEY`\n"
            return report

        print("   ü§ñ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è AI-–∞–Ω–∞–ª–∏–∑ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏...")
        ai_analysis = self.ai_analyzer.analyze_terminology(self.documents)
        report += ai_analysis

        return report

    def _generate_recommendations(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞ —Å —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é."""
        report = self._header("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é")

        if not self.ai_analyzer:
            report += "*–≠—Ç–æ—Ç –æ—Ç—á—ë—Ç —Ç—Ä–µ–±—É–µ—Ç AI-–∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.*\n\n"
            report += "–ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å —Ñ–ª–∞–≥–æ–º `--ai-analysis` –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞:\n"
            report += "```bash\n"
            report += "python3 ops/build_report.py --report recommendations --ai-analysis\n"
            report += "```\n\n"
            report += "**–¢—Ä–µ–±–æ–≤–∞–Ω–∏—è:**\n"
            report += "- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ: `pip install anthropic`\n"
            report += "- –ó–∞–¥–∞–π—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è `ANTHROPIC_API_KEY`\n"
            return report

        print("   ü§ñ –í—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è AI-–∞–Ω–∞–ª–∏–∑ –¥–ª—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π...")
        ai_analysis = self.ai_analyzer.analyze_recommendations(self.documents, self.by_family)
        report += ai_analysis

        return report

    def _generate_links_map(self) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∫–∞—Ä—Ç—ã —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏."""
        report = self._header("–ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–≤—è–∑–µ–π
        total_links = sum(len(d.wikilinks) for d in self.documents)
        docs_with_links = sum(1 for d in self.documents if d.wikilinks)

        report += "## 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞\n\n"
        report += f"- –í—Å–µ–≥–æ —Å—Å—ã–ª–æ–∫: {total_links}\n"
        report += f"- –î–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å–æ —Å—Å—ã–ª–∫–∞–º–∏: {docs_with_links}\n"
        report += f"- –°—Ä–µ–¥–Ω–µ–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç: {total_links / len(self.documents):.1f}\n\n"

        # –¢–æ–ø –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å—Å—ã–ª–∫–∞–º
        incoming = defaultdict(int)
        for doc in self.documents:
            for link in doc.wikilinks:
                incoming[link.lower()] += 1

        report += "## 2. –¢–æ–ø-10 –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –ø–æ –≤—Ö–æ–¥—è—â–∏–º —Å—Å—ã–ª–∫–∞–º\n\n"
        report += "| ‚Ññ | –î–æ–∫—É–º–µ–Ω—Ç | –í—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫ |\n"
        report += "|---|----------|----------------|\n"

        for i, (name, count) in enumerate(sorted(incoming.items(), key=lambda x: -x[1])[:10], 1):
            report += f"| {i} | {name[:50]} | {count} |\n"

        report += "\n## 3. –ò–∑–æ–ª–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–±–µ–∑ —Å—Å—ã–ª–æ–∫)\n\n"
        isolated = [d for d in self.documents if not d.wikilinks]
        report += f"–ù–∞–π–¥–µ–Ω–æ {len(isolated)} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –±–µ–∑ –∏—Å—Ö–æ–¥—è—â–∏—Ö —Å—Å—ã–ª–æ–∫.\n\n"

        return report

    # ==================== –£–¢–ò–õ–ò–¢–´ ====================

    def _find_doc_by_pattern(self, pattern: str) -> Optional[Document]:
        """–ü–æ–∏—Å–∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞ –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É –≤ –Ω–∞–∑–≤–∞–Ω–∏–∏."""
        regex = re.compile(pattern, re.IGNORECASE)
        for doc in self.documents:
            if regex.search(doc.name):
                return doc
        return None

    def _extract_summary(self, doc: Document, max_sentences: int = 3) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫—Ä–∞—Ç–∫–æ–≥–æ —Ä–µ–∑—é–º–µ –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        # –£–±–∏—Ä–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏ —Å–ª—É–∂–µ–±–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
        text = re.sub(r'^#.*$', '', doc.body, flags=re.MULTILINE)
        text = re.sub(r'\|.*\|', '', text)  # –£–±–∏—Ä–∞–µ–º —Ç–∞–±–ª–∏—Ü—ã
        text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)  # –£–±–∏—Ä–∞–µ–º –∫–æ–¥
        text = re.sub(r'\[([^\]]+)\]\([^)]+\)', r'\1', text)  # –£–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏
        text = re.sub(r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]', r'\1', text)  # –£–±–∏—Ä–∞–µ–º wikilinks

        # –ù–∞—Ö–æ–¥–∏–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        sentences = [s.strip() for s in sentences if len(s.strip()) > 20]

        return " ".join(sentences[:max_sentences])

    def _extract_first_sentence(self, doc: Document) -> str:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–∞."""
        summary = self._extract_summary(doc, max_sentences=1)
        return summary[:100] if summary else ""

    def _find_similar_name(self, name: str, candidates: List[str], threshold: float = 0.8) -> Optional[str]:
        """–ü–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏—è (–ø—Ä–æ—Å—Ç–æ–π –∞–ª–≥–æ—Ä–∏—Ç–º)."""
        name_lower = name.lower()
        for candidate in candidates:
            candidate_lower = candidate.lower()
            # –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤—Ö–æ–∂–¥–µ–Ω–∏–µ
            if name_lower in candidate_lower or candidate_lower in name_lower:
                return candidate
        return None


def save_report(content: str, filename: str):
    """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—Ç—á—ë—Ç–∞ –≤ —Ñ–∞–π–ª."""
    output_path = REPORTS_DIR / filename
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(content, encoding="utf-8")
    print(f"‚úÖ –û—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {output_path}")


def main():
    parser = argparse.ArgumentParser(description="–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤")
    parser.add_argument(
        "--report", "-r",
        required=True,
        choices=["architecture-snapshot", "content-completeness", "technical-issues",
                 "terminology", "recommendations", "links-map", "all"],
        help="–¢–∏–ø –æ—Ç—á—ë—Ç–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏"
    )
    parser.add_argument(
        "--output", "-o",
        help="–ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á—ë—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é: –≤ –ø–∞–ø–∫—É –æ—Ç—á—ë—Ç–æ–≤)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="–¢–æ–ª—å–∫–æ –ø–æ–∫–∞–∑–∞—Ç—å –æ—Ç—á—ë—Ç, –Ω–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å"
    )
    parser.add_argument(
        "--ai-analysis",
        action="store_true",
        help="–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å AI (Claude) –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏–∏ –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π"
    )

    args = parser.parse_args()

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∑–∞–ø—É—â–µ–Ω–æ –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞
    if not CONTENT_DIR.exists():
        print(f"‚ùå –ü–∞–ø–∫–∞ {CONTENT_DIR} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ó–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∫—Ä–∏–ø—Ç –∏–∑ –∫–æ—Ä–Ω—è –ø—Ä–æ–µ–∫—Ç–∞.")
        sys.exit(1)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º AI-–∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏
    ai_analyzer = None
    if args.ai_analysis:
        try:
            ai_analyzer = AIAnalyzer()
            print("‚úÖ AI-–∞–Ω–∞–ª–∏–∑ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω")
        except RuntimeError as e:
            print(f"‚ö†Ô∏è  {e}")
            print("   –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –±–µ–∑ AI-–∞–Ω–∞–ª–∏–∑–∞...")

    generator = ReportGenerator(ai_analyzer=ai_analyzer)
    generator.scan_documents()

    report_files = {
        "architecture-snapshot": "–ê—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–Ω—ã–π —Å–ª–µ–ø–æ–∫ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ 0.4.md",
        "content-completeness": "–°–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–∞—è –ø–æ–ª–Ω–æ—Ç–∞ –æ–ø–∏—Å–∞–Ω–∏—è 0.4.md",
        "technical-issues": "–ü—Ä–æ—Ç–∏–≤–æ—Ä–µ—á–∏—è –∏ –Ω–µ—Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ 0.4.md",
        "terminology": "–¢–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∞—è —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç—å 0.4.md",
        "recommendations": "–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —Ä–∞–∑–≤–∏—Ç–∏—é 0.4.md",
        "links-map": "–ö–∞—Ä—Ç–∞ —Å–≤—è–∑–µ–π –º–µ–∂–¥—É –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ 0.4.md",
    }

    if args.report == "all":
        reports_to_generate = list(report_files.keys())
    else:
        reports_to_generate = [args.report]

    for report_type in reports_to_generate:
        print(f"\nüìù –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç—á—ë—Ç–∞: {report_type}")

        try:
            content = generator.generate(report_type)

            if args.dry_run:
                print("\n" + "=" * 60)
                print(content[:2000])
                print("..." if len(content) > 2000 else "")
                print("=" * 60)
            else:
                filename = args.output if args.output and args.report != "all" else report_files[report_type]
                save_report(content, filename)

        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ {report_type}: {e}")
            import traceback
            traceback.print_exc()

    print("\n‚úÖ –ì–æ—Ç–æ–≤–æ!")


if __name__ == "__main__":
    main()
