# Единое хранилище знаний (Memory Bank): Enterprise-ready архитектура

Этот документ предлагает варианты централизованного memory bank для **промышленной эксплуатации** экосистемы развития интеллекта.

> **Статус:** Draft v2 (Enterprise edition)
> **Дата:** 2025-11-12
> **Автор:** Архитектор экосистемы
> **Масштаб:** Команда 10 человек, миллион ИИ-агентов + участников

---

## TL;DR (200–300 слов)

Экосистеме нужен **промышленный memory bank**, способный обслуживать **миллион конкурентных запросов** от ИИ-агентов и участников при разработке **командой из 10 человек**. Это требует принципиально иной архитектуры: распределённое хранилище, multi-region deployment, CDN, кэширование, observability, auto-scaling.

**Четыре production-ready варианта:**

1. **Cloud-native / Serverless** (Supabase/Firebase + Vercel/Cloudflare) — максимальная скорость разработки, автоматический scaling, pay-per-use. Managed services берут на себя инфраструктуру, команда фокусируется на логике. Стоимость $3k-15k/мес при миллионе агентов.

2. **Hybrid Platform / Kubernetes** (K8s + PostgreSQL + MinIO + Redis) — полный контроль, on-premise или облако, гибкость масштабирования. Требует Platform Engineer в команде. Стоимость $5k-20k/мес + зарплата DevOps.

3. **Modern Data Platform** (Supabase + ClickHouse + Qdrant + Kafka) — оптимизирован для аналитики и ML. OLTP (Postgres) + OLAP (ClickHouse) + векторный поиск (Qdrant) + event streaming. Команда Data/ML Engineers. Стоимость $10k-30k/мес.

4. **Hybrid Cloud + Edge** (Multi-region + Edge Functions + CDN) — минимальные латентности, глобальная доступность. Cloudflare Workers + R2, Fly.io для динамики, Vercel Edge для UI. Стоимость $5k-25k/мес.

**Ключевые отличия от MVP-версии:**
- ✅ Горизонтальное масштабирование (sharding, read replicas)
- ✅ Multi-region deployment (latency <100ms globally)
- ✅ Observability stack (метрики, логи, трейсы, алерты)
- ✅ CI/CD с автотестами и canary deployments
- ✅ Rate limiting и quota management для агентов
- ✅ Cost optimization (cold storage, compression, TTL)
- ✅ Team workflows (mono/multi-repo, IaC, on-call)

**Новые варианты инфраструктуры:**
- **Proxmox + Ceph** для on-premise (полный контроль, капекс)
- **Supabase** для быстрого старта (Postgres + Auth + Storage + Realtime)
- **ClickHouse** для аналитики (OLAP, сжатие 10-100x)
- **Qdrant/Milvus** для векторного поиска at scale
- **MinIO** для S3-compatible object storage on-premise

---

## Архитектурные принципы для масштаба

### Масштаб требований

**Трафик:**
- 1 млн агентов × 10 запросов/час = **10M requests/hour** (~2800 RPS)
- Пиковая нагрузка (10x) = **28k RPS**
- 80% reads, 20% writes

**Данные:**
- Знания (curated): ~10GB (медленно растут)
- Профили двойников: 1M × 50KB = **50GB**
- События (append-only): 100M events/month × 1KB = **100GB/месяц** = 1.2TB/год
- Артефакты (S3): ~10TB/год
- Итого: **~15TB** через 3 года

**Команда (10 человек):**
- 1 Tech Lead / Architect
- 2 Backend Engineers (API, бизнес-логика)
- 1-2 Platform/DevOps Engineers (infrastructure, CI/CD)
- 1-2 Data Engineers (ETL, warehouses, ML pipelines)
- 1 ML Engineer (векторный поиск, рекомендации)
- 2 Frontend Engineers (Web/Mobile)
- 1 QA/SRE (тестирование, monitoring, on-call)

### Принципы проектирования

**P1. Separation of Concerns**
- OLTP (транзакции) ≠ OLAP (аналитика) ≠ Search (поиск)
- Разные workloads → разные хранилища

**P2. Read-Heavy Optimization**
- CDN для статики (guides, ontology)
- Multi-tier caching (browser → CDN → Redis → DB)
- Read replicas и connection pooling

**P3. Event-Driven Architecture**
- Async processing через event bus (Kafka/NATS)
- Декаплинг сервисов через events
- Audit log = event sourcing

**P4. Observability-First**
- Метрики (RED/USE method): Rate, Errors, Duration / Utilization, Saturation, Errors
- Distributed tracing (OpenTelemetry)
- Centralized logging (Loki/ELK)
- Alerts и on-call runbooks

**P5. Cost-Aware Design**
- Hot/Warm/Cold storage tiers
- Compression (zstd для логов, Parquet для аналитики)
- Spot instances для batch jobs
- Reserved instances для базовых сервисов

**P6. Zero-Downtime Deployments**
- Blue/Green или Canary deployments
- Database migrations с backward compatibility
- Feature flags

---

## Вариант 1: Cloud-native / Serverless

### Идея и целевая логика

Максимально используем **managed services** чтобы команда из 10 человек могла сфокусироваться на продукте, а не на инфраструктуре. Supabase (Postgres + Auth + Storage + Realtime) как основа, Vercel/Cloudflare для edge functions и CDN, managed Kafka (Confluent/AWS MSK), managed vector DB (Pinecone). Auto-scaling, pay-per-use, ~zero ops.

**Single source of truth:** комбинация Supabase (структурированные данные + auth), S3/R2 (объекты), Pinecone (векторы), Kafka (события). Все managed, все масштабируются автоматически.

### Структура хранилища

```
Cloud-Native Architecture:

┌─────────────────────────────────────────────────────────────┐
│                    EDGE LAYER (CDN)                          │
│  Cloudflare CDN / Vercel Edge Network                       │
│  • Static assets (guides, images)                           │
│  • Edge Functions (auth check, rate limit)                  │
│  • DDoS protection, WAF                                     │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │ HTTPS/HTTP2
                            │
┌─────────────────────────────────────────────────────────────┐
│                   API GATEWAY LAYER                          │
│  ┌────────────────┐  ┌────────────────┐                    │
│  │ GraphQL API    │  │  REST API      │                    │
│  │ (Hasura/       │  │  (API Gateway) │                    │
│  │  Supabase)     │  │                │                    │
│  │                │  │                │                    │
│  │ • Query opti-  │  │ • Rate limit   │                    │
│  │   mization     │  │ • Auth (JWT)   │                    │
│  │ • Permissions  │  │ • Quota mgmt   │                    │
│  └────────────────┘  └────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │
┌─────────────────────────────────────────────────────────────┐
│                   SERVICE LAYER                              │
│  ┌─────────────┐  ┌─────────────┐  ┌──────────────┐        │
│  │  Vercel     │  │ Cloudflare  │  │  AWS Lambda  │        │
│  │  Functions  │  │   Workers   │  │  / GCF       │        │
│  │             │  │             │  │              │        │
│  │ • AI Agent  │  │ • Auth edge │  │ • ETL jobs   │        │
│  │   Proxy     │  │ • Transform │  │ • ML infer   │        │
│  │ • Business  │  │ • Routing   │  │ • Batch proc │        │
│  │   Logic     │  │             │  │              │        │
│  └─────────────┘  └─────────────┘  └──────────────┘        │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │
┌─────────────────────────────────────────────────────────────┐
│                    DATA LAYER                                │
│                                                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Supabase (Managed Postgres + Auth + Storage + RT)    │ │
│  │                                                         │ │
│  │  • digital_twins (profiles, 50+ params)               │ │
│  │  • trajectories (current path, progress)              │ │
│  │  • artifacts (metadata, content in S3)                │ │
│  │  • transactions (token/fiat ledger)                   │ │
│  │  • wallets (balances)                                 │ │
│  │  • epistemic_status (reputation)                      │ │
│  │                                                         │ │
│  │  Read Replicas: 3x (multi-region)                     │ │
│  │  Connection Pooler: PgBouncer managed                 │ │
│  │  Backups: PITR (Point-in-Time Recovery)               │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌────────────────┐  ┌────────────────┐  ┌──────────────┐  │
│  │  S3 / R2       │  │  Pinecone      │  │  Kafka (MSK) │  │
│  │  (Object       │  │  (Vector DB)   │  │  (Events)    │  │
│  │   Storage)     │  │                │  │              │  │
│  │                │  │  • 1M vectors  │  │  Topics:     │  │
│  │ • Guides       │  │  • 1536 dims   │  │  - learning  │  │
│  │ • Artifacts    │  │  • Sub-100ms   │  │  - economic  │  │
│  │ • Attachments  │  │  • Metadata    │  │  - agent     │  │
│  │                │  │    filtering   │  │  - system    │  │
│  │ Lifecycle:     │  │                │  │              │  │
│  │ • Hot 30d      │  │  Replicas: 3x  │  │  Retention:  │  │
│  │ • Archive 1yr  │  │  regions       │  │  30 days     │  │
│  └────────────────┘  └────────────────┘  └──────────────┘  │
│                                                              │
│  ┌────────────────┐  ┌────────────────┐                    │
│  │  Redis Cloud   │  │  ClickHouse    │                    │
│  │  (Cache)       │  │  Cloud         │                    │
│  │                │  │  (Analytics)   │                    │
│  │ • Sessions     │  │                │                    │
│  │ • Rate limits  │  │ • Aggregations │                    │
│  │ • Twin cache   │  │ • Dashboards   │                    │
│  │ • Leaderboards │  │ • Exports      │                    │
│  │                │  │                │                    │
│  │ HA Cluster     │  │  Compression   │                    │
│  │ 3 nodes        │  │  10-100x       │                    │
│  └────────────────┘  └────────────────┘                    │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │
┌─────────────────────────────────────────────────────────────┐
│                   MONITORING & OPS                           │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Observability Stack                                   │ │
│  │                                                         │ │
│  │  • Datadog / New Relic (APM, infra monitoring)        │ │
│  │  • Sentry (error tracking)                            │ │
│  │  • LogDNA / Loki (centralized logging)                │ │
│  │  • PagerDuty (on-call alerts)                         │ │
│  │  • Grafana Cloud (custom dashboards)                  │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

**Дополнительные компоненты:**

```
Knowledge Management (Curated):
  Git Repo (GitHub)
    ↓ (CI/CD: GitHub Actions)
  S3/R2 (versioned Markdown)
    ↓ (CDN)
  Cloudflare Pages (static site)

Event Processing:
  Kafka Topics
    ↓ (Stream Processors: Kafka Streams / Flink)
  PostgreSQL (aggregated state)
  ClickHouse (analytics)

Vector Indexing:
  S3 (documents)
    ↓ (Lambda trigger)
  OpenAI Embeddings API
    ↓
  Pinecone (indexed)
```

### Потоки работы (7 этапов для миллиона агентов)

**1. Ingestion (Пополнение) — 28k RPS пик**

- **Curated content**: Команда → Git → GitHub Actions → S3 → CDN invalidation
- **User-generated**: API Gateway (rate limit по agent_id) → Lambda → Kafka → async processors
- **External sources**: Scheduled ETL (Airflow managed / AWS Step Functions) → S3 staging → validation → ingest

**Ключевые техники:**
- Rate limiting: 100 req/min per agent_id (Redis sliding window)
- Batch writes: группировка 100 events перед записью в Kafka
- Backpressure: reject новые запросы при queue > threshold

**2. Нормализация и валидация — Async**

- **Stream processing**: Kafka → Flink job → validate JSON Schema → enrich (lookup twin profile) → deduplicate (Redis)
- **Data quality**: Great Expectations checks на выходе из Flink
- **Dead letter queue**: невалидные events → DLQ topic → алерт в Slack → ручная обработка

**Масштабирование:**
- Flink cluster: 10 task managers, auto-scale на CPU >70%
- Kafka partitions: 32 per topic (parallelism)

**3. Версионирование и аудит**

- **Postgres**: Row-level audit через triggers (created_at, updated_at, updated_by)
- **S3**: Object versioning enabled, lifecycle policy (удаление старых версий через 90 дней)
- **Event sourcing**: Kafka topics = append-only log (retention 30 дней, архив в S3)
- **ADR**: Git repo рядом с кодом

**4. Индексация и поиск — Multi-tier**

**Вектор (semantic search):**
- S3 new object → Lambda trigger → batch (100 docs) → OpenAI Embeddings API → Pinecone upsert
- Pinecone: 1M vectors, p1 pods (sub-100ms), 3 replicas
- Query: API → Redis cache (TTL 1h) → Pinecone → Postgres metadata join → return

**Keyword (exact match):**
- Postgres full-text search (GIN index on tsvector)
- Fallback если semantic не нашёл

**Graph navigation:**
- Postgres recursive CTEs для онтологии (небольшой граф, <10k nodes)
- Или Neo4j Aura если граф растёт

**5. Публикация и контракты**

- **API Gateway**: Kong/Tyk (self-hosted) или AWS API Gateway с rate limits/quotas
- **GraphQL**: Hasura над Postgres (permission rules по роли)
- **REST**: FastAPI/Node.js functions на Vercel
- **WebSocket**: Supabase Realtime для live updates (новые артефакты, уведомления)

**Rate limits по tier:**
- Free tier: 10 req/min
- Pro tier: 100 req/min
- Enterprise: 1000 req/min
- Internal agents: 10k req/min

**6. Контроль качества — Continuous**

- **CI/CD**: GitHub Actions → unit tests → integration tests (Testcontainers) → deploy staging → smoke tests → canary 10% → full rollout
- **Data quality**: dbt tests на витринах данных (uniqueness, not_null, relationships)
- **Monitoring**:
  - RED metrics: Rate (RPS), Errors (5xx rate), Duration (p95 latency)
  - SLO: p95 latency <200ms, availability 99.9%, error rate <0.1%
- **Alerts**: PagerDuty при нарушении SLO, on-call rotation (недельная)

**7. Бэкапы и disaster recovery**

- **Postgres**:
  - PITR (Point-in-Time Recovery) с 7-дневным окном
  - Ежедневные снепшоты → S3 (другой region)
  - RTO (Recovery Time Objective): <1 час
  - RPO (Recovery Point Objective): <5 минут
- **S3/R2**: Cross-region replication (CRR)
- **Kafka**: Multi-region setup или архив в S3 через Kafka Connect
- **Restore testing**: ежемесячная процедура восстановления из бэкапа в staging

### Где живут и как версионируются

| Сущность | Основное хранилище | Кэш | Версионирование | Бэкап |
|----------|-------------------|-----|-----------------|-------|
| **Онтология/глоссарий** | S3 (Markdown) + CDN | Cloudflare Cache (7 дней) | Git tags + S3 versions | S3 CRR |
| **Профили двойников** | Postgres (JSON column) | Redis (TTL 1ч) | updated_at + audit table | PITR + снепшоты |
| **Руководства/ДЗ** | S3 + Postgres (metadata) | CDN | Git + S3 versions | S3 CRR |
| **События/журналы** | Kafka → ClickHouse | Redis (aggregates) | Kafka offset + ClickHouse parts | S3 archive |
| **Транзакции** | Postgres (append-only) | Redis (balance cache) | Immutable rows | PITR + снепшоты |
| **Артефакты агентов** | S3 (файлы) + Postgres (meta) | CDN | S3 versions | S3 CRR |
| **Векторы (embeddings)** | Pinecone | - | Version field в metadata | Pinecone backups |

### Интеграции и Apps SDK Actions

**10 базовых actions для миллиона агентов:**

1. **`query_knowledge(query, filters, limit)`**
   - Semantic search → Pinecone → Redis cache
   - Fallback: Postgres full-text
   - Rate limit: 100/min per agent

2. **`get_twin_profile(twin_id, include_fields)`**
   - Redis GET → cache hit (99%)
   - Cache miss → Postgres → Redis SET (TTL 1h)
   - Поддержка field selection (только нужные поля)

3. **`log_event(event_type, twin_id, payload)`**
   - Kafka Producer async
   - Batch size 100 events
   - Return 202 Accepted immediately

4. **`create_artifact(twin_id, content, metadata)`**
   - Upload → S3 pre-signed URL (client → S3 direct)
   - Webhook callback → API → Postgres insert
   - Async: trigger vector indexing

5. **`update_trajectory(twin_id, action, evidence)`**
   - Optimistic locking (version field)
   - Postgres UPDATE + invalidate Redis cache
   - Emit event → Kafka

6. **`transaction(from, to, amount, currency, reason)`**
   - Postgres transaction (ACID)
   - Balance check + transfer
   - Emit event → Kafka → ClickHouse analytics

7. **`search_agents(query, filters, sort)`**
   - Postgres with indexes (agent_type, created_at)
   - Pagination (cursor-based)
   - Return metadata + S3 signed URLs

8. **`get_recommendations(twin_id, type, limit)`**
   - ML model inference (API call или edge function)
   - Personalized: Vector similarity (Pinecone)
   - Collaborative: ClickHouse aggregations

9. **`subscribe_realtime(channels)`**
   - WebSocket via Supabase Realtime
   - Channels: twin:{id}, community, global
   - Auto-reconnect on disconnect

10. **`batch_query(queries[])`**
    - GraphQL batch request
    - DataLoader pattern (N+1 избежать)
    - Max 50 queries per batch

**Rate limiting strategy:**
```typescript
// Redis Lua script для sliding window
const rateLimitKey = `rate:${agentId}:${window}`;
const count = await redis.incr(rateLimitKey);
if (count === 1) await redis.expire(rateLimitKey, windowSeconds);

if (count > limit) {
  throw new RateLimitError({
    retryAfter: windowSeconds,
    quota: { limit, remaining: 0, reset: Date.now() + windowSeconds * 1000 }
  });
}
```

### Trade-offs

**Плюсы:**
- ✅ **Zero infrastructure management**: команда фокусируется на продукте
- ✅ **Auto-scaling**: handled by providers, no capacity planning
- ✅ **Global distribution**: multi-region из коробки (Cloudflare/Vercel)
- ✅ **Fast time-to-market**: 2-4 недели до production
- ✅ **Built-in auth/permissions**: Supabase RLS, Hasura permissions

**Минусы:**
- ❌ **Vendor lock-in**: миграция с Supabase/Pinecone сложна
- ❌ **Cost unpredictability**: pay-per-use может "выстрелить" при аномалиях
- ❌ **Less control**: нельзя оптимизировать на уровне query planner/kernel
- ❌ **Cold starts**: serverless functions (50-500ms latency spike)
- ❌ **API limits**: managed services имеют hard limits (например, Pinecone 10k req/min на tier)

### Когда выбирать

- ✅ Приоритет: скорость разработки > контроль
- ✅ Команда <15 человек, нет выделенного DevOps
- ✅ Бюджет позволяет $10k-20k/мес на инфраструктуру
- ✅ Workload predictable или готовы к cost spikes
- ✅ Не критичны 50-100ms дополнительной latency от serverless

**Ориентировочная стоимость (1M агентов, 2800 RPS средняя):**
- Supabase Pro: $25 + compute ~$2k = **$2025/мес**
- Pinecone p1.x1: $70 + storage $200 = **$270/мес**
- Cloudflare R2: 1TB + 10M ops = **$15/мес**
- Redis Cloud: 5GB HA = **$200/мес**
- Kafka (Confluent): 500GB ingress = **$1500/мес**
- ClickHouse Cloud: 100GB + queries = **$500/мес**
- Vercel Pro: functions + bandwidth = **$500/мес**
- Observability (Datadog): 10 hosts = **$1500/мес**
- **Итого: ~$6500/мес базовый + burst capacity**

При пиках (10x traffic) может доходить до **$15k-20k/мес**.

---

## Вариант 2: Hybrid Platform / Kubernetes (On-Premise + Cloud)

### Идея и целевая логика

Полный контроль над инфраструктурой: **Kubernetes кластер** (on-premise на Proxmox + Ceph или в облаке GKE/EKS) с self-hosted компонентами. PostgreSQL + Redis + MinIO + Kafka + Qdrant (векторная БД). Позволяет оптимизировать на уровне hardware, снизить costs при высоких объёмах, compliance (данные в своём ДЦ).

**Single source of truth:** Kubernetes-операторы управляют БД/хранилищами, GitOps (ArgoCD/Flux) для деплоя, Infrastructure as Code (Terraform/Pulumi).

### Структура хранилища

```
Kubernetes Architecture (on Proxmox or Cloud):

┌─────────────────────────────────────────────────────────────┐
│                   INGRESS LAYER                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Cloudflare (DDoS, WAF)                                │ │
│  │         ↓                                               │ │
│  │  Nginx Ingress Controller / Traefik                    │ │
│  │         ↓                                               │ │
│  │  cert-manager (Let's Encrypt TLS)                      │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
                            ▲
                            │
┌─────────────────────────────────────────────────────────────┐
│            KUBERNETES CLUSTER (3 regions)                    │
│                                                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  APPLICATION PODS                                       │ │
│  │                                                         │ │
│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │ │
│  │  │  API Gateway │  │  GraphQL     │  │  Agent      │ │ │
│  │  │  (Kong)      │  │  (Hasura)    │  │  Orchestr.  │ │ │
│  │  │              │  │              │  │  (Temporal) │ │ │
│  │  │  HPA:        │  │  HPA:        │  │             │ │ │
│  │  │  min 5       │  │  min 3       │  │  HPA:       │ │ │
│  │  │  max 50      │  │  max 30      │  │  min 2      │ │ │
│  │  │  CPU 70%     │  │  CPU 80%     │  │  max 20     │ │ │
│  │  └──────────────┘  └──────────────┘  └─────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │ │
│  │  │  ML Inference│  │  ETL Workers │  │  WebSocket  │ │ │
│  │  │  (FastAPI)   │  │  (Celery)    │  │  (Socket.io)│ │ │
│  │  │              │  │              │  │             │ │ │
│  │  │  GPU nodes   │  │  Spot inst.  │  │  Sticky     │ │ │
│  │  │  (NVIDIA)    │  │  (cheap)     │  │  sessions   │ │ │
│  │  └──────────────┘  └──────────────┘  └─────────────┘ │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  STATEFUL SERVICES (Operators)                         │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  PostgreSQL (CloudNativePG operator)             │ │ │
│  │  │                                                   │ │ │
│  │  │  Primary: 1 (writes)                             │ │ │
│  │  │  Replicas: 3 (reads, auto-failover)             │ │ │
│  │  │  Storage: Ceph RBD (IOPS optimized)              │ │ │
│  │  │  Connection Pooler: PgBouncer (1000 conns)      │ │ │
│  │  │  Backups: pgBackRest → MinIO (PITR)             │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  Redis (Redis Operator)                          │ │ │
│  │  │                                                   │ │ │
│  │  │  Cluster Mode: 3 masters + 3 replicas           │ │ │
│  │  │  Persistence: AOF + RDB                          │ │ │
│  │  │  Max Memory: 64GB per node                       │ │ │
│  │  │  Eviction: allkeys-lru                           │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  Kafka (Strimzi operator)                        │ │ │
│  │  │                                                   │ │ │
│  │  │  Brokers: 5 (HA)                                 │ │ │
│  │  │  Replication factor: 3                           │ │ │
│  │  │  Partitions: 32 per topic                        │ │ │
│  │  │  Storage: Ceph (fast SSD pool)                   │ │ │
│  │  │  Retention: 7 days hot + archive to MinIO       │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  Qdrant (Vector DB, self-hosted)                │ │ │
│  │  │                                                   │ │ │
│  │  │  Nodes: 3 (sharding + replication)              │ │ │
│  │  │  Vectors: 1M (1536 dims)                         │ │ │
│  │  │  HNSW index (m=16, ef_construct=100)            │ │ │
│  │  │  Storage: 50GB SSD                               │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  MinIO (S3-compatible object storage)           │ │ │
│  │  │                                                   │ │ │
│  │  │  Nodes: 4 (distributed mode)                     │ │ │
│  │  │  Erasure coding: 2 parity                        │ │ │
│  │  │  Storage backend: Ceph (HDD pool, cheap)         │ │ │
│  │  │  Versioning enabled                              │ │ │
│  │  │  Lifecycle: hot 30d → archive 1yr → delete       │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  ClickHouse (OLAP, analytics)                    │ │ │
│  │  │                                                   │ │ │
│  │  │  Cluster: 3 shards × 2 replicas                  │ │ │
│  │  │  Storage: Ceph (compression 10x)                 │ │ │
│  │  │  MergeTree tables with partitioning by date     │ │ │
│  │  │  TTL: 90 days hot → 1 year cold → delete         │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  OBSERVABILITY STACK                                   │ │
│  │                                                         │ │
│  │  ┌──────────────┐  ┌──────────────┐  ┌─────────────┐ │ │
│  │  │ Prometheus   │  │  Loki        │  │  Jaeger     │ │ │
│  │  │ (Metrics)    │  │  (Logs)      │  │  (Traces)   │ │ │
│  │  │              │  │              │  │             │ │ │
│  │  │ • kube-state │  │ • promtail   │  │ • OTLP      │ │ │
│  │  │ • node-exp.  │  │ • retention  │  │ • sampling  │ │ │
│  │  │ • cadvisor   │  │   7 days     │  │   1%        │ │ │
│  │  └──────────────┘  └──────────────┘  └─────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  Grafana (Dashboards)                            │ │ │
│  │  │  • RED metrics per service                       │ │ │
│  │  │  • Resource utilization (CPU, mem, disk)         │ │ │
│  │  │  • SLO tracking (latency, availability)          │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  │                                                         │ │
│  │  ┌──────────────────────────────────────────────────┐ │ │
│  │  │  Alertmanager (Alerts)                           │ │ │
│  │  │  • Routes to Slack / PagerDuty                   │ │ │
│  │  │  • Severity levels: warning, critical            │ │ │
│  │  │  • On-call schedules                             │ │ │
│  │  └──────────────────────────────────────────────────┘ │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘

┌─────────────────────────────────────────────────────────────┐
│              UNDERLYING INFRASTRUCTURE                       │
│                                                              │
│  Option A: On-Premise (Proxmox + Ceph)                     │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Proxmox Cluster (5 nodes)                             │ │
│  │    ├─ Node 1-3: K8s masters + workers (SSD)           │ │
│  │    ├─ Node 4-5: Storage nodes (Ceph OSD, HDD+SSD)     │ │
│  │                                                         │ │
│  │  Ceph Cluster                                          │ │
│  │    ├─ RBD pool (SSD): databases, high IOPS            │ │
│  │    ├─ CephFS: shared volumes                          │ │
│  │    └─ RGW pool (HDD): object storage (MinIO backend)  │ │
│  │                                                         │ │
│  │  Networking: 10GbE, dual uplinks                      │ │
│  │  Power: dual PSU, UPS backup                          │ │
│  └────────────────────────────────────────────────────────┘ │
│                                                              │
│  Option B: Cloud (GKE / EKS / AKS)                         │
│  ┌────────────────────────────────────────────────────────┐ │
│  │  Managed Kubernetes (multi-zone)                       │ │
│  │  Node pools:                                           │ │
│  │    ├─ General: n2-standard-8 (5-50 nodes, auto-scale)│ │
│  │    ├─ GPU: n1-standard-8 + Tesla T4 (2-10 nodes)     │ │
│  │    └─ Spot: preemptible (batch jobs, 50% discount)   │ │
│  │                                                         │ │
│  │  Storage: GCP Persistent Disk SSD / EBS gp3           │ │
│  │  Network: VPC, Cloud NAT, Load Balancers              │ │
│  └────────────────────────────────────────────────────────┘ │
└─────────────────────────────────────────────────────────────┘
```

### Потоки работы

**(Аналогичны Варианту 1, но с self-hosted компонентами)**

**Ключевые отличия:**
- Больше контроля над оптимизацией (query планы, kernel tuning)
- Требуется Platform Engineer для операционки
- Стоимость предсказуемее (фиксированные ресурсы)

### Trade-offs

**Плюсы:**
- ✅ **Полный контроль**: оптимизация на уровне БД/ядра/сети
- ✅ **Cost-effective at scale**: при >100TB данных дешевле облака
- ✅ **Compliance**: данные в своём ДЦ (GDPR, 152-ФЗ)
- ✅ **No vendor lock-in**: портируемость между облаками/on-prem
- ✅ **Custom hardware**: GPU для ML, NVMe для БД

**Минусы:**
- ❌ **Operational overhead**: требует Platform/SRE engineer (1-2 FTE)
- ❌ **Longer setup**: 1-2 месяца до production-ready
- ❌ **Capacity planning**: нужно прогнозировать рост
- ❌ **Hardware maintenance** (для on-premise): замена дисков, апгрейды
- ❌ **Disaster recovery**: сложнее настроить multi-region

### Когда выбирать

- ✅ Команда >10 человек, есть выделенный DevOps/Platform engineer
- ✅ Объёмы данных >50TB или трафик >10k RPS стабильно
- ✅ Compliance требует on-premise (банки, госсектор)
- ✅ Бюджет есть на capex (железо) или готовы к reserved instances
- ✅ Важна предсказуемая стоимость

**Ориентировочная стоимость:**

**Option A: On-Premise (Proxmox + Ceph)**
- Hardware (5 серверов): $50k capex (амортизация 3 года = $1400/мес)
- Colocation: $500/мес (стойка + 10GbE)
- Электричество: $300/мес
- Platform engineer: $8k/мес (зарплата)
- **Итого: ~$10k/мес** (после амортизации железа)

**Option B: Cloud (GKE)**
- Compute: 20 n2-standard-8 nodes × $250 = $5k/мес
- Storage: 10TB SSD × $170 = $1.7k/мес
- Network: $500/мес
- Platform engineer: $8k/мес
- **Итого: ~$15k/мес**

При масштабе 1M агентов on-premise выгоднее через 1.5 года.

---

## Вариант 3: Modern Data Platform (Analytics-first)

### Идея

Оптимизация под **аналитику и ML**: OLTP (Supabase/Postgres) для транзакций + OLAP (ClickHouse) для аналитики + векторный поиск (Qdrant) + event streaming (Kafka). Разделение workloads для максимальной производительности каждого.

**Single source of truth:** события в Kafka → ETL в ClickHouse (агрегации) + Postgres (оперативные данные) + Qdrant (семантика).

### Структура (кратко)

```
Data Platform Architecture:

OLTP Layer: Supabase (Postgres)
  ├─ Operational data (twins, trajectories, wallets)
  ├─ Row-level security
  └─ Realtime subscriptions

Event Streaming: Confluent Cloud (Kafka)
  ├─ Topics: learning, economic, agent, system
  ├─ Stream processors: ksqlDB
  └─ Connect: sinks to ClickHouse + Postgres

OLAP Layer: ClickHouse Cloud
  ├─ Events archive (compressed 10-100x)
  ├─ Aggregations (daily/hourly rollups)
  ├─ Dashboards (Grafana, Metabase)
  └─ ML feature store

Vector Layer: Qdrant Cloud
  ├─ Embeddings (OpenAI ada-002)
  ├─ Semantic search (<50ms p95)
  └─ Recommendations engine

ML Platform: Databricks / SageMaker
  ├─ Training pipelines (Airflow orchestration)
  ├─ Feature engineering (dbt + ClickHouse)
  ├─ Model registry (MLflow)
  └─ Inference: FastAPI on K8s
```

### Trade-offs

**Плюсы:**
- ✅ **Лучшая аналитика**: ClickHouse сжатие 10-100x, запросы в 10-100x быстрее Postgres
- ✅ **Масштабируемость событий**: Kafka handle миллионы events/sec
- ✅ **ML-ready**: Feature store, training pipelines из коробки
- ✅ **Разделение workloads**: OLTP не мешает OLAP

**Минусы:**
- ❌ **Сложность**: больше moving parts (5+ сервисов)
- ❌ **Дублирование данных**: Postgres + ClickHouse
- ❌ **Стоимость**: managed Kafka + ClickHouse + Databricks = дорого
- ❌ **Консистентность**: eventual consistency между системами

### Когда выбирать

- ✅ Аналитика и ML — ключевые функции (рекомендации, персонализация)
- ✅ Объём событий >100M/день
- ✅ Команда есть Data/ML Engineers (2-3 FTE)
- ✅ Бюджет $20k-40k/мес на data platform

**Ориентировочная стоимость (1M агентов):**
- Supabase Pro: $2k/мес
- Confluent Cloud: $3k/мес
- ClickHouse Cloud: $2k/мес
- Qdrant Cloud: $500/мес
- Databricks: $5k/мес (compute + storage)
- Airflow (MWAA): $500/мес
- Data engineer(s): $12k/мес (зарплата × 1.5 FTE)
- **Итого: ~$25k/мес**

---

## Вариант 4: Hybrid Cloud + Edge (Global Distribution)

### Идея

**Минимальные латентности** для глобальной аудитории: Edge Functions (Cloudflare Workers, Vercel Edge) + Multi-region databases + CDN. Пользователь всегда обращается к ближайшему edge location (<50ms latency).

**Single source of truth:** распределённая система с eventual consistency, CRDTs для конфликт-резолвинга.

### Структура (кратко)

```
Edge-First Architecture:

Edge Layer (200+ locations):
  Cloudflare Workers / Vercel Edge Functions
  ├─ Auth checks (JWT verify)
  ├─ Rate limiting (Durable Objects)
  ├─ Routing (geo-based)
  └─ Read-heavy queries (cache-first)

Regional Hubs (5 regions):
  Primary: US-East
  Secondary: EU-West, Asia-Pacific, US-West, South America

  Each region:
    ├─ Postgres read replica (Neon/PlanetScale multi-region)
    ├─ Redis cache
    ├─ Qdrant vector DB replica
    └─ API servers (K8s or Cloud Run)

Central (write-master):
  Primary Postgres (US-East)
  Kafka event bus
  Write operations routed here

Sync:
  Postgres: streaming replication (<1s lag)
  Redis: async replication
  Qdrant: async sync (5-10s lag acceptable)
```

### Trade-offs

**Плюсы:**
- ✅ **Минимальные латентности**: <50ms globally
- ✅ **High availability**: multi-region failover
- ✅ **DDoS protection**: edge layer поглощает атаки
- ✅ **Compliance**: данные в регионе пользователя

**Минусы:**
- ❌ **Eventual consistency**: сложнее reasoning
- ❌ **Конфликты**: нужны CRDTs или last-write-wins
- ❌ **Стоимость**: multi-region = 3-5x дороже
- ❌ **Debugging**: распределённые трейсы сложнее

### Когда выбирать

- ✅ Глобальная аудитория, latency критична (<100ms SLA)
- ✅ Высокие требования к availability (99.99%+)
- ✅ Бюджет $15k-40k/мес
- ✅ Eventual consistency приемлема

**Ориентировочная стоимость:**
- Cloudflare Workers: $5/мес + $0.5 per million requests = $1.5k/мес
- Neon/PlanetScale multi-region: $5k/мес
- Redis Enterprise multi-region: $2k/мес
- Qdrant Cloud (3 regions): $1.5k/мес
- Kafka multi-region: $4k/мес
- CDN (Cloudflare): $200/мес
- **Итого: ~$14k/мес базовый** + рост с трафиком

---

## Сводная таблица сравнения (для миллиона агентов)

| Критерий | Cloud-native Serverless | Hybrid K8s (Proxmox/Cloud) | Modern Data Platform | Hybrid Cloud + Edge |
|----------|-------------------------|----------------------------|----------------------|---------------------|
| **Время до production** | 🟢 2-4 недели | 🟡 1-2 месяца | 🔴 2-3 месяца | 🟡 1-2 месяца |
| **Команда (FTE)** | 🟢 8-10 (без DevOps) | 🟡 10-12 (+Platform eng) | 🔴 12-15 (+Data/ML eng) | 🟡 10-12 (+Platform) |
| **Стоимость (1M агентов)** | 🟡 $6k-20k/мес | 🟢 $10k-15k/мес (on-prem) | 🔴 $25k-40k/мес | 🟡 $14k-30k/мес |
| **Latency p95** | 🟡 100-200ms | 🟡 50-150ms (single-region) | 🟡 100-200ms | 🟢 <50ms (multi-region) |
| **Throughput (RPS)** | 🟢 Auto-scale до 100k+ | 🟢 Планируется (до 50k) | 🟢 100k+ (Kafka) | 🟢 Unlimited (edge) |
| **Масштабируемость данных** | 🟢 Петабайты (S3/R2) | 🟡 До 100TB (Ceph) | 🟢 Петабайты (ClickHouse) | 🟢 Петабайты (multi-region) |
| **Аналитика** | 🟡 ClickHouse Cloud | 🟢 Self-hosted ClickHouse | 🟢 Native (ClickHouse) | 🟡 Regional aggregation |
| **ML/AI capabilities** | 🟡 Pinecone + external | 🟢 Qdrant + GPU nodes | 🟢 Databricks/SageMaker | 🟡 Edge inference limited |
| **Observability** | 🟢 Managed (Datadog) | 🟢 Self-hosted (Prom+Grafana) | 🟢 Both options | 🟢 Multi-region tracing |
| **Compliance (GDPR)** | 🟡 Vendor-dependent | 🟢 Full control | 🟡 Vendor-dependent | 🟢 Data residency per region |
| **Disaster Recovery RTO** | 🟢 <15 минут (auto) | 🟡 <1 час (manual) | 🟡 <30 минут | 🟢 <5 минут (failover) |
| **Vendor lock-in** | 🔴 Высокий | 🟢 Низкий (OSS stack) | 🟡 Средний | 🔴 Высокий (edge platforms) |
| **Developer experience** | 🟢 Отличный (SDK, docs) | 🟡 Средний (требует k8s знаний) | 🟡 Средний (сложная архитектура) | 🟢 Хороший (edge SDKs) |
| **Cost predictability** | 🔴 Низкая (pay-per-use) | 🟢 Высокая (фикс. ресурсы) | 🟡 Средняя | 🔴 Низкая (traffic-based) |
| **Cold start latency** | 🔴 50-500ms | 🟢 0ms (always hot) | 🟢 0ms | 🟡 10-50ms (edge warm) |

**Легенда:**
🟢 Отлично
🟡 Средне
🔴 Ограничение

---

## Рекомендация для команды 10 человек + 1M агентов

### Гибридная стратегия: Cloud-native + постепенный переход к Kubernetes

**Фаза 1 (месяцы 1-6): Cloud-native MVP**
- Supabase + Pinecone + Kafka (Confluent) + Vercel
- Быстрый time-to-market, focus на продукт
- Команда 8-10 человек без DevOps
- Бюджет: $6k-10k/мес

**Фаза 2 (месяцы 6-12): Гибрид (Cloud + K8s)**
- Kubernetes (GKE/EKS) для custom workloads
- Self-hosted Qdrant + Redis (cost optimization)
- Supabase остаётся для OLTP (проверенное решение)
- +1 Platform Engineer
- Бюджет: $10k-15k/мес

**Фаза 3 (год 2+): Data Platform + On-Premise опция**
- ClickHouse для аналитики
- Kafka self-hosted (Strimzi на K8s)
- Опциональный on-premise кластер (Proxmox + Ceph) если volumes >100TB
- +1-2 Data Engineers
- Бюджет: $15k-25k/мес

### Следующие 5 шагов (конкретные)

**Шаг 1: Setup Cloud-native foundation (неделя 1-2)**

```bash
# 1. Создать Supabase project
supabase init
supabase db push

# 2. Setup Kafka (Confluent Cloud trial)
confluent kafka cluster create ecosystem-events \
  --cloud gcp \
  --region us-central1

# 3. Setup vector DB (Pinecone)
pinecone create-index knowledge-base \
  --dimension 1536 \
  --metric cosine \
  --replicas 3

# 4. Deploy API (Vercel)
vercel deploy --prod

# 5. Setup monitoring (Datadog trial)
datadog-agent install
```

**Шаг 2: Implement rate limiting & quotas (неделя 3)**

```typescript
// middleware/rateLimit.ts
import { Redis } from '@upstash/redis';

const redis = Redis.fromEnv();

export async function rateLimitMiddleware(req, agentId: string) {
  const key = `rate:${agentId}:${Math.floor(Date.now() / 60000)}`;
  const count = await redis.incr(key);

  if (count === 1) {
    await redis.expire(key, 60);
  }

  const limit = getTierLimit(agentId); // 10, 100, 1000 по tier

  if (count > limit) {
    throw new RateLimitError({
      retryAfter: 60,
      remaining: 0
    });
  }

  return { remaining: limit - count };
}
```

**Шаг 3: Setup CI/CD pipeline (неделя 4)**

```yaml
# .github/workflows/deploy.yml
name: Deploy

on:
  push:
    branches: [main]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - run: npm ci
      - run: npm test
      - run: npm run lint

  deploy-staging:
    needs: test
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3
      - uses: vercel/actions@v2
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-env: preview

  smoke-test:
    needs: deploy-staging
    runs-on: ubuntu-latest
    steps:
      - run: |
          curl -f https://staging-api.example.com/health || exit 1

  deploy-production:
    needs: smoke-test
    runs-on: ubuntu-latest
    steps:
      - uses: vercel/actions@v2
        with:
          vercel-token: ${{ secrets.VERCEL_TOKEN }}
          vercel-env: production
          vercel-args: '--prod'
```

**Шаг 4: Implement observability stack (месяц 2)**

```yaml
# docker-compose.observability.yml
version: '3.8'

services:
  prometheus:
    image: prom/prometheus:latest
    ports: ["9090:9090"]
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana:latest
    ports: ["3000:3000"]
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=secret

  loki:
    image: grafana/loki:latest
    ports: ["3100:3100"]

  promtail:
    image: grafana/promtail:latest
    volumes:
      - /var/log:/var/log

  jaeger:
    image: jaegertracing/all-in-one:latest
    ports:
      - "16686:16686"  # UI
      - "14268:14268"  # collector
```

```typescript
// instrument.ts (OpenTelemetry)
import { NodeSDK } from '@opentelemetry/sdk-node';
import { getNodeAutoInstrumentations } from '@opentelemetry/auto-instrumentations-node';
import { JaegerExporter } from '@opentelemetry/exporter-jaeger';

const sdk = new NodeSDK({
  traceExporter: new JaegerExporter({
    endpoint: 'http://jaeger:14268/api/traces'
  }),
  instrumentations: [getNodeAutoInstrumentations()]
});

sdk.start();
```

**Шаг 5: Create chaos testing & load testing suite (месяц 3)**

```typescript
// tests/load/k6-test.js
import http from 'k6/http';
import { check, sleep } from 'k6';

export const options = {
  stages: [
    { duration: '2m', target: 100 },   // ramp-up
    { duration: '5m', target: 1000 },  // steady state
    { duration: '2m', target: 0 },     // ramp-down
  ],
  thresholds: {
    http_req_duration: ['p(95)<200'], // 95% requests < 200ms
    http_req_failed: ['rate<0.01'],    // <1% errors
  },
};

export default function() {
  const res = http.post('https://api.example.com/query_knowledge', JSON.stringify({
    query: 'системное мышление',
    limit: 10
  }), {
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${__ENV.API_TOKEN}`
    },
  });

  check(res, {
    'status is 200': (r) => r.status === 200,
    'response time < 200ms': (r) => r.timings.duration < 200,
  });

  sleep(1);
}
```

```bash
# Запуск load test
k6 run --vus 1000 --duration 10m tests/load/k6-test.js
```

---

## Self-Critique и риски

**✅ Что учтено:**
- Промышленный масштаб (1M агентов, 28k RPS)
- Команда 10 человек (роли, зарплаты)
- 4 разных парадигмы (serverless, K8s, data platform, edge)
- Дополнительные технологии (Proxmox, Ceph, Qdrant, ClickHouse)
- Реальные стоимости и trade-offs
- Конкретные следующие шаги с кодом

**⚠️ Риски:**

1. **Cost overruns при serverless**
   - Pay-per-use может "выстрелить" при DDoS или bug (retry loops)
   - **Митигация**: жёсткие rate limits, budget alerts, circuit breakers

2. **Complexity управления K8s**
   - Требует опыта, on-call rotation
   - **Митигация**: начать с managed K8s (GKE/EKS), постепенно расти

3. **Data consistency в distributed системах**
   - Eventual consistency, конфликты при multi-region writes
   - **Митигация**: CRDTs, conflict resolution policies, тестирование сценариев

4. **Vendor lock-in**
   - Supabase, Pinecone, Cloudflare — сложно мигрировать
   - **Митигация**: абстракции (Repository pattern), периодический аудит альтернатив

5. **Team scaling**
   - 10 человек → 20+ потребует реорганизации (micro-teams)
   - **Митигация**: заложить модульность с начала, mono-repo с workspaces

6. **Cold storage costs**
   - 15TB через 3 года, даже cold storage = $150-300/мес
   - **Митигация**: агрессивные lifecycle policies, compression, тарификация участникам

7. **Regulatory compliance**
   - GDPR (EU), 152-ФЗ (RU), данные пользователей
   - **Митигация**: legal audit, Data Processing Agreements, opt-in/opt-out

**❓ Вопросы для уточнения:**

1. **География пользователей**: преимущественно RU/EU/Global? (влияет на выбор regions)
2. **Sensitivity данных**: нужен ли on-premise или достаточно cloud с DPA?
3. **ML workloads**: насколько критичны real-time рекомендации vs batch?
4. **Budget flexibility**: может ли бюджет варьироваться 2x month-to-month?
5. **Team hiring**: есть ли возможность нанять DevOps/Data Engineers или аутсорс?
6. **Time constraints**: deadline для production-ready (3 месяца vs 6+ месяцев)?
7. **ИИ-агенты ownership**: кто владеет агентами — платформа или участники? (влияет на auth model)

**🔍 Дальнейшая работа:**

- **Capacity planning**: точный расчёт на основе реальных метрик (pilot с 1000 агентов)
- **Cost modeling**: детальная таблица costs для каждого варианта при разных scales
- **Disaster Recovery Runbook**: пошаговые процедуры восстановления
- **Security audit**: penetration testing, OWASP Top 10, secrets management
- **Compliance checklist**: GDPR, CCPA, ISO 27001 если нужно
- **Team onboarding plan**: как новые разработчики быстро включаются в проект

---

## Финальная рекомендация

Для команды **10 человек** и масштаба **миллион агентов**:

**Старт (месяцы 1-6):**
→ **Вариант 1: Cloud-native / Serverless**
   - Supabase + Pinecone + Kafka (Confluent) + Vercel
   - Минимум ops, максимум фокус на продукте
   - Бюджет $6k-15k/мес управляем

**Рост (месяцы 6-18):**
→ **Гибрид: Вариант 1 + элементы Варианта 2**
   - Kubernetes (GKE) для custom workloads (ML inference, batch jobs)
   - Self-hosted Qdrant + Redis (cost optimization при росте)
   - +1 Platform Engineer
   - Бюджет $10k-20k/мес

**Масштаб (год 2+):**
→ **Вариант 3 (Data Platform) или Вариант 4 (Multi-region)**
   - Если аналитика критична → Data Platform (ClickHouse, Databricks)
   - Если latency критична → Multi-region + Edge
   - Опционально on-premise (Proxmox + Ceph) если volumes >100TB
   - Команда растёт до 15+ человек
   - Бюджет $20k-40k/мес

**Ключевой принцип:** начать просто (serverless), расти инкрементально, добавлять сложность только когда она оправдана метриками.
